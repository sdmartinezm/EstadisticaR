<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 17 Regresión logística | Estadística aplicada con R</title>
  <meta name="description" content="Capítulo 17 Regresión logística | Estadística aplicada con R" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 17 Regresión logística | Estadística aplicada con R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 17 Regresión logística | Estadística aplicada con R" />
  
  
  



<meta name="date" content="2021-05-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.gif" type="image/x-icon" />
<link rel="prev" href="selección-de-variables-y-construcción-de-modelos.html"/>
<link rel="next" href="más-allá.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Estadística aplicada con R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#convenciones"><i class="fa fa-check"></i><b>1.1</b> Convenciones</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introducción-a-r.html"><a href="introducción-a-r.html"><i class="fa fa-check"></i><b>2</b> Introducción a <code>R</code></a>
<ul>
<li class="chapter" data-level="2.1" data-path="introducción-a-r.html"><a href="introducción-a-r.html#primeros-pasos"><i class="fa fa-check"></i><b>2.1</b> Primeros pasos</a></li>
<li class="chapter" data-level="2.2" data-path="introducción-a-r.html"><a href="introducción-a-r.html#cálculos-básicos"><i class="fa fa-check"></i><b>2.2</b> Cálculos básicos</a></li>
<li class="chapter" data-level="2.3" data-path="introducción-a-r.html"><a href="introducción-a-r.html#obteniendo-ayuda"><i class="fa fa-check"></i><b>2.3</b> Obteniendo ayuda</a></li>
<li class="chapter" data-level="2.4" data-path="introducción-a-r.html"><a href="introducción-a-r.html#instalación-de-paquetes"><i class="fa fa-check"></i><b>2.4</b> Instalación de paquetes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="datos-y-programación.html"><a href="datos-y-programación.html"><i class="fa fa-check"></i><b>3</b> Datos y programación</a>
<ul>
<li class="chapter" data-level="3.1" data-path="datos-y-programación.html"><a href="datos-y-programación.html#tipos-de-datos"><i class="fa fa-check"></i><b>3.1</b> Tipos de datos</a></li>
<li class="chapter" data-level="3.2" data-path="datos-y-programación.html"><a href="datos-y-programación.html#estructuras-de-datos"><i class="fa fa-check"></i><b>3.2</b> Estructuras de datos</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="datos-y-programación.html"><a href="datos-y-programación.html#vectores"><i class="fa fa-check"></i><b>3.2.1</b> Vectores</a></li>
<li class="chapter" data-level="3.2.2" data-path="datos-y-programación.html"><a href="datos-y-programación.html#vectorización"><i class="fa fa-check"></i><b>3.2.2</b> Vectorización</a></li>
<li class="chapter" data-level="3.2.3" data-path="datos-y-programación.html"><a href="datos-y-programación.html#operadores-logicos"><i class="fa fa-check"></i><b>3.2.3</b> Operadores logicos</a></li>
<li class="chapter" data-level="3.2.4" data-path="datos-y-programación.html"><a href="datos-y-programación.html#más-vectorización"><i class="fa fa-check"></i><b>3.2.4</b> Más vectorización</a></li>
<li class="chapter" data-level="3.2.5" data-path="datos-y-programación.html"><a href="datos-y-programación.html#matrices"><i class="fa fa-check"></i><b>3.2.5</b> Matrices</a></li>
<li class="chapter" data-level="3.2.6" data-path="datos-y-programación.html"><a href="datos-y-programación.html#listas"><i class="fa fa-check"></i><b>3.2.6</b> Listas</a></li>
<li class="chapter" data-level="3.2.7" data-path="datos-y-programación.html"><a href="datos-y-programación.html#marcos-de-datos"><i class="fa fa-check"></i><b>3.2.7</b> Marcos de datos</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="datos-y-programación.html"><a href="datos-y-programación.html#conceptos-básicos-de-programación"><i class="fa fa-check"></i><b>3.3</b> Conceptos básicos de programación</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="datos-y-programación.html"><a href="datos-y-programación.html#flujo-de-control"><i class="fa fa-check"></i><b>3.3.1</b> Flujo de control</a></li>
<li class="chapter" data-level="3.3.2" data-path="datos-y-programación.html"><a href="datos-y-programación.html#funciones"><i class="fa fa-check"></i><b>3.3.2</b> Funciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html"><i class="fa fa-check"></i><b>4</b> Resumen de datos</a>
<ul>
<li class="chapter" data-level="4.1" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#resumen-estadístico"><i class="fa fa-check"></i><b>4.1</b> Resumen estadístico</a>
<ul>
<li class="chapter" data-level="" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#tendencia-central"><i class="fa fa-check"></i>Tendencia central</a></li>
<li class="chapter" data-level="" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#dispersión"><i class="fa fa-check"></i>Dispersión</a></li>
<li class="chapter" data-level="" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#categórica"><i class="fa fa-check"></i>Categórica</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#graficas"><i class="fa fa-check"></i><b>4.2</b> Graficas</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#histogramas"><i class="fa fa-check"></i><b>4.2.1</b> Histogramas</a></li>
<li class="chapter" data-level="4.2.2" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#gráfico-de-barras"><i class="fa fa-check"></i><b>4.2.2</b> Gráfico de barras</a></li>
<li class="chapter" data-level="4.2.3" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#diagramas-de-cajas"><i class="fa fa-check"></i><b>4.2.3</b> Diagramas de cajas</a></li>
<li class="chapter" data-level="4.2.4" data-path="resumen-de-datos.html"><a href="resumen-de-datos.html#gráfico-de-dispersión"><i class="fa fa-check"></i><b>4.2.4</b> Gráfico de dispersión</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html"><i class="fa fa-check"></i><b>5</b> Probabilidad y estadística en <code>R</code></a>
<ul>
<li class="chapter" data-level="5.1" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#probabilidad-en-r"><i class="fa fa-check"></i><b>5.1</b> Probabilidad en <code>R</code></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#distribuciones"><i class="fa fa-check"></i><b>5.1.1</b> Distribuciones</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#pruebas-de-hipótesis-en-r"><i class="fa fa-check"></i><b>5.2</b> Pruebas de hipótesis en <code>R</code></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#prueba-t-de-una-muestra-revisión"><i class="fa fa-check"></i><b>5.2.1</b> Prueba t de una muestra: revisión</a></li>
<li class="chapter" data-level="5.2.2" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#prueba-t-de-una-muestra-ejemplo"><i class="fa fa-check"></i><b>5.2.2</b> Prueba t de una muestra: ejemplo</a></li>
<li class="chapter" data-level="5.2.3" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#prueba-t-de-dos-muestras-revisión"><i class="fa fa-check"></i><b>5.2.3</b> Prueba t de dos muestras: revisión</a></li>
<li class="chapter" data-level="5.2.4" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#prueba-t-de-dos-muestras-ejemplo"><i class="fa fa-check"></i><b>5.2.4</b> Prueba t de dos muestras: Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#simulación"><i class="fa fa-check"></i><b>5.3</b> Simulación</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#diferencias-emparejadas"><i class="fa fa-check"></i><b>5.3.1</b> Diferencias emparejadas</a></li>
<li class="chapter" data-level="5.3.2" data-path="probabilidad-y-estadística-en-r.html"><a href="probabilidad-y-estadística-en-r.html#distribución-de-una-media-muestral"><i class="fa fa-check"></i><b>5.3.2</b> Distribución de una media muestral</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="recursos-r.html"><a href="recursos-r.html"><i class="fa fa-check"></i><b>6</b> Recursos <code>R</code></a>
<ul>
<li class="chapter" data-level="6.1" data-path="recursos-r.html"><a href="recursos-r.html#referencias-y-tutoriales-para-principiantes"><i class="fa fa-check"></i><b>6.1</b> Referencias y tutoriales para principiantes</a></li>
<li class="chapter" data-level="6.2" data-path="recursos-r.html"><a href="recursos-r.html#referencias-intermedias"><i class="fa fa-check"></i><b>6.2</b> Referencias intermedias</a></li>
<li class="chapter" data-level="6.3" data-path="recursos-r.html"><a href="recursos-r.html#referencias-avanzadas"><i class="fa fa-check"></i><b>6.3</b> Referencias avanzadas</a></li>
<li class="chapter" data-level="6.4" data-path="recursos-r.html"><a href="recursos-r.html#comparaciones-rápidas-con-otros-lenguajes"><i class="fa fa-check"></i><b>6.4</b> Comparaciones rápidas con otros lenguajes</a></li>
<li class="chapter" data-level="6.5" data-path="recursos-r.html"><a href="recursos-r.html#vídeos-de-rstudio-y-rmarkdown"><i class="fa fa-check"></i><b>6.5</b> Vídeos de RStudio y RMarkdown</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html"><i class="fa fa-check"></i><b>7</b> Regresión lineal simple</a>
<ul>
<li class="chapter" data-level="7.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#modelado"><i class="fa fa-check"></i><b>7.1</b> Modelado</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#modelo-de-regresión-lineal-simple"><i class="fa fa-check"></i><b>7.1.1</b> Modelo de regresión lineal simple</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#enfoque-de-mínimos-cuadrados"><i class="fa fa-check"></i><b>7.2</b> Enfoque de mínimos cuadrados</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#haciendo-predicciones"><i class="fa fa-check"></i><b>7.2.1</b> Haciendo predicciones</a></li>
<li class="chapter" data-level="7.2.2" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#residuales"><i class="fa fa-check"></i><b>7.2.2</b> Residuales</a></li>
<li class="chapter" data-level="7.2.3" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#estimación-de-la-varianza"><i class="fa fa-check"></i><b>7.2.3</b> Estimación de la varianza</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#descomposición-de-variación"><i class="fa fa-check"></i><b>7.3</b> Descomposición de variación</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#coeficiente-de-determinación"><i class="fa fa-check"></i><b>7.3.1</b> Coeficiente de determinación</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#la-función-lm"><i class="fa fa-check"></i><b>7.4</b> La función <code>lm</code></a></li>
<li class="chapter" data-level="7.5" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#enfoque-de-estimación-de-máxima-verosimilitud-mle"><i class="fa fa-check"></i><b>7.5</b> Enfoque de estimación de máxima verosimilitud (MLE)</a></li>
<li class="chapter" data-level="7.6" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#simulando-slr"><i class="fa fa-check"></i><b>7.6</b> Simulando SLR</a></li>
<li class="chapter" data-level="7.7" data-path="regresión-lineal-simple.html"><a href="regresión-lineal-simple.html#historia"><i class="fa fa-check"></i><b>7.7</b> Historia</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html"><i class="fa fa-check"></i><b>8</b> Inferencia para regresión lineal simple</a>
<ul>
<li class="chapter" data-level="8.1" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#teorema-de-gauss-markov"><i class="fa fa-check"></i><b>8.1</b> Teorema de Gauss-Markov</a></li>
<li class="chapter" data-level="8.2" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#distribuciones-muestrales"><i class="fa fa-check"></i><b>8.2</b> Distribuciones muestrales</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#simular-distribuciones-muestrales"><i class="fa fa-check"></i><b>8.2.1</b> Simular distribuciones muestrales</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#errores-estándar"><i class="fa fa-check"></i><b>8.3</b> Errores estándar</a></li>
<li class="chapter" data-level="8.4" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#intervalos-de-confianza-para-pendiente-e-intercepto"><i class="fa fa-check"></i><b>8.4</b> Intervalos de confianza para pendiente e Intercepto</a></li>
<li class="chapter" data-level="8.5" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#pruebas-de-hipótesis"><i class="fa fa-check"></i><b>8.5</b> Pruebas de hipótesis</a></li>
<li class="chapter" data-level="8.6" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#ejemplo-cars"><i class="fa fa-check"></i><b>8.6</b> Ejemplo <code>cars</code></a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#pruebas-en-r"><i class="fa fa-check"></i><b>8.6.1</b> Pruebas en <code>R</code></a></li>
<li class="chapter" data-level="8.6.2" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#significancia-de-la-regresión-prueba-t."><i class="fa fa-check"></i><b>8.6.2</b> Significancia de la regresión, prueba t.</a></li>
<li class="chapter" data-level="8.6.3" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#intervalos-de-confianza-en-r"><i class="fa fa-check"></i><b>8.6.3</b> Intervalos de confianza en <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#intervalo-de-confianza-para-la-respuesta-promedio"><i class="fa fa-check"></i><b>8.7</b> Intervalo de confianza para la respuesta Promedio</a></li>
<li class="chapter" data-level="8.8" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#intervalo-de-predicción-para-nuevas-observaciones"><i class="fa fa-check"></i><b>8.8</b> Intervalo de predicción para nuevas observaciones</a></li>
<li class="chapter" data-level="8.9" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#bandas-de-confianza-y-predicción"><i class="fa fa-check"></i><b>8.9</b> Bandas de confianza y predicción</a></li>
<li class="chapter" data-level="8.10" data-path="inferencia-para-regresión-lineal-simple.html"><a href="inferencia-para-regresión-lineal-simple.html#significancia-de-la-regresión-prueba-f"><i class="fa fa-check"></i><b>8.10</b> Significancia de la regresión, prueba F</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html"><i class="fa fa-check"></i><b>9</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="9.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#enfoque-matricial-para-la-regresión"><i class="fa fa-check"></i><b>9.1</b> Enfoque matricial para la regresión</a></li>
<li class="chapter" data-level="9.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#distribución-muestral"><i class="fa fa-check"></i><b>9.2</b> Distribución muestral</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#pruebas-de-un-solo-parámetro"><i class="fa fa-check"></i><b>9.2.1</b> Pruebas de un solo parámetro</a></li>
<li class="chapter" data-level="9.2.2" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#intervalos-de-confianza"><i class="fa fa-check"></i><b>9.2.2</b> Intervalos de confianza</a></li>
<li class="chapter" data-level="9.2.3" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#intervalos-de-confianza-para-la-respuesta-media"><i class="fa fa-check"></i><b>9.2.3</b> Intervalos de confianza para la respuesta media</a></li>
<li class="chapter" data-level="9.2.4" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#intervalos-de-predicción"><i class="fa fa-check"></i><b>9.2.4</b> Intervalos de predicción</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#significancia-de-la-regresión"><i class="fa fa-check"></i><b>9.3</b> Significancia de la regresión</a></li>
<li class="chapter" data-level="9.4" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#modelos-anidados"><i class="fa fa-check"></i><b>9.4</b> Modelos anidados</a></li>
<li class="chapter" data-level="9.5" data-path="regresión-lineal-múltiple.html"><a href="regresión-lineal-múltiple.html#simulación-1"><i class="fa fa-check"></i><b>9.5</b> Simulación</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html"><i class="fa fa-check"></i><b>10</b> Construcción del modelo</a>
<ul>
<li class="chapter" data-level="10.1" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#familia-forma-y-ajuste."><i class="fa fa-check"></i><b>10.1</b> Familia, forma, y ajuste.</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#ajuste."><i class="fa fa-check"></i><b>10.1.1</b> Ajuste.</a></li>
<li class="chapter" data-level="10.1.2" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#forma"><i class="fa fa-check"></i><b>10.1.2</b> Forma</a></li>
<li class="chapter" data-level="10.1.3" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#familia"><i class="fa fa-check"></i><b>10.1.3</b> Familia</a></li>
<li class="chapter" data-level="10.1.4" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#modelo-asumido-modelo-ajustado"><i class="fa fa-check"></i><b>10.1.4</b> Modelo asumido, modelo ajustado</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#explicación-versus-predicción"><i class="fa fa-check"></i><b>10.2</b> Explicación versus predicción</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#explicación"><i class="fa fa-check"></i><b>10.2.1</b> Explicación</a></li>
<li class="chapter" data-level="10.2.2" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#predicción"><i class="fa fa-check"></i><b>10.2.2</b> Predicción</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="construcción-del-modelo.html"><a href="construcción-del-modelo.html#resumen"><i class="fa fa-check"></i><b>10.3</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html"><i class="fa fa-check"></i><b>11</b> Interacciones y predictores categóricos</a>
<ul>
<li class="chapter" data-level="11.1" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html#variables-ficticias-dummy"><i class="fa fa-check"></i><b>11.1</b> Variables ficticias (Dummy)</a></li>
<li class="chapter" data-level="11.2" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html#interacciones"><i class="fa fa-check"></i><b>11.2</b> Interacciones</a></li>
<li class="chapter" data-level="11.3" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html#variables-factor"><i class="fa fa-check"></i><b>11.3</b> Variables factor</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html#factores-con-más-de-dos-niveles"><i class="fa fa-check"></i><b>11.3.1</b> Factores con más de dos niveles</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html#parametrización"><i class="fa fa-check"></i><b>11.4</b> Parametrización</a></li>
<li class="chapter" data-level="11.5" data-path="interacciones-y-predictores-categóricos.html"><a href="interacciones-y-predictores-categóricos.html#construcción-de-modelos-más-grandes"><i class="fa fa-check"></i><b>11.5</b> Construcción de modelos más grandes</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html"><i class="fa fa-check"></i><b>12</b> Análisis de varianza</a>
<ul>
<li class="chapter" data-level="12.1" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#experimentos"><i class="fa fa-check"></i><b>12.1</b> Experimentos</a></li>
<li class="chapter" data-level="12.2" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#prueba-t-de-dos-muestras"><i class="fa fa-check"></i><b>12.2</b> Prueba t de dos muestras</a></li>
<li class="chapter" data-level="12.3" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#anova-de-una-vía"><i class="fa fa-check"></i><b>12.3</b> ANOVA de una vía</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#variables-factor-1"><i class="fa fa-check"></i><b>12.3.1</b> Variables factor</a></li>
<li class="chapter" data-level="12.3.2" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#algo-de-simulación"><i class="fa fa-check"></i><b>12.3.2</b> Algo de simulación</a></li>
<li class="chapter" data-level="12.3.3" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#potencia"><i class="fa fa-check"></i><b>12.3.3</b> Potencia</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#pruebas-post-hoc"><i class="fa fa-check"></i><b>12.4</b> Pruebas Post Hoc</a></li>
<li class="chapter" data-level="12.5" data-path="análisis-de-varianza.html"><a href="análisis-de-varianza.html#anova-de-dos-vías"><i class="fa fa-check"></i><b>12.5</b> ANOVA de dos vías</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html"><i class="fa fa-check"></i><b>13</b> Diagnóstico de modelos</a>
<ul>
<li class="chapter" data-level="13.1" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#supuestos-del-modelo"><i class="fa fa-check"></i><b>13.1</b> Supuestos del modelo</a></li>
<li class="chapter" data-level="13.2" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#comprobación-de-supuestos"><i class="fa fa-check"></i><b>13.2</b> Comprobación de supuestos</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#gráfica-de-ajustados-versus-residuos"><i class="fa fa-check"></i><b>13.2.1</b> Gráfica de ajustados versus residuos</a></li>
<li class="chapter" data-level="13.2.2" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#prueba-de-breusch-pagan"><i class="fa fa-check"></i><b>13.2.2</b> Prueba de Breusch-Pagan</a></li>
<li class="chapter" data-level="13.2.3" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#histogramas-1"><i class="fa fa-check"></i><b>13.2.3</b> Histogramas</a></li>
<li class="chapter" data-level="13.2.4" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#gráficos-q-q"><i class="fa fa-check"></i><b>13.2.4</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="13.2.5" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#prueba-de-shapiro-wilk"><i class="fa fa-check"></i><b>13.2.5</b> Prueba de Shapiro-Wilk</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#observaciones-inusuales"><i class="fa fa-check"></i><b>13.3</b> Observaciones inusuales</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#apalancamiento"><i class="fa fa-check"></i><b>13.3.1</b> Apalancamiento</a></li>
<li class="chapter" data-level="13.3.2" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#valores-atípicos"><i class="fa fa-check"></i><b>13.3.2</b> Valores atípicos</a></li>
<li class="chapter" data-level="13.3.3" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#influencia"><i class="fa fa-check"></i><b>13.3.3</b> Influencia</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#ejemplos-de-análisis-de-datos"><i class="fa fa-check"></i><b>13.4</b> Ejemplos de análisis de datos</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#buenos-diagnósticos"><i class="fa fa-check"></i><b>13.4.1</b> Buenos diagnósticos</a></li>
<li class="chapter" data-level="13.4.2" data-path="diagnóstico-de-modelos.html"><a href="diagnóstico-de-modelos.html#diagnóstico-sospechoso"><i class="fa fa-check"></i><b>13.4.2</b> Diagnóstico sospechoso</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="transformaciones.html"><a href="transformaciones.html"><i class="fa fa-check"></i><b>14</b> Transformaciones</a>
<ul>
<li class="chapter" data-level="14.1" data-path="transformaciones.html"><a href="transformaciones.html#transformación-de-respuesta"><i class="fa fa-check"></i><b>14.1</b> Transformación de respuesta</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="transformaciones.html"><a href="transformaciones.html#transformaciones-estabilizadoras-de-varianza"><i class="fa fa-check"></i><b>14.1.1</b> Transformaciones estabilizadoras de varianza</a></li>
<li class="chapter" data-level="14.1.2" data-path="transformaciones.html"><a href="transformaciones.html#transformaciones-de-box-cox"><i class="fa fa-check"></i><b>14.1.2</b> Transformaciones de Box-Cox</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="transformaciones.html"><a href="transformaciones.html#transformación-del-predictor"><i class="fa fa-check"></i><b>14.2</b> Transformación del predictor</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="transformaciones.html"><a href="transformaciones.html#polinomios"><i class="fa fa-check"></i><b>14.2.1</b> Polinomios</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="transformaciones.html"><a href="transformaciones.html#transformaciones-de-respuesta"><i class="fa fa-check"></i>Transformaciones de respuesta</a></li>
<li class="chapter" data-level="" data-path="transformaciones.html"><a href="transformaciones.html#transformaciones-de-predictores"><i class="fa fa-check"></i>Transformaciones de predictores</a>
<ul>
<li class="chapter" data-level="14.2.2" data-path="transformaciones.html"><a href="transformaciones.html#un-modelo-cuadrático"><i class="fa fa-check"></i><b>14.2.2</b> Un modelo cuadrático</a></li>
<li class="chapter" data-level="14.2.3" data-path="transformaciones.html"><a href="transformaciones.html#sobreajuste-y-extrapolación"><i class="fa fa-check"></i><b>14.2.3</b> Sobreajuste y extrapolación</a></li>
<li class="chapter" data-level="14.2.4" data-path="transformaciones.html"><a href="transformaciones.html#comparación-de-modelos-polinomiales"><i class="fa fa-check"></i><b>14.2.4</b> Comparación de modelos polinomiales</a></li>
<li class="chapter" data-level="14.2.5" data-path="transformaciones.html"><a href="transformaciones.html#poly-función-y-polinomios-ortogonales"><i class="fa fa-check"></i><b>14.2.5</b> <code>poly()</code> Función y polinomios ortogonales</a></li>
<li class="chapter" data-level="14.2.6" data-path="transformaciones.html"><a href="transformaciones.html#función-de-inhibición"><i class="fa fa-check"></i><b>14.2.6</b> Función de inhibición</a></li>
<li class="chapter" data-level="14.2.7" data-path="transformaciones.html"><a href="transformaciones.html#ejemplo-con-datos"><i class="fa fa-check"></i><b>14.2.7</b> Ejemplo con datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="colinealidad.html"><a href="colinealidad.html"><i class="fa fa-check"></i><b>15</b> Colinealidad</a>
<ul>
<li class="chapter" data-level="15.1" data-path="colinealidad.html"><a href="colinealidad.html#colinealidad-exacta"><i class="fa fa-check"></i><b>15.1</b> Colinealidad exacta</a></li>
<li class="chapter" data-level="15.2" data-path="colinealidad.html"><a href="colinealidad.html#colinealidad-1"><i class="fa fa-check"></i><b>15.2</b> Colinealidad</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="colinealidad.html"><a href="colinealidad.html#factor-de-inflación-de-la-varianza."><i class="fa fa-check"></i><b>15.2.1</b> Factor de inflación de la varianza.</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="colinealidad.html"><a href="colinealidad.html#simulación-2"><i class="fa fa-check"></i><b>15.3</b> Simulación</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html"><i class="fa fa-check"></i><b>16</b> Selección de variables y construcción de modelos</a>
<ul>
<li class="chapter" data-level="16.1" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#criterio-de-calidad"><i class="fa fa-check"></i><b>16.1</b> Criterio de calidad</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#criterio-de-información-de-akaike"><i class="fa fa-check"></i><b>16.1.1</b> Criterio de información de Akaike</a></li>
<li class="chapter" data-level="16.1.2" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#criterio-de-información-bayesiano"><i class="fa fa-check"></i><b>16.1.2</b> Criterio de información Bayesiano</a></li>
<li class="chapter" data-level="16.1.3" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#r-cuadrado-ajustado"><i class="fa fa-check"></i><b>16.1.3</b> R cuadrado ajustado</a></li>
<li class="chapter" data-level="16.1.4" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#rmse-con-validación-cruzada"><i class="fa fa-check"></i><b>16.1.4</b> RMSE con validación cruzada</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#procedimientos-de-selección"><i class="fa fa-check"></i><b>16.2</b> Procedimientos de selección</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#búsqueda-hacia-atrás-backward"><i class="fa fa-check"></i><b>16.2.1</b> Búsqueda hacia atrás (Backward)</a></li>
<li class="chapter" data-level="16.2.2" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#búsqueda-hacia-adelante-forward"><i class="fa fa-check"></i><b>16.2.2</b> Búsqueda hacia adelante (Forward)</a></li>
<li class="chapter" data-level="16.2.3" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#búsqueda-por-pasos-stepwise"><i class="fa fa-check"></i><b>16.2.3</b> Búsqueda por pasos (Stepwise)</a></li>
<li class="chapter" data-level="16.2.4" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#búsqueda-exhaustiva"><i class="fa fa-check"></i><b>16.2.4</b> Búsqueda exhaustiva</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#términos-de-orden-superior"><i class="fa fa-check"></i><b>16.3</b> Términos de orden superior</a></li>
<li class="chapter" data-level="16.4" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#explicación-versus-predicción-1"><i class="fa fa-check"></i><b>16.4</b> Explicación versus predicción</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#explicación-1"><i class="fa fa-check"></i><b>16.4.1</b> Explicación</a></li>
<li class="chapter" data-level="16.4.2" data-path="selección-de-variables-y-construcción-de-modelos.html"><a href="selección-de-variables-y-construcción-de-modelos.html#predicción-1"><i class="fa fa-check"></i><b>16.4.2</b> Predicción</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regresión-logística.html"><a href="regresión-logística.html"><i class="fa fa-check"></i><b>17</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="17.1" data-path="regresión-logística.html"><a href="regresión-logística.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>17.1</b> Modelos lineales generalizados</a></li>
<li class="chapter" data-level="17.2" data-path="regresión-logística.html"><a href="regresión-logística.html#respuesta-binaria"><i class="fa fa-check"></i><b>17.2</b> Respuesta binaria</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="regresión-logística.html"><a href="regresión-logística.html#ajuste-de-la-regresión-logística"><i class="fa fa-check"></i><b>17.2.1</b> Ajuste de la regresión logística</a></li>
<li class="chapter" data-level="17.2.2" data-path="regresión-logística.html"><a href="regresión-logística.html#problemas-de-ajuste"><i class="fa fa-check"></i><b>17.2.2</b> Problemas de ajuste</a></li>
<li class="chapter" data-level="17.2.3" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplos-de-simulación"><i class="fa fa-check"></i><b>17.2.3</b> Ejemplos de simulación</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regresión-logística.html"><a href="regresión-logística.html#trabajar-con-regresión-logística"><i class="fa fa-check"></i><b>17.3</b> Trabajar con regresión logística</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="regresión-logística.html"><a href="regresión-logística.html#pruebas-con-glms"><i class="fa fa-check"></i><b>17.3.1</b> Pruebas con GLMs</a></li>
<li class="chapter" data-level="17.3.2" data-path="regresión-logística.html"><a href="regresión-logística.html#prueba-de-wald"><i class="fa fa-check"></i><b>17.3.2</b> Prueba de Wald</a></li>
<li class="chapter" data-level="17.3.3" data-path="regresión-logística.html"><a href="regresión-logística.html#prueba-de-razón-de-verosimilitud"><i class="fa fa-check"></i><b>17.3.3</b> Prueba de razón de verosimilitud</a></li>
<li class="chapter" data-level="17.3.4" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplo-saheart"><i class="fa fa-check"></i><b>17.3.4</b> Ejemplo <code>SAheart</code></a></li>
<li class="chapter" data-level="17.3.5" data-path="regresión-logística.html"><a href="regresión-logística.html#intervalos-de-confianza-1"><i class="fa fa-check"></i><b>17.3.5</b> Intervalos de confianza</a></li>
<li class="chapter" data-level="17.3.6" data-path="regresión-logística.html"><a href="regresión-logística.html#intervalos-de-confianza-para-la-respuesta-promedio"><i class="fa fa-check"></i><b>17.3.6</b> Intervalos de confianza para la respuesta promedio</a></li>
<li class="chapter" data-level="17.3.7" data-path="regresión-logística.html"><a href="regresión-logística.html#sintaxis-de-la-fórmula"><i class="fa fa-check"></i><b>17.3.7</b> Sintaxis de la fórmula</a></li>
<li class="chapter" data-level="17.3.8" data-path="regresión-logística.html"><a href="regresión-logística.html#desviación"><i class="fa fa-check"></i><b>17.3.8</b> Desviación</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="regresión-logística.html"><a href="regresión-logística.html#clasificación"><i class="fa fa-check"></i><b>17.4</b> Clasificación</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplo-spam"><i class="fa fa-check"></i><b>17.4.1</b> Ejemplo <code>spam</code></a></li>
<li class="chapter" data-level="17.4.2" data-path="regresión-logística.html"><a href="regresión-logística.html#evaluar-clasificadores"><i class="fa fa-check"></i><b>17.4.2</b> Evaluar clasificadores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="más-allá.html"><a href="más-allá.html"><i class="fa fa-check"></i><b>18</b> Más allá</a>
<ul>
<li class="chapter" data-level="18.1" data-path="más-allá.html"><a href="más-allá.html#rstudio"><i class="fa fa-check"></i><b>18.1</b> RStudio</a></li>
<li class="chapter" data-level="18.2" data-path="más-allá.html"><a href="más-allá.html#tidy-data"><i class="fa fa-check"></i><b>18.2</b> Tidy Data</a></li>
<li class="chapter" data-level="18.3" data-path="más-allá.html"><a href="más-allá.html#visualización"><i class="fa fa-check"></i><b>18.3</b> Visualización</a></li>
<li class="chapter" data-level="18.4" data-path="más-allá.html"><a href="más-allá.html#aplicaciones-web"><i class="fa fa-check"></i><b>18.4</b> Aplicaciones web</a></li>
<li class="chapter" data-level="18.5" data-path="más-allá.html"><a href="más-allá.html#diseño-experimental"><i class="fa fa-check"></i><b>18.5</b> Diseño experimental</a></li>
<li class="chapter" data-level="18.6" data-path="más-allá.html"><a href="más-allá.html#aprendizaje-automático-machine-learning"><i class="fa fa-check"></i><b>18.6</b> Aprendizaje automático (Machine Learning)</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="más-allá.html"><a href="más-allá.html#aprendizaje-profundo-deep-learning"><i class="fa fa-check"></i><b>18.6.1</b> Aprendizaje profundo (Deep Learning)</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="más-allá.html"><a href="más-allá.html#series-de-tiempo"><i class="fa fa-check"></i><b>18.7</b> Series de tiempo</a></li>
<li class="chapter" data-level="18.8" data-path="más-allá.html"><a href="más-allá.html#bayesiana"><i class="fa fa-check"></i><b>18.8</b> Bayesiana</a></li>
<li class="chapter" data-level="18.9" data-path="más-allá.html"><a href="más-allá.html#computación-de-alto-rendimiento."><i class="fa fa-check"></i><b>18.9</b> Computación de alto rendimiento.</a></li>
<li class="chapter" data-level="18.10" data-path="más-allá.html"><a href="más-allá.html#recursos-adicionales-de-r"><i class="fa fa-check"></i><b>18.10</b> Recursos adicionales de <code>R</code></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/daviddalpiaz/appliedstats" target="blank">&copy; 2020 Adapatado de David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Estadística aplicada con R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresión-logística" class="section level1" number="17">
<h1><span class="header-section-number">Capítulo 17</span> Regresión logística</h1>
<p>Después de leer este capítulo, podrá:</p>
<ul>
<li>Comprender cómo los modelos lineales generalizados son una generalización de los modelos lineales ordinarios.</li>
<li>Utilice la regresión logística para modelar una respuesta binaria.</li>
<li>Aplicar conceptos aprendidos para modelos lineales ordinarios a la regresión logística.</li>
<li>Utilizar regresión logística para realizar la clasificación.</li>
</ul>
<p>Hasta ahora solo hemos considerado modelos para variables de respuesta numérica. ¿Qué pasa con las variables respuesta que solo toman valores enteros? ¿Qué pasa con una variable respuesta que es categórica? ¿Podemos utilizar modelos lineales en estas situaciones? ¡Sí! El modelo que hemos estado usando, al que llamaremos <em>regresión lineal ordinaria</em>, es en realidad un caso específico del <em>modelo lineal generalizado</em> más general. (¿No son buenos los estadísticos para nombrar cosas?)</p>
<div id="modelos-lineales-generalizados" class="section level2" number="17.1">
<h2><span class="header-section-number">17.1</span> Modelos lineales generalizados</h2>
<p>Hasta ahora, hemos tenido variables respuesta que, condicionadas por los predictores, se modelaron utilizando una distribución normal con una media que es una combinación lineal de los predictores. Esta combinación lineal es lo que hace que un modelo lineal sea “lineal”.</p>
<p><span class="math display">\[
Y \mid {\bf X} = {\bf x} \sim N(\beta_0 + \beta_1x_1 + \ldots + \beta_{p - 1}x_{p - 1}, \ \sigma^2)
\]</span></p>
<p>Ahora permitiremos dos modificaciones de esta situación, lo que nos permitirá usar modelos lineales en muchas más situaciones. En lugar de utilizar una distribución normal para la respuesta condicionada a los predictores, permitiremos otras distribuciones. Además, en lugar de que la media condicional sea una combinación lineal de los predictores, puede ser alguna función de una combinación lineal de los predictores.</p>
<p>En <em>general</em>, un modelo lineal generalizado tiene tres partes:</p>
<ul>
<li>Una <strong>distribución</strong> de la respuesta condicionada a los predictores. (Técnicamente, esta distribución debe ser de la <a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank">familia exponencial</a>).</li>
<li>Una <strong>combinación lineal</strong> de los predictores <span class="math inline">\(p - 1\)</span>, <span class="math inline">\(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_{p - 1} x_{p - 1}\)</span>, que escribimos como <span class="math inline">\(\eta({\bf x})\)</span>. Es decir,</li>
</ul>
<p><span class="math display">\[\eta({\bf x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots  + \beta_{p - 1} x_{p - 1}\]</span></p>
<ul>
<li>Una función <strong>enlace</strong>, <span class="math inline">\(g()\)</span>, que define cómo $({}) $, la combinación lineal de los predictores, se relaciona con la media de la respuesta condicionada a los predictores, <span class="math inline">\(\text{E}[Y \mid {\bf X} = {\bf x}]\)</span>.
<span class="math display">\[
\eta({\bf x}) = g\left(\text{E}[Y \mid {\bf X} = {\bf x}]\right).
\]</span></li>
</ul>
<p>La siguiente tabla resume tres ejemplos de un modelo lineal generalizado:</p>
<table>
<colgroup>
<col width="22%" />
<col width="23%" />
<col width="26%" />
<col width="27%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Regresión lineal</th>
<th>Regresión Poisson</th>
<th>Regresión logística</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y \mid {\bf X} = {\bf x}\)</span></td>
<td><span class="math inline">\(N(\mu({\bf x}), \sigma^2)\)</span></td>
<td><span class="math inline">\(\text{Pois}(\lambda({\bf x}))\)</span></td>
<td><span class="math inline">\(\text{Bern}(p({\bf x}))\)</span></td>
</tr>
<tr class="even">
<td><strong>Nombre de la distribución</strong></td>
<td>Normal</td>
<td>Poisson</td>
<td>Bernoulli (Binomial)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\text{E}[Y \mid {\bf X} = {\bf x}]\)</span></td>
<td><span class="math inline">\(\mu({\bf x})\)</span></td>
<td><span class="math inline">\(\lambda({\bf x})\)</span></td>
<td><span class="math inline">\(p({\bf x})\)</span></td>
</tr>
<tr class="even">
<td><strong>Soporte</strong></td>
<td>Real: <span class="math inline">\((-\infty, \infty)\)</span></td>
<td>Entero: <span class="math inline">\(0, 1, 2, \ldots\)</span></td>
<td>Entero: <span class="math inline">\(0, 1\)</span></td>
</tr>
<tr class="odd">
<td><strong>Uso</strong></td>
<td>Datos numéricos</td>
<td>Datos de conteos (entero)</td>
<td>Datos binarios</td>
</tr>
<tr class="even">
<td><strong>Nombre del enlace</strong></td>
<td>Identity</td>
<td>Log</td>
<td>Logit</td>
</tr>
<tr class="odd">
<td><strong>Función de enlace</strong></td>
<td><span class="math inline">\(\eta({\bf x}) = \mu({\bf x})\)</span></td>
<td><span class="math inline">\(\eta({\bf x}) = \log(\lambda({\bf x}))\)</span></td>
<td><span class="math inline">\(\eta({\bf x}) = \log \left(\frac{p({\bf x})}{1 - p({\bf x})} \right)\)</span></td>
</tr>
<tr class="even">
<td><strong>Función de la media</strong></td>
<td><span class="math inline">\(\mu({\bf x}) = \eta({\bf x})\)</span></td>
<td><span class="math inline">\(\lambda({\bf x}) = e^{\eta({\bf x})}\)</span></td>
<td><span class="math inline">\(p({\bf x}) = \frac{e^{\eta({\bf x})}}{1 + e^{\eta({\bf x})}} = \frac{1}{1 + e^{-\eta({\bf x})}}\)</span></td>
</tr>
</tbody>
</table>
<p>Como en la regresión lineal ordinaria, buscaremos “ajustar” el modelo estimando los parámetros <span class="math inline">\(\beta\)</span>. Para ello utilizaremos el método de máxima verosimilitud.</p>
<p>Tenga en cuenta que una distribución Bernoulli es un caso específico de una distribución binomial donde el parámetro <span class="math inline">\(n\)</span> de un binomio es <span class="math inline">\(1\)</span>. La regresión binomial también es posible, pero nos centraremos en el caso Bernoulli, mucho más popular.</p>
<p>Entonces, en general, los GLM relacionan la media de la respuesta con una combinación lineal de predictores, <span class="math inline">\(\eta({\bf x})\)</span>, mediante el uso de una función de enlace, <span class="math inline">\(g()\)</span>. Es decir,</p>
<p><span class="math display">\[
\eta({\bf x}) = g\left(\text{E}[Y \mid {\bf X} = {\bf x}]\right).
\]</span></p>
<p>La media es entonces</p>
<p><span class="math display">\[
\text{E}[Y \mid {\bf X} = {\bf x}] = g^{-1}(\eta({\bf x})).
\]</span></p>
</div>
<div id="respuesta-binaria" class="section level2" number="17.2">
<h2><span class="header-section-number">17.2</span> Respuesta binaria</h2>
<p>Para ilustrar el uso de un GLM, nos centraremos en el caso de la variable respuestas binaria codificada con <span class="math inline">\(0\)</span> y <span class="math inline">\(1\)</span>. En la práctica, estos <span class="math inline">\(0\)</span> y <span class="math inline">\(1\)</span> codificarán para dos clases como sí/no, gato/perro, enfermo/sano, etc.</p>
<p><span class="math display">\[
Y = 
\begin{cases} 
      1 &amp; \text{Si} \\
      0 &amp; \text{No} 
\end{cases}
\]</span></p>
<p>Primero, definimos una notación que usaremos en todo momento.</p>
<p><span class="math display">\[
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}]
\]</span></p>
<p>Con una respuesta binaria (Bernoulli), nos centraremos principalmente en el caso cuando <span class="math inline">\(Y = 1\)</span>, ya que con solo dos posibilidades, es trivial obtener probabilidades cuando <span class="math inline">\(Y = 0\)</span>.</p>
<p><span class="math display">\[
P[Y = 0 \mid {\bf X} = {\bf x}] + P[Y = 1 \mid {\bf X} = {\bf x}] = 1
\]</span></p>
<p><span class="math display">\[
P[Y = 0 \mid {\bf X} = {\bf x}] = 1 - p({\bf x})
\]</span></p>
<p>Ahora definimos el modelo de <strong>regresión logística</strong>.</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \ldots  + \beta_{p - 1} x_{p - 1}
\]</span></p>
<p>Inmediatamente notamos algunas similitudes con la regresión lineal ordinaria, en particular, del lado derecho. Esta es nuestra combinación lineal habitual de predictores. Tenemos nuestros predictores habituales <span class="math inline">\(p - 1\)</span> para un total de <span class="math inline">\(p\)</span> parámetros <span class="math inline">\(\beta\)</span>. (Tenga en cuenta que muchos más textos centrados en el aprendizaje automático utilizarán <span class="math inline">\(p\)</span> como número de predictores. Esta es una elección arbitraria, pero debe tenerla en cuenta).</p>
<p>El lado izquierdo se llama <strong>log odds</strong>, que es el log de los odds. Los odds son la probabilidad de un evento positivo <span class="math inline">\((Y = 1)\)</span> dividida por la probabilidad de un evento negativo <span class="math inline">\((Y = 0)\)</span>. Entonces, cuando los odds son <span class="math inline">\(1\)</span>, los dos eventos tienen la misma probabilidad. Los odds superiores a <span class="math inline">\(1\)</span> favorecen un evento positivo. Lo contrario es cierto cuando los odds son inferiores a <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[
\frac{p({\bf x})}{1 - p({\bf x})} = \frac{P[Y = 1 \mid {\bf X} = {\bf x}]}{P[Y = 0 \mid {\bf X} = {\bf x}]}
\]</span></p>
<p>Básicamente, el log de los odds son la transformación <a href="https://en.wikipedia.org/wiki/Logit" target="_blank">logit</a> aplicada a <span class="math inline">\(p({\bf x})\)</span>.</p>
<p><span class="math display">\[
\text{logit}(\xi) = \log\left(\frac{\xi}{1 - \xi}\right)
\]</span></p>
<p>También será útil definir el logit inverso, también conocido como la función “logística” o <a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank">sigmoid</a>.</p>
<p><span class="math display">\[
\text{logit}^{-1}(\xi) = \frac{e^\xi}{1 + e^{\xi}} = \frac{1}{1 + e^{-\xi}}
\]</span></p>
<p>Tenga en cuenta que para <span class="math inline">\(x \in (-\infty, \infty))\)</span>, esta función genera valores entre 0 y 1.</p>
<p>Los estudiantes a menudo preguntan, ¿dónde está el término de error? La respuesta es que es algo específico del modelo normal. Primero observe que el modelo con el término de error,</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1x_1 + \ldots + \beta_qx_q + \epsilon, \ \ \epsilon \sim N(0, \sigma^2)
\]</span></p>
<p>en su lugar se puede escribir como</p>
<p><span class="math display">\[
Y \mid {\bf X} = {\bf x} \sim N(\beta_0 + \beta_1x_1 + \ldots + \beta_qx_q, \ \sigma^2).
\]</span></p>
<p>Si bien nuestro enfoque principal es estimar la media, <span class="math inline">\(\beta_0 + \beta_1x_1 + \ldots + \beta_qx_q\)</span>, también hay otro parámetro, <span class="math inline">\(\sigma^2\)</span> que debe estimarse. Este es el resultado de que la distribución normal tiene dos parámetros.</p>
<p>Con la regresión logística, que usa la distribución de Bernoulli, solo necesitamos estimar el parámetro único de la distribución de Bernoulli <span class="math inline">\(p({\bf x})\)</span>, que resulta ser su media.</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \ldots  + \beta_{q} x_{q}
\]</span></p>
<p>Entonces, aunque introdujimos primero la regresión lineal ordinaria, de alguna manera, la regresión logística es en realidad más simple.</p>
<p>Tenga en cuenta que la aplicación de la transformación logit inversa nos permite obtener una expresión para <span class="math inline">\(p({\bf x})\)</span>.</p>
<p><span class="math display">\[
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}] = \frac{e^{\beta_0 + \beta_1 x_{1} + \cdots + \beta_{p-1} x_{(p-1)}}}{1 + e^{\beta_0 + \beta_1 x_{1} + \cdots + \beta_{p-1} x_{(p-1)}}}
\]</span></p>
<div id="ajuste-de-la-regresión-logística" class="section level3" number="17.2.1">
<h3><span class="header-section-number">17.2.1</span> Ajuste de la regresión logística</h3>
<p>Con <span class="math inline">\(n\)</span> observaciones, escribimos el modelo indexado con <span class="math inline">\(i\)</span> para notar que se está aplicando a cada observación.</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x_i})}{1 - p({\bf x_i)})}\right) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}
\]</span></p>
<p>Podemos aplicar la transformación logit inversa para obtener <span class="math inline">\(P[Y_i = 1 \mid {\bf X_i} = {\bf x_i}]\)</span> para cada observación. Dado que estas son probabilidades, es bueno que usemos una función que devuelve valores entre <span class="math inline">\(0\)</span> y <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[
p({\bf x_i}) = P[Y_i = 1 \mid {\bf X_i} = {\bf x_i}] = \frac{e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}}
\]</span></p>
<p><span class="math display">\[
1 - p({\bf x_i}) = P[Y_i = 0 \mid {\bf X} = {\bf x_i}] = \frac{1}{1 + e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}}
\]</span></p>
<p>Para “ajustar” este modelo, es decir, estimar los parámetros <span class="math inline">\(\beta\)</span>, usaremos la máxima verosimilitud.</p>
<p><span class="math display">\[
\boldsymbol{{\beta}} = [\beta_0, \beta_1, \beta_2, \beta_3, \ldots, \beta_{p - 1}]
\]</span></p>
<p>Primero escribimos la verosimilitud dados los datos observados.</p>
<p><span class="math display">\[
L(\boldsymbol{{\beta}}) = \prod_{i = 1}^{n} P[Y_i = y_i \mid {\bf X_i} = {\bf x_i}]
\]</span></p>
<p>Esto ya es técnicamente una función de los parámetros <span class="math inline">\(\beta\)</span>, pero haremos algunos cambios para que esto sea más explícito.</p>
<p><span class="math display">\[
L(\boldsymbol{{\beta}}) = \prod_{i = 1}^{n} p({\bf x_i})^{y_i} (1 - p({\bf x_i}))^{(1 - y_i)}
\]</span></p>
<p><span class="math display">\[
L(\boldsymbol{{\beta}}) = \prod_{i : y_i = 1}^{n} p({\bf x_i}) \prod_{j : y_j = 0}^{n} (1 - p({\bf x_j}))
\]</span></p>
<p><span class="math display">\[
L(\boldsymbol{{\beta}}) = \prod_{i : y_i = 1}^{} \frac{e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}}{1 + e^{\beta_0 + \beta_1 x_{i1} + \cdots + \beta_{p-1} x_{i(p-1)}}} \prod_{j : y_j = 0}^{} \frac{1}{1 + e^{\beta_0 + \beta_1 x_{j1} + \cdots + \beta_{p-1} x_{j(p-1)}}}
\]</span></p>
<p>Desafortunadamente, a diferencia de la regresión lineal ordinaria, no existe una solución analítica para este problema de maximización. En cambio, deberá resolverse numéricamente. Afortunadamente, <code>R</code> se encargará de esto por nosotros usando un algoritmo de mínimos cuadrados reponderados iterativamente. (Dejaremos los detalles para un curso de optimización o aprendizaje automático, que probablemente también discutirá estrategias de optimización alternativas).</p>
</div>
<div id="problemas-de-ajuste" class="section level3" number="17.2.2">
<h3><span class="header-section-number">17.2.2</span> Problemas de ajuste</h3>
<p>Debemos tener en cuenta que, si existe algún <span class="math inline">\(\beta^*\)</span> tal que</p>
<p><span class="math display">\[
{\bf x_i}^{\top} \boldsymbol{{\beta}^*} &gt; 0 \implies y_i = 1
\]</span></p>
<p>y</p>
<p><span class="math display">\[
{\bf x_i}^{\top} \boldsymbol{{\beta}^*} &lt; 0 \implies y_i = 0
\]</span></p>
<p>para todas las observaciones, el MLE no es único. Se dice que estos datos son separables.</p>
<p>Esto, y otros problemas numéricos similares relacionados con las probabilidades estimadas cercanas a 0 o 1, devolverán una advertencia en <code>R</code>:</p>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<p>Cuando esto sucede, el modelo todavía está “ajustado”, pero hay consecuencias, es decir, los coeficientes estimados son muy sospechosos. Este es un problema al intentar interpretar el modelo. Cuando esto sucede, el modelo a menudo seguirá siendo útil para crear un clasificador, que se discutirá más adelante. Sin embargo, todavía está sujeto a las evaluaciones habituales de los clasificadores para determinar qué tan bien se está desempeñando. Para obtener más información, consulte <a href="https://link.springer.com/content/pdf/10.1007/978-1-4757-2719-7_7.pdf" target="_blank">Modern Applied Statistics with S-PLUS, Chapter 7</a>.</p>
</div>
<div id="ejemplos-de-simulación" class="section level3" number="17.2.3">
<h3><span class="header-section-number">17.2.3</span> Ejemplos de simulación</h3>
<div class="sourceCode" id="cb1420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1420-1"><a href="regresión-logística.html#cb1420-1" aria-hidden="true" tabindex="-1"></a>sim_logistic_data <span class="ot">=</span> <span class="cf">function</span>(<span class="at">sample_size =</span> <span class="dv">25</span>, <span class="at">beta_0 =</span> <span class="sc">-</span><span class="dv">2</span>, <span class="at">beta_1 =</span> <span class="dv">3</span>) {</span>
<span id="cb1420-2"><a href="regresión-logística.html#cb1420-2" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="at">n =</span> sample_size)</span>
<span id="cb1420-3"><a href="regresión-logística.html#cb1420-3" aria-hidden="true" tabindex="-1"></a>  eta <span class="ot">=</span> beta_0 <span class="sc">+</span> beta_1 <span class="sc">*</span> x</span>
<span id="cb1420-4"><a href="regresión-logística.html#cb1420-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>eta))</span>
<span id="cb1420-5"><a href="regresión-logística.html#cb1420-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="at">n =</span> sample_size, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> p)</span>
<span id="cb1420-6"><a href="regresión-logística.html#cb1420-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(y, x)</span>
<span id="cb1420-7"><a href="regresión-logística.html#cb1420-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Podría pensar, ¿por qué no usar simplemente la regresión lineal ordinaria? Incluso con una respuesta binaria, nuestro objetivo sigue siendo modelar (alguna función de) <span class="math inline">\(\text{E}[Y \mid {\bf X} = {\bf x}]\)</span>. Sin embargo, con una respuesta binaria codificada como <span class="math inline">\(0\)</span> y <span class="math inline">\(1\)</span>, <span class="math inline">\(\text{E}[Y \mid {\bf X} = {\bf x}] = P[Y = 1 \mid {\bf X} = {\bf x}]\)</span> desde</p>
<p><span class="math display">\[
\begin{aligned}
\text{E}[Y \mid {\bf X} = {\bf x}] &amp;=  1 \cdot P[Y = 1 \mid {\bf X} = {\bf x}] + 0 \cdot P[Y = 0 \mid {\bf X} = {\bf x}] \\
                                  &amp;= P[Y = 1 \mid {\bf X} = {\bf x}]
\end{aligned}
\]</span></p>
<p>Entonces, ¿por qué no podemos usar la regresión lineal ordinaria para estimar <span class="math inline">\(\text{E}[Y \mid {\bf X} = {\bf x}]\)</span>, y por lo tanto <span class="math inline">\(P[Y = 1 \mid {\bf X} = {\bf x}]\)</span>?</p>
<p>Para investigar, simulemos datos del siguiente modelo:</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = -2 + 3 x
\]</span></p>
<p>Otra forma de escribir esto, que coincide mejor con la función que estamos usando para simular los datos:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i \mid {\bf X_i} = {\bf x_i} &amp;\sim \text{Bern}(p_i) \\
p_i &amp;= p({\bf x_i}) = \frac{1}{1 + e^{-\eta({\bf x_i})}} \\
\eta({\bf x_i}) &amp;= -2 + 3 x_i
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb1421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1421-1"><a href="regresión-logística.html#cb1421-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1421-2"><a href="regresión-logística.html#cb1421-2" aria-hidden="true" tabindex="-1"></a>example_data <span class="ot">=</span> <span class="fu">sim_logistic_data</span>()</span>
<span id="cb1421-3"><a href="regresión-logística.html#cb1421-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(example_data)</span></code></pre></div>
<pre><code>##   y          x
## 1 0 -0.6264538
## 2 1  0.1836433
## 3 0 -0.8356286
## 4 1  1.5952808
## 5 0  0.3295078
## 6 0 -0.8204684</code></pre>
<p>Después de simular un conjunto de datos, ajustaremos tanto la regresión lineal ordinaria como la regresión logística. Observe que actualmente la variable respuesta <code>y</code> es una variable numérica que solo toma los valores <code>0</code> y <code>1</code>. Más adelante veremos que también podemos ajustar la regresión logística cuando la respuesta es una variable factor con solo dos niveles. (Generalmente, se prefiere tener una respuesta factor, pero tener una respuesta ficticia (dummy) permite hacer la comparación con el uso de la regresión lineal ordinaria).</p>
<div class="sourceCode" id="cb1423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1423-1"><a href="regresión-logística.html#cb1423-1" aria-hidden="true" tabindex="-1"></a><span class="co"># regresión lineal ordinaria</span></span>
<span id="cb1423-2"><a href="regresión-logística.html#cb1423-2" aria-hidden="true" tabindex="-1"></a>fit_lm  <span class="ot">=</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data)</span>
<span id="cb1423-3"><a href="regresión-logística.html#cb1423-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Regresión logística</span></span>
<span id="cb1423-4"><a href="regresión-logística.html#cb1423-4" aria-hidden="true" tabindex="-1"></a>fit_glm <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data, <span class="at">family =</span> binomial)</span></code></pre></div>
<p>Observe que la sintaxis es extremadamente similar. ¿Qué ha cambiado?</p>
<ul>
<li><code>lm()</code> se ha convertido en <code>glm()</code></li>
<li>Hemos agregado el argumento <code>family = binomial</code></li>
</ul>
<p>En muchos sentidos, <code>lm()</code> es solo una versión más específica de <code>glm()</code>. Por ejemplo</p>
<div class="sourceCode" id="cb1424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1424-1"><a href="regresión-logística.html#cb1424-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data)</span></code></pre></div>
<p>en realidad ajustaría la regresión lineal ordinaria que hemos visto en el pasado. Por defecto, <code>glm()</code> usa el argumento <code>family = gaussian</code>. Es decir, estamos ajustando un GLM con una respuesta normalmente distribuida y la función de identidad como enlace.</p>
<p>El argumento <code>family</code> para <code>glm()</code> en realidad especifica tanto la distribución como la función de enlace. Si no se hace explícita, la función de enlace se elige para que sea la <strong>función de enlace canónica</strong>, que es esencialmente la función de enlace matemática más conveniente. Consulte <code>?Glm</code> y <code>?family</code> para obtener más detalles. Por ejemplo, el siguiente código especifica explícitamente la función de enlace que se utilizó anteriormente de forma predeterminada.</p>
<div class="sourceCode" id="cb1425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1425-1"><a href="regresión-logística.html#cb1425-1" aria-hidden="true" tabindex="-1"></a><span class="co"># llamada más detallada a glm para regresión logística</span></span>
<span id="cb1425-2"><a href="regresión-logística.html#cb1425-2" aria-hidden="true" tabindex="-1"></a>fit_glm <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span></code></pre></div>
<p>Hacer predicciones con un objeto de tipo <code>glm</code> es ligeramente diferente a hacer predicciones después de ajustar con <code>lm()</code>. En el caso de la regresión logística, con <code>family = binomial</code>, tenemos:</p>
<table>
<colgroup>
<col width="66%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th><code>type</code></th>
<th>Devuelve</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>"link"</code> [por defecto]</td>
<td><span class="math inline">\(\hat{\eta}({\bf x}) = \log\left(\frac{\hat{p}({\bf x})}{1 - \hat{p}({\bf x})}\right)\)</span></td>
</tr>
<tr class="even">
<td><code>"response"</code></td>
<td><span class="math inline">\(\hat{p}({\bf x}) = \frac{e^{\hat{\eta}({\bf x})}}{1 + e^{\hat{\eta}({\bf x})}} = \frac{1}{1 + e^{-\hat{\eta}({\bf x})}}\)</span></td>
</tr>
</tbody>
</table>
<p>Es decir, <code>type = "link"</code> obtendrá el log odds, mientras que <code>type = "response"</code> devolverá la media estimada, en este caso, <span class="math inline">\(P[Y = 1 \mid {\bf X} = {\bf x}]\)</span> para cada observación.</p>
<div class="sourceCode" id="cb1426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1426-1"><a href="regresión-logística.html#cb1426-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data, </span>
<span id="cb1426-2"><a href="regresión-logística.html#cb1426-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">ylab =</span> <span class="st">&quot;Probabilidad estimada&quot;</span>, </span>
<span id="cb1426-3"><a href="regresión-logística.html#cb1426-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Regresión ordinaria vs logística&quot;</span>)</span>
<span id="cb1426-4"><a href="regresión-logística.html#cb1426-4" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb1426-5"><a href="regresión-logística.html#cb1426-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(fit_lm, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb1426-6"><a href="regresión-logística.html#cb1426-6" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">predict</span>(fit_glm, <span class="fu">data.frame</span>(x), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>), </span>
<span id="cb1426-7"><a href="regresión-logística.html#cb1426-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb1426-8"><a href="regresión-logística.html#cb1426-8" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Ordinaria&quot;</span>, <span class="st">&quot;Logística&quot;</span>, <span class="st">&quot;Datos&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>), </span>
<span id="cb1426-9"><a href="regresión-logística.html#cb1426-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">20</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="EstadisticaR_files/figure-html/unnamed-chunk-660-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Dado que solo tenemos una única variable predictora, podemos mostrar gráficamente esta situación. Primero, tenga en cuenta que los datos se grafican utilizando puntos negros. La respuesta <code>y</code> solo toma los valores <code>0</code> y <code>1</code>.</p>
<p>A continuación, necesitamos discutir las dos líneas agregadas a la gráfica. La primera, la línea naranja sólida, es la regresión lineal ordinaria ajustada.</p>
<p>La curva azul punteada es la regresión logística estimada. Es útil darse cuenta de que no estamos trazando una estimación de <span class="math inline">\(Y\)</span> para ninguno de los dos. (A veces puede parecer así con la regresión lineal ordinaria, pero eso no es lo que está sucediendo). Para ambos, estamos trazando <span class="math inline">\(\hat{\text{E}}[Y \mid {\bf X} = {\bf x}]\)</span>, la media estimada, que para una respuesta binaria resulta ser una estimación de <span class="math inline">\(P[Y = 1 \mid {\bf X} = {\bf x}]\)</span>.</p>
<p>Vemos inmediatamente por qué la regresión lineal ordinaria no es una buena idea. Mientras estima la media, vemos que produce estimaciones que son menores que 0. (¡Y en otras situaciones podría producir estimaciones mayores que 1!) Si la media es una probabilidad, no queremos probabilidades menores que 0 o mayores que 1.</p>
<p>Introduzca regresión logística. Dado que la salida de la función logit inversa está restringida a estar entre 0 y 1, nuestras estimaciones tienen mucho más sentido como probabilidades. Veamos nuestros coeficientes estimados. (Con mucho redondeo, por simplicidad).</p>
<div class="sourceCode" id="cb1427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1427-1"><a href="regresión-logística.html#cb1427-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">coef</span>(fit_glm), <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## (Intercept)           x 
##        -2.3         3.7</code></pre>
<p>Nuestro modelo estimado es entonces:</p>
<p><span class="math display">\[
\log\left(\frac{\hat{p}({\bf x})}{1 - \hat{p}({\bf x})}\right) = -2.3 + 3.7 x
\]</span></p>
<p>Debido a que no estamos estimando directamente la media, sino una función de la media, debemos tener cuidado con nuestra interpretación de <span class="math inline">\(\hat{\beta}_1 = 3.7\)</span>. Esto significa que, para un aumento de una unidad en <span class="math inline">\(x\)</span>, los log odds cambian (en este caso aumentan) en <span class="math inline">\(3.7\)</span>. Además, dado que <span class="math inline">\(\hat{\beta}_1\)</span> es positivo, a medida que aumentamos <span class="math inline">\(x\)</span>, también aumentamos <span class="math inline">\(\hat{p}({\bf x})\)</span>. Para ver cuánto, tenemos que considerar la función logística inversa.</p>
<p>Por ejemplo, tenemos:</p>
<p><span class="math display">\[
\hat{P}[Y = 1 \mid X = -0.5] = \frac{e^{-2.3 + 3.7 \cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \cdot (-0.5)}} \approx 0.016
\]</span></p>
<p><span class="math display">\[
\hat{P}[Y = 1 \mid X = 0] = \frac{e^{-2.3 + 3.7 \cdot (0)}}{1 + e^{-2.3 + 3.7 \cdot (0)}} \approx 0.09112296
\]</span></p>
<p><span class="math display">\[
\hat{P}[Y = 1 \mid X = 1] = \frac{e^{-2.3 + 3.7 \cdot (1)}}{1 + e^{-2.3 + 3.7 \cdot (1)}} \approx 0.8021839
\]</span></p>
<p>Ahora que sabemos que debemos usar la regresión logística y no la regresión lineal ordinaria, consideremos otro ejemplo. Esta vez, consideremos el modelo</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = 1 + -4 x.
\]</span></p>
<p>Nuevamente, podríamos reescribir esto para que coincida mejor con la función que estamos usando para simular los datos:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i \mid {\bf X_i} = {\bf x_i} &amp;\sim \text{Bern}(p_i) \\
p_i &amp;= p({\bf x_i}) = \frac{1}{1 + e^{-\eta({\bf x_i})}} \\
\eta({\bf x_i}) &amp;= 1 + -4 x_i
\end{aligned}
\]</span></p>
<p>En este modelo, a medida que aumenta <span class="math inline">\(x\)</span>, las probabilidades logarítmicas disminuyen.</p>
<div class="sourceCode" id="cb1429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1429-1"><a href="regresión-logística.html#cb1429-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1429-2"><a href="regresión-logística.html#cb1429-2" aria-hidden="true" tabindex="-1"></a>example_data <span class="ot">=</span> <span class="fu">sim_logistic_data</span>(<span class="at">sample_size =</span> <span class="dv">50</span>, <span class="at">beta_0 =</span> <span class="dv">1</span>, <span class="at">beta_1 =</span> <span class="sc">-</span><span class="dv">4</span>)</span></code></pre></div>
<p>Simulamos nuevamente algunas observaciones de este modelo, luego ajustamos la regresión logística.</p>
<div class="sourceCode" id="cb1430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1430-1"><a href="regresión-logística.html#cb1430-1" aria-hidden="true" tabindex="-1"></a>fit_glm <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data, <span class="at">family =</span> binomial)</span></code></pre></div>
<div class="sourceCode" id="cb1431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1431-1"><a href="regresión-logística.html#cb1431-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data, </span>
<span id="cb1431-2"><a href="regresión-logística.html#cb1431-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">ylab =</span> <span class="st">&quot;Probabilidad estimada&quot;</span>, </span>
<span id="cb1431-3"><a href="regresión-logística.html#cb1431-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Regresión logística, probabilidad decreciente&quot;</span>)</span>
<span id="cb1431-4"><a href="regresión-logística.html#cb1431-4" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb1431-5"><a href="regresión-logística.html#cb1431-5" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">predict</span>(fit_glm, <span class="fu">data.frame</span>(x), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>), </span>
<span id="cb1431-6"><a href="regresión-logística.html#cb1431-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb1431-7"><a href="regresión-logística.html#cb1431-7" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(boot<span class="sc">::</span><span class="fu">inv.logit</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="dv">4</span> <span class="sc">*</span> x), <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb1431-8"><a href="regresión-logística.html#cb1431-8" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Probabilidad verdadera&quot;</span>, <span class="st">&quot;Probabilidad estimada&quot;</span>, <span class="st">&quot;Datos&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>), </span>
<span id="cb1431-9"><a href="regresión-logística.html#cb1431-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">20</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="EstadisticaR_files/figure-html/unnamed-chunk-664-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Vemos que esta vez, a medida que <span class="math inline">\(x\)</span> aumenta, <span class="math inline">\(\hat{p}({\bf x})\)</span> disminuye.</p>
<p>Ahora veamos un ejemplo en el que la probabilidad estimada no siempre simplemente aumenta o disminuye. Al igual que la regresión lineal ordinaria, la combinación lineal de predictores puede contener transformaciones de predictores (en este caso, un término cuadrático) e interacciones.</p>
<div class="sourceCode" id="cb1432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1432-1"><a href="regresión-logística.html#cb1432-1" aria-hidden="true" tabindex="-1"></a>sim_quadratic_logistic_data <span class="ot">=</span> <span class="cf">function</span>(<span class="at">sample_size =</span> <span class="dv">25</span>) {</span>
<span id="cb1432-2"><a href="regresión-logística.html#cb1432-2" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="at">n =</span> sample_size)</span>
<span id="cb1432-3"><a href="regresión-logística.html#cb1432-3" aria-hidden="true" tabindex="-1"></a>  eta <span class="ot">=</span> <span class="sc">-</span><span class="fl">1.5</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x <span class="sc">+</span> x <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb1432-4"><a href="regresión-logística.html#cb1432-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>eta))</span>
<span id="cb1432-5"><a href="regresión-logística.html#cb1432-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="at">n =</span> sample_size, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> p)</span>
<span id="cb1432-6"><a href="regresión-logística.html#cb1432-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(y, x)</span>
<span id="cb1432-7"><a href="regresión-logística.html#cb1432-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = -1.5 + 0.5x + x^2.
\]</span></p>
<p>Nuevamente, podríamos reescribir esto para que coincida mejor con la función que estamos usando para simular los datos:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i \mid {\bf X_i} = {\bf x_i} &amp;\sim \text{Bern}(p_i) \\
p_i &amp;= p({\bf x_i}) = \frac{1}{1 + e^{-\eta({\bf x_i})}} \\
\eta({\bf x_i}) &amp;= -1.5 + 0.5x_i + x_i^2
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb1433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1433-1"><a href="regresión-logística.html#cb1433-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb1433-2"><a href="regresión-logística.html#cb1433-2" aria-hidden="true" tabindex="-1"></a>example_data <span class="ot">=</span> <span class="fu">sim_quadratic_logistic_data</span>(<span class="at">sample_size =</span> <span class="dv">50</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1434-1"><a href="regresión-logística.html#cb1434-1" aria-hidden="true" tabindex="-1"></a>fit_glm <span class="ot">=</span> <span class="fu">glm</span>(y <span class="sc">~</span> x <span class="sc">+</span> <span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> example_data, <span class="at">family =</span> binomial)</span></code></pre></div>
<div class="sourceCode" id="cb1435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1435-1"><a href="regresión-logística.html#cb1435-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y <span class="sc">~</span> x, <span class="at">data =</span> example_data, </span>
<span id="cb1435-2"><a href="regresión-logística.html#cb1435-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">ylab =</span> <span class="st">&quot;Probabilidad estimada&quot;</span>, </span>
<span id="cb1435-3"><a href="regresión-logística.html#cb1435-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Regresión logística, relación cuadrática&quot;</span>)</span>
<span id="cb1435-4"><a href="regresión-logística.html#cb1435-4" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb1435-5"><a href="regresión-logística.html#cb1435-5" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">predict</span>(fit_glm, <span class="fu">data.frame</span>(x), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>), </span>
<span id="cb1435-6"><a href="regresión-logística.html#cb1435-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb1435-7"><a href="regresión-logística.html#cb1435-7" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(boot<span class="sc">::</span><span class="fu">inv.logit</span>(<span class="sc">-</span><span class="fl">1.5</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x <span class="sc">+</span> x <span class="sc">^</span> <span class="dv">2</span>), </span>
<span id="cb1435-8"><a href="regresión-logística.html#cb1435-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb1435-9"><a href="regresión-logística.html#cb1435-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Probabilidad verdadera&quot;</span>, <span class="st">&quot;Probabilidad estimada&quot;</span>, <span class="st">&quot;Datos&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>), </span>
<span id="cb1435-10"><a href="regresión-logística.html#cb1435-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">20</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="EstadisticaR_files/figure-html/unnamed-chunk-668-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="trabajar-con-regresión-logística" class="section level2" number="17.3">
<h2><span class="header-section-number">17.3</span> Trabajar con regresión logística</h2>
<p>Si bien el modelo de regresión logística no es exactamente el mismo que el modelo de regresión lineal ordinario, porque ambos usan una combinación <strong>lineal</strong> de los predictores</p>
<p><span class="math display">\[
\eta({\bf x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots  + \beta_{p - 1} x_{p - 1}
\]</span></p>
<p>trabajar con regresión logística es muy similar. Muchas de las cosas que hicimos con la regresión lineal ordinaria se pueden hacer con la regresión logística de una manera muy similar. Por ejemplo,</p>
<ul>
<li>Prueba de un solo parámetro <span class="math inline">\(\beta\)</span></li>
<li>Prueba de un conjunto de parámetros <span class="math inline">\(\beta\)</span></li>
<li>Especificación de la fórmula en <code>R</code></li>
<li>Interpretación de parámetros y estimaciones</li>
<li>Intervalos de confianza para parámetros</li>
<li>Intervalos de confianza para la respuesta media</li>
<li>Selección de variable</li>
</ul>
<p>Después de una introducción a las nuevas pruebas, demostraremos cada una de ellas con un ejemplo.</p>
<div id="pruebas-con-glms" class="section level3" number="17.3.1">
<h3><span class="header-section-number">17.3.1</span> Pruebas con GLMs</h3>
<p>Al igual que la regresión lineal ordinaria, vamos a querer realizar la prueba de hipótesis. Nuevamente querremos pruebas de un solo parámetro y de múltiples parámetros.</p>
</div>
<div id="prueba-de-wald" class="section level3" number="17.3.2">
<h3><span class="header-section-number">17.3.2</span> Prueba de Wald</h3>
<p>En regresión lineal ordinaria, realizamos la prueba de</p>
<p><span class="math display">\[
H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \neq 0
\]</span></p>
<p>usando una prueba <span class="math inline">\(t\)</span>.</p>
<p>Para el modelo de regresión logística,</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \ldots  + \beta_{p - 1} x_{p - 1}
\]</span></p>
<p>podemos volver a realizar una prueba de</p>
<p><span class="math display">\[
H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \neq 0
\]</span></p>
<p>sin embargo, el estadístico de prueba y su distribución ya no son <span class="math inline">\(t\)</span>. Vemos que el estadístico de prueba toma la misma forma</p>
<p><span class="math display">\[
z = \frac{\hat{\beta}_j - \beta_j}{\text{SE}[\hat{\beta}_j]} \overset{\text{approx}}{\sim} N(0, 1)
\]</span></p>
<p>pero ahora estamos realizando una prueba <span class="math inline">\(z\)</span>, ya que el estadístico de prueba se aproxima a una distribución normal estándar, <em>siempre que tengamos una muestra lo suficientemente grande</em>. (La prueba <span class="math inline">\(t\)</span> para la regresión lineal ordinaria, asumiendo que los supuestos eran correctos, tenía una distribución exacta para cualquier tamaño de muestra).</p>
<p>Omitiremos algunos de los detalles exactos de los cálculos, ya que <code>R</code> obtendrá el error estándar para nosotros. El uso de esta prueba será extremadamente similar a la prueba <span class="math inline">\(t\)</span> para la regresión lineal ordinaria. Básicamente, lo único que cambia es la distribución del estadístico de prueba.</p>
</div>
<div id="prueba-de-razón-de-verosimilitud" class="section level3" number="17.3.3">
<h3><span class="header-section-number">17.3.3</span> Prueba de razón de verosimilitud</h3>
<p>Considere el siguiente modelo <strong>completo</strong>,</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x_i})}{1 - p({\bf x_i})}\right) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{(p-1)} x_{i(p-1)} + \epsilon_i
\]</span></p>
<p>Este modelo tiene <span class="math inline">\(p-1\)</span> predictores, para un total de <span class="math inline">\(p\)</span> parámetros <span class="math inline">\(\beta\)</span>. Denotaremos el MLE de estos parámetros <span class="math inline">\(\beta\)</span> como <span class="math inline">\(\hat{\beta}_{\text{Full}}\)</span></p>
<p>Ahora considere un modelo <strong>nulo</strong> (o <strong>reducido</strong>),</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x_i})}{1 - p({\bf x_i})}\right) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{(q-1)} x_{i(q-1)} + \epsilon_i
\]</span></p>
<p>donde <span class="math inline">\(q&lt;p\)</span>. Este modelo tiene <span class="math inline">\(q-1\)</span> predictores, para un total de <span class="math inline">\(q\)</span> parámetros <span class="math inline">\(\beta\)</span>. Denotaremos el MLE de estos parámetros <span class="math inline">\(\beta\)</span> como <span class="math inline">\(\hat{\beta}_{\text{Null}}\)</span></p>
<p>La diferencia entre estos dos modelos se puede codificar mediante la hipótesis nula de una prueba.</p>
<p><span class="math display">\[
H_0: \beta_q = \beta_{q+1} = \cdots = \beta_{p - 1} = 0.
\]</span></p>
<p>Esto implica que el modelo reducido está anidado dentro del modelo completo.</p>
<p>Luego definimos un estadístico de prueba, <span class="math inline">\(D\)</span>,</p>
<p><span class="math display">\[
D = -2 \log \left( \frac{L(\boldsymbol{\hat{\beta}_{\text{Null}}})} {L(\boldsymbol{\hat{\beta}_{\text{Full}}})} \right) = 2 \log \left( \frac{L(\boldsymbol{\hat{\beta}_{\text{Full}}})} {L(\boldsymbol{\hat{\beta}_{\text{Null}}})} \right) = 2 \left( \ell(\hat{\beta}_{\text{Full}}) - \ell(\hat{\beta}_{\text{Null}})\right)
\]</span></p>
<p>donde <span class="math inline">\(L\)</span> denota la verosimilitud y <span class="math inline">\(\ell\)</span> denota la log-verosimilitud Para una muestra lo suficientemente grande, este estadístico de prueba tiene una distribución Chi-cuadrado aproximada</p>
<p><span class="math display">\[
D \overset{\text{approx}}{\sim} \chi^2_{k}
\]</span></p>
<p>donde <span class="math inline">\(k = p - q\)</span>, la diferencia en el número de parámetros de los dos modelos.</p>
<p>Esta prueba, que llamaremos <strong>Prueba de razón de verosimilitud</strong>, será análoga a la prueba ANOVA <span class="math inline">\(F\)</span> para regresión logística. Curiosamente, para realizar la prueba de razón de verosimilitud, en realidad usaremos nuevamente la función <code>anova()</code> en <code>R</code> !.</p>
<p>La prueba de razón de verosimilitud es en realidad una prueba bastante general, sin embargo, aquí hemos presentado una aplicación específica a los modelos de regresión logística anidados.</p>
</div>
<div id="ejemplo-saheart" class="section level3" number="17.3.4">
<h3><span class="header-section-number">17.3.4</span> Ejemplo <code>SAheart</code></h3>
<p>Para ilustrar el uso de la regresión logística, usaremos el conjunto de datos <code>SAheart</code> del paquete <code>ElemStatLearn</code>.</p>
<div class="sourceCode" id="cb1436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1436-1"><a href="regresión-logística.html#cb1436-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;bestglm&quot;)</span></span>
<span id="cb1436-2"><a href="regresión-logística.html#cb1436-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bestglm)</span>
<span id="cb1436-3"><a href="regresión-logística.html#cb1436-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;SAheart&quot;</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">sbp</th>
<th align="right">tobacco</th>
<th align="right">ldl</th>
<th align="right">adiposity</th>
<th align="left">famhist</th>
<th align="right">typea</th>
<th align="right">obesity</th>
<th align="right">alcohol</th>
<th align="right">age</th>
<th align="right">chd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">160</td>
<td align="right">12.00</td>
<td align="right">5.73</td>
<td align="right">23.11</td>
<td align="left">Present</td>
<td align="right">49</td>
<td align="right">25.30</td>
<td align="right">97.20</td>
<td align="right">52</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">144</td>
<td align="right">0.01</td>
<td align="right">4.41</td>
<td align="right">28.61</td>
<td align="left">Absent</td>
<td align="right">55</td>
<td align="right">28.87</td>
<td align="right">2.06</td>
<td align="right">63</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">118</td>
<td align="right">0.08</td>
<td align="right">3.48</td>
<td align="right">32.28</td>
<td align="left">Present</td>
<td align="right">52</td>
<td align="right">29.14</td>
<td align="right">3.81</td>
<td align="right">46</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">170</td>
<td align="right">7.50</td>
<td align="right">6.41</td>
<td align="right">38.03</td>
<td align="left">Present</td>
<td align="right">51</td>
<td align="right">31.99</td>
<td align="right">24.26</td>
<td align="right">58</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">134</td>
<td align="right">13.60</td>
<td align="right">3.50</td>
<td align="right">27.78</td>
<td align="left">Present</td>
<td align="right">60</td>
<td align="right">25.99</td>
<td align="right">57.34</td>
<td align="right">49</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">132</td>
<td align="right">6.20</td>
<td align="right">6.47</td>
<td align="right">36.21</td>
<td align="left">Present</td>
<td align="right">62</td>
<td align="right">30.77</td>
<td align="right">14.14</td>
<td align="right">45</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Estos datos provienen de una muestra retrospectiva de hombres en una región de alto riesgo de enfermedades cardíacas en Western Cape, Sudáfrica. La variable <code>chd</code>, que usaremos como respuesta, indica si la enfermedad coronaria está presente en un individuo o no. Tenga en cuenta que esto está codificado como una variable numérica <code>0</code> / <code>1</code>. Usando esto como respuesta con <code>glm()</code> es importante indicar <code>family = binomial</code>, de lo contrario se ajustará la regresión lineal ordinaria. Más adelante, veremos el uso de una respuesta de variable de factor, que en realidad se prefiere, ya que no se puede ajustar la regresión lineal ordinaria accidentalmente.</p>
<p>Los predictores son varias medidas para cada individuo, muchas de ellas relacionadas con la salud del corazón. Por ejemplo, <code>sbp</code>, presión arterial sistólica y <code>ldl</code>, colesterol unido a lipoproteínas de baja densidad. Para obtener todos los detalles, utilice <code>?SAheart</code>.</p>
<p>Comenzaremos por intentar modelar la probabilidad de enfermedad coronaria con base en el colesterol de lipoproteínas de baja densidad. Es decir, nos ajustaremos al modelo.</p>
<p><span class="math display">\[
\log\left(\frac{P[\texttt{chd} = 1]}{1 - P[\texttt{chd} = 1]}\right) = \beta_0 + \beta_{\texttt{ldl}} x_{\texttt{ldl}}
\]</span></p>
<div class="sourceCode" id="cb1437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1437-1"><a href="regresión-logística.html#cb1437-1" aria-hidden="true" tabindex="-1"></a>chd_mod_ldl <span class="ot">=</span> <span class="fu">glm</span>(chd <span class="sc">~</span> ldl, <span class="at">data =</span> SAheart, <span class="at">family =</span> binomial)</span>
<span id="cb1437-2"><a href="regresión-logística.html#cb1437-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">jitter</span>(chd, <span class="at">factor =</span> <span class="fl">0.1</span>) <span class="sc">~</span> ldl, <span class="at">data =</span> SAheart, <span class="at">pch =</span> <span class="dv">20</span>, </span>
<span id="cb1437-3"><a href="regresión-logística.html#cb1437-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Probabilidad de CHD&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Colesterol de lipoproteínas de baja densidad&quot;</span>)</span>
<span id="cb1437-4"><a href="regresión-logística.html#cb1437-4" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb1437-5"><a href="regresión-logística.html#cb1437-5" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">predict</span>(chd_mod_ldl, <span class="fu">data.frame</span>(<span class="at">ldl =</span> x), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>), </span>
<span id="cb1437-6"><a href="regresión-logística.html#cb1437-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="EstadisticaR_files/figure-html/unnamed-chunk-671-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Como antes, graficamos los datos además de las probabilidades estimadas. Tenga en cuenta que hemos “alterado” los datos para que sean más fáciles de visualizar, pero los datos solo toman valores <code>0</code> y <code>1</code>.</p>
<p>Como era de esperar, este gráfico indica que a medida que aumenta <code>ldl</code>, también lo hace la probabilidad de <code>chd</code>.</p>
<div class="sourceCode" id="cb1438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1438-1"><a href="regresión-logística.html#cb1438-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(chd_mod_ldl))</span></code></pre></div>
<pre><code>##               Estimate Std. Error   z value     Pr(&gt;|z|)
## (Intercept) -1.9686681 0.27307908 -7.209150 5.630207e-13
## ldl          0.2746613 0.05163983  5.318787 1.044615e-07</code></pre>
<p>Para realizar la prueba</p>
<p><span class="math display">\[
H_0: \beta_{\texttt{ldl}} = 0
\]</span></p>
<p>usamos la función <code>summary()</code> como lo hemos hecho tantas veces antes. Al igual que la prueba <span class="math inline">\(t\)</span> para la regresión lineal ordinaria, devuelve la estimación del parámetro, su error estándar, el estadístico de prueba relevante (<span class="math inline">\(z\)</span>) y su valor p. Aquí tenemos un valor p increíblemente bajo, por lo que rechazamos la hipótesis nula. La variable <code>ldl</code> parece ser un predictor significativo.</p>
<p>Al ajustar la regresión logística, podemos usar la misma sintaxis de fórmula que la regresión lineal ordinaria. Entonces, para ajustar un modelo aditivo usando todos los predictores disponibles, usamos:</p>
<div class="sourceCode" id="cb1440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1440-1"><a href="regresión-logística.html#cb1440-1" aria-hidden="true" tabindex="-1"></a>chd_mod_additive <span class="ot">=</span> <span class="fu">glm</span>(chd <span class="sc">~</span> ., <span class="at">data =</span> SAheart, <span class="at">family =</span> binomial)</span></code></pre></div>
<p>Luego, podemos usar la prueba de razón de verosimilitud para comparar los dos modelos. Específicamente, estamos probando</p>
<p><span class="math display">\[
H_0: \beta_{\texttt{sbp}} = \beta_{\texttt{tobacco}} = \beta_{\texttt{adiposity}} = \beta_{\texttt{famhist}} = \beta_{\texttt{typea}} = \beta_{\texttt{obesity}} = \beta_{\texttt{alcohol}} = \beta_{\texttt{age}} = 0
\]</span></p>
<p>Podríamos calcular manualmente el estadístico de prueba,</p>
<div class="sourceCode" id="cb1441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1441-1"><a href="regresión-logística.html#cb1441-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">as.numeric</span>(<span class="fu">logLik</span>(chd_mod_ldl) <span class="sc">-</span> <span class="fu">logLik</span>(chd_mod_additive))</span></code></pre></div>
<pre><code>## [1] 92.13879</code></pre>
<p>O podríamos utilizar la función <code>anova()</code>. Al especificar <code>test = "LRT"</code>, <code>R</code> usará la prueba de razón de verosimilitud para comparar los dos modelos.</p>
<div class="sourceCode" id="cb1443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1443-1"><a href="regresión-logística.html#cb1443-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(chd_mod_ldl, chd_mod_additive, <span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: chd ~ ldl
## Model 2: chd ~ sbp + tobacco + ldl + adiposity + famhist + typea + obesity + 
##     alcohol + age
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1       460     564.28                          
## 2       452     472.14  8   92.139 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Vemos que el estadístico de prueba que acabamos de calcular aparece en la salida. El valor p muy pequeño sugiere que preferimos el modelo más grande.</p>
<p>Si bien preferimos el modelo aditivo en comparación con el modelo de un solo predictor, ¿realmente necesitamos todos los predictores en el modelo aditivo? Para seleccionar un subconjunto de predictores, podemos usar un procedimiento paso a paso como hicimos con la regresión lineal ordinaria. Recuerde que AIC y BIC se definieron en términos de probabilidades. Aquí demostramos el uso de AIC con un procedimiento de selección hacia atrás.</p>
<div class="sourceCode" id="cb1445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1445-1"><a href="regresión-logística.html#cb1445-1" aria-hidden="true" tabindex="-1"></a>chd_mod_selected <span class="ot">=</span> <span class="fu">step</span>(chd_mod_additive, <span class="at">trace =</span> <span class="dv">0</span>)</span>
<span id="cb1445-2"><a href="regresión-logística.html#cb1445-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(chd_mod_selected)</span></code></pre></div>
<pre><code>##    (Intercept)        tobacco            ldl famhistPresent          typea 
##    -6.44644451     0.08037533     0.16199164     0.90817526     0.03711521 
##            age 
##     0.05046038</code></pre>
<p>Podríamos volver a comparar este modelo con los modelos aditivos.</p>
<p><span class="math display">\[
H_0: \beta_{\texttt{sbp}} = \beta_{\texttt{adiposity}} = \beta_{\texttt{obesity}} = \beta_{\texttt{alcohol}} = 0
\]</span></p>
<div class="sourceCode" id="cb1447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1447-1"><a href="regresión-logística.html#cb1447-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(chd_mod_selected, chd_mod_additive, <span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: chd ~ tobacco + ldl + famhist + typea + age
## Model 2: chd ~ sbp + tobacco + ldl + adiposity + famhist + typea + obesity + 
##     alcohol + age
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1       456     475.69                     
## 2       452     472.14  4   3.5455    0.471</code></pre>
<p>Aquí parece que preferiríamos el modelo seleccionado.</p>
</div>
<div id="intervalos-de-confianza-1" class="section level3" number="17.3.5">
<h3><span class="header-section-number">17.3.5</span> Intervalos de confianza</h3>
<p>Podemos crear intervalos de confianza para los parámetros <span class="math inline">\(\beta\)</span> usando la función <code>confint()</code> como hicimos con la regresión lineal ordinaria.</p>
<div class="sourceCode" id="cb1449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1449-1"><a href="regresión-logística.html#cb1449-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(chd_mod_selected, <span class="at">level =</span> <span class="fl">0.99</span>)</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                       0.5 %      99.5 %
## (Intercept)    -8.941825274 -4.18278990
## tobacco         0.015704975  0.14986616
## ldl             0.022923610  0.30784590
## famhistPresent  0.330033483  1.49603366
## typea           0.006408724  0.06932612
## age             0.024847330  0.07764277</code></pre>
<p>Tenga en cuenta que podríamos crear intervalos reordenando los resultados de la prueba de Wald para obtener el intervalo de confianza de Wald. Esto estaría dado por</p>
<p><span class="math display">\[
\hat{\beta}_j \pm z_{\alpha/2} \cdot \text{SE}[\hat{\beta}_j].
\]</span></p>
<p>Sin embargo, <code>R</code> utiliza un enfoque ligeramente diferente basado en un concepto llamado verosimilitud de perfil. (Los detalles los omitiremos). En última instancia, los intervalos informados serán similares, pero el método utilizado por <code>R</code> es más común en la práctica, probablemente al menos parcialmente porque es el enfoque predeterminado en <code>R</code>. Compruebe cómo se comparan los intervalos que utilizan la fórmula anterior con los de la salida de <code>confint()</code>. (O, tenga en cuenta que el uso de <code>confint.default()</code> devolverá los resultados del cálculo del intervalo de confianza de Wald).</p>
</div>
<div id="intervalos-de-confianza-para-la-respuesta-promedio" class="section level3" number="17.3.6">
<h3><span class="header-section-number">17.3.6</span> Intervalos de confianza para la respuesta promedio</h3>
<p>Los intervalos de confianza para la respuesta promedio requieren una reflexión adicional. Con una muestra “suficientemente grande”, tenemos</p>
<p><span class="math display">\[
\frac{\hat{\eta}({\bf x}) - \eta({\bf x})}{\text{SE}[\hat{\eta}({\bf x})]} \overset{\text{approx}}{\sim} N(0, 1)
\]</span></p>
<p>Entonces podemos crear intervalos de confianza aproximados, al <span class="math inline">\((1 - \alpha)\%\)</span> para <span class="math inline">\(\eta({\bf x})\)</span> usando</p>
<p><span class="math display">\[
\hat{\eta}({\bf x}) \pm z_{\alpha/2} \cdot \text{SE}[\hat{\eta}({\bf x})]
\]</span></p>
<p>donde <span class="math inline">\(z_{\alpha/2}\)</span> es el valor crítico tal que <span class="math inline">\(P(Z &gt; z_{\alpha/2}) = \alpha/2\)</span>.</p>
<p>Este no es un intervalo particularmente interesante. En cambio, lo que realmente queremos es un intervalo para la respuesta promedio, <span class="math inline">\(p({\bf x})\)</span>. Para obtener un intervalo para <span class="math inline">\(p({\bf x})\)</span>, simplemente aplicamos la transformación logit inversa a los puntos finales del intervalo para <span class="math inline">\(\eta.\)</span></p>
<p><span class="math display">\[
\left(\text{logit}^{-1}(\hat{\eta}({\bf x}) - z_{\alpha/2} \cdot \text{SE}[\hat{\eta}({\bf x})] ), \ \text{logit}^{-1}(\hat{\eta}({\bf x}) + z_{\alpha/2} \cdot \text{SE}[\hat{\eta}({\bf x})])\right)
\]</span></p>
<p>Para demostrar la creación de estos intervalos, consideraremos una nueva observación.</p>
<div class="sourceCode" id="cb1452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1452-1"><a href="regresión-logística.html#cb1452-1" aria-hidden="true" tabindex="-1"></a>new_obs <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb1452-2"><a href="regresión-logística.html#cb1452-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">sbp =</span> <span class="fl">148.0</span>,</span>
<span id="cb1452-3"><a href="regresión-logística.html#cb1452-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">tobacco =</span> <span class="dv">5</span>,</span>
<span id="cb1452-4"><a href="regresión-logística.html#cb1452-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">ldl =</span> <span class="dv">12</span>,</span>
<span id="cb1452-5"><a href="regresión-logística.html#cb1452-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">adiposity =</span> <span class="fl">31.23</span>,</span>
<span id="cb1452-6"><a href="regresión-logística.html#cb1452-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">famhist =</span> <span class="st">&quot;Present&quot;</span>,</span>
<span id="cb1452-7"><a href="regresión-logística.html#cb1452-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">typea =</span> <span class="dv">47</span>,</span>
<span id="cb1452-8"><a href="regresión-logística.html#cb1452-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">obesity =</span> <span class="fl">28.50</span>,</span>
<span id="cb1452-9"><a href="regresión-logística.html#cb1452-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">alcohol =</span> <span class="fl">23.89</span>,</span>
<span id="cb1452-10"><a href="regresión-logística.html#cb1452-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">age =</span> <span class="dv">60</span></span>
<span id="cb1452-11"><a href="regresión-logística.html#cb1452-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Primero, usaremos la función <code>predict()</code> para obtener <span class="math inline">\(\hat{\eta}({\bf x})\)</span> para esta observación.</p>
<div class="sourceCode" id="cb1453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1453-1"><a href="regresión-logística.html#cb1453-1" aria-hidden="true" tabindex="-1"></a>eta_hat <span class="ot">=</span> <span class="fu">predict</span>(chd_mod_selected, new_obs, <span class="at">se.fit =</span> <span class="cn">TRUE</span>, <span class="at">type =</span> <span class="st">&quot;link&quot;</span>)</span>
<span id="cb1453-2"><a href="regresión-logística.html#cb1453-2" aria-hidden="true" tabindex="-1"></a>eta_hat</span></code></pre></div>
<pre><code>## $fit
##        1 
## 1.579545 
## 
## $se.fit
## [1] 0.4114796
## 
## $residual.scale
## [1] 1</code></pre>
<p>Al establecer <code>se.fit = TRUE</code>, <code>R</code> también calcula <span class="math inline">\(\text{SE}[\hat{\eta}({\bf x})]\)</span>. Tenga en cuenta que usamos <code>type = "link"</code>, pero este es en realidad un valor predeterminado. Lo agregamos para enfatizar que la salida de <code>predict()</code> será el valor de la función de enlace.</p>
<div class="sourceCode" id="cb1455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1455-1"><a href="regresión-logística.html#cb1455-1" aria-hidden="true" tabindex="-1"></a>z_crit <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">qnorm</span>(<span class="fl">0.975</span>), <span class="dv">2</span>)</span>
<span id="cb1455-2"><a href="regresión-logística.html#cb1455-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(z_crit, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1.96</code></pre>
<p>Después de obtener el valor crítico correcto, podemos crear fácilmente un intervalo de confianza al <span class="math inline">\(95\%\)</span> para <span class="math inline">\(\eta({\bf x})\)</span>.</p>
<div class="sourceCode" id="cb1457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1457-1"><a href="regresión-logística.html#cb1457-1" aria-hidden="true" tabindex="-1"></a>eta_hat<span class="sc">$</span>fit <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> z_crit <span class="sc">*</span> eta_hat<span class="sc">$</span>se.fit</span></code></pre></div>
<pre><code>## [1] 0.773045 2.386045</code></pre>
<p>Ahora simplemente necesitamos aplicar la transformación correcta para hacer de este un intervalo de confianza para <span class="math inline">\(p({\bf x})\)</span>, la probabilidad de enfermedad coronaria para esta observación. Tenga en cuenta que el paquete <code>boot</code> contiene las funciones <code>logit()</code> e <code>inv.logit()</code> que son las transformaciones logit y logit inverso, respectivamente.</p>
<div class="sourceCode" id="cb1459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1459-1"><a href="regresión-logística.html#cb1459-1" aria-hidden="true" tabindex="-1"></a>boot<span class="sc">::</span><span class="fu">inv.logit</span>(eta_hat<span class="sc">$</span>fit <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> z_crit <span class="sc">*</span> eta_hat<span class="sc">$</span>se.fit)</span></code></pre></div>
<pre><code>## [1] 0.6841792 0.9157570</code></pre>
<p>Como era de esperar, los límites de este intervalo están entre 0 y 1. Además, dado que ambos límites del intervalo para <span class="math inline">\(\eta({\bf x})\)</span> son positivos, ambos límites del intervalo para <span class="math inline">\(p({\bf x})\)</span> son mayores que 0.5.</p>
</div>
<div id="sintaxis-de-la-fórmula" class="section level3" number="17.3.7">
<h3><span class="header-section-number">17.3.7</span> Sintaxis de la fórmula</h3>
<p>Sin pensarlo realmente, hemos estado usando nuestro conocimiento previo de la sintaxis de la fórmula del modelo de <code>R</code> para ajustar la regresión logística.</p>
<div id="interacciones-1" class="section level4" number="17.3.7.1">
<h4><span class="header-section-number">17.3.7.1</span> Interacciones</h4>
<p>Agreguemos una interacción entre LDL e historia familiar para el modelo que seleccionamos.</p>
<div class="sourceCode" id="cb1461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1461-1"><a href="regresión-logística.html#cb1461-1" aria-hidden="true" tabindex="-1"></a>chd_mod_interaction <span class="ot">=</span> <span class="fu">glm</span>(chd <span class="sc">~</span> alcohol <span class="sc">+</span> ldl <span class="sc">+</span> famhist <span class="sc">+</span> typea <span class="sc">+</span> age <span class="sc">+</span> ldl<span class="sc">:</span>famhist, </span>
<span id="cb1461-2"><a href="regresión-logística.html#cb1461-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> SAheart, <span class="at">family =</span> binomial)</span>
<span id="cb1461-3"><a href="regresión-logística.html#cb1461-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(chd_mod_interaction)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist, 
##     family = binomial, data = SAheart)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9082  -0.8308  -0.4550   0.9286   2.5152  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        -6.043472   0.937186  -6.449 1.13e-10 ***
## alcohol             0.003800   0.004332   0.877  0.38033    
## ldl                 0.035593   0.071448   0.498  0.61837    
## famhistPresent     -0.733836   0.618131  -1.187  0.23515    
## typea               0.036253   0.012172   2.978  0.00290 ** 
## age                 0.062416   0.009723   6.419 1.37e-10 ***
## ldl:famhistPresent  0.314311   0.114922   2.735  0.00624 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 596.11  on 461  degrees of freedom
## Residual deviance: 477.46  on 455  degrees of freedom
## AIC: 491.46
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Según la prueba <span class="math inline">\(z\)</span> vista en el resumen anterior, esta interacción es significativa. El efecto de las LDL sobre la probabilidad de CHD es diferente según los antecedentes familiares.</p>
</div>
<div id="términos-polinomiales" class="section level4" number="17.3.7.2">
<h4><span class="header-section-number">17.3.7.2</span> Términos polinomiales</h4>
<p>Tomemos el modelo anterior y ahora agreguemos un término polinomial.</p>
<div class="sourceCode" id="cb1463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1463-1"><a href="regresión-logística.html#cb1463-1" aria-hidden="true" tabindex="-1"></a>chd_mod_int_quad <span class="ot">=</span> <span class="fu">glm</span>(chd <span class="sc">~</span> alcohol <span class="sc">+</span> ldl <span class="sc">+</span> famhist <span class="sc">+</span> typea <span class="sc">+</span> age <span class="sc">+</span> ldl<span class="sc">:</span>famhist <span class="sc">+</span> <span class="fu">I</span>(ldl<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb1463-2"><a href="regresión-logística.html#cb1463-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data =</span> SAheart, <span class="at">family =</span> binomial)</span>
<span id="cb1463-3"><a href="regresión-logística.html#cb1463-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(chd_mod_int_quad)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist + 
##     I(ldl^2), family = binomial, data = SAheart)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8953  -0.8311  -0.4556   0.9276   2.5204  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        -6.096747   1.065952  -5.720 1.07e-08 ***
## alcohol             0.003842   0.004350   0.883  0.37716    
## ldl                 0.056876   0.214420   0.265  0.79081    
## famhistPresent     -0.723769   0.625167  -1.158  0.24698    
## typea               0.036248   0.012171   2.978  0.00290 ** 
## age                 0.062299   0.009788   6.365 1.95e-10 ***
## I(ldl^2)           -0.001587   0.015076  -0.105  0.91617    
## ldl:famhistPresent  0.311615   0.117559   2.651  0.00803 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 596.11  on 461  degrees of freedom
## Residual deviance: 477.45  on 454  degrees of freedom
## AIC: 493.45
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Como era de esperar, dado que esta variable transformada adicional no se eligió inteligentemente, no es significativa. Sin embargo, esto nos permite enfatizar el hecho de que la notación de sintaxis que habíamos estado usando con <code>lm()</code> funciona básicamente exactamente igual para <code>glm()</code>, sin embargo ahora entendemos que esto está especificando la combinación lineal de predicciones , <span class="math inline">\(\eta({\bf x})\)</span>.</p>
<p>Es decir, lo anterior se ajusta al modelo.</p>
<p><span class="math display">\[
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = 
\beta_0 +
\beta_{1}x_{\texttt{alcohol}} +
\beta_{2}x_{\texttt{ldl}} +
\beta_{3}x_{\texttt{famhist}} +
\beta_{4}x_{\texttt{typea}} +
\beta_{5}x_{\texttt{age}} +
\beta_{6}x_{\texttt{ldl}}x_{\texttt{famhist}} +
\beta_{7}x_{\texttt{ldl}}^2
\]</span></p>
<p>¡Es posible que se haya dado cuenta de esto antes de que lo escribiéramos explícitamente!</p>
</div>
</div>
<div id="desviación" class="section level3" number="17.3.8">
<h3><span class="header-section-number">17.3.8</span> Desviación</h3>
<p>Probablemente haya notado que la salida de <code>summary()</code> también es muy similar a la de la regresión lineal ordinaria. Una diferencia es la “desviación” que se informa. <code>Null deviance</code> es la desviación del modelo nulo, es decir, un modelo sin predictores. La <code>Residual deviance</code> es la desviación del modelo que se ajustó.</p>
<p><a href="https://en.wikipedia.org/wiki/Deviance_(statistics)" target="_blank"><strong>Desviación</strong></a> compara el modelo con un modelo saturado. (Sin observaciones repetidas, un modelo saturado es un modelo que se ajusta perfectamente, utilizando un parámetro para cada observación). Esencialmente, la desviación es una <em>suma de cuadrados residual</em> generalizada para GLM. Al igual que RSS, la desviación disminuye a medida que aumenta la complejidad del modelo.</p>
<div class="sourceCode" id="cb1465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1465-1"><a href="regresión-logística.html#cb1465-1" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(chd_mod_ldl)</span></code></pre></div>
<pre><code>## [1] 564.2788</code></pre>
<div class="sourceCode" id="cb1467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1467-1"><a href="regresión-logística.html#cb1467-1" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(chd_mod_selected)</span></code></pre></div>
<pre><code>## [1] 475.6856</code></pre>
<div class="sourceCode" id="cb1469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1469-1"><a href="regresión-logística.html#cb1469-1" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(chd_mod_additive)</span></code></pre></div>
<pre><code>## [1] 472.14</code></pre>
<p>Tenga en cuenta que estos están anidados y vemos que la desviación disminuye a medida que aumenta el tamaño del modelo. Entonces, si bien una desviación más baja es mejor, si el modelo se vuelve demasiado grande, puede estar sobreajustado. Tenga en cuenta que <code>R</code> también genera AIC en el resumen, que penalizará según el tamaño del modelo, para evitar el sobreajuste.</p>
</div>
</div>
<div id="clasificación" class="section level2" number="17.4">
<h2><span class="header-section-number">17.4</span> Clasificación</h2>
<p>Hasta ahora, hemos utilizado principalmente la regresión logística para estimar las probabilidades de clase. El siguiente paso algo obvio es usar estas probabilidades para hacer “predicciones”, que en este contexto, llamaríamos <strong>clasificaciones</strong>. Según los valores de los predictores, ¿debería clasificarse una observación como <span class="math inline">\(Y=1\)</span> o como <span class="math inline">\(Y=0\)</span>?</p>
<p>Supongamos que no necesitáramos estimar probabilidades a partir de datos y, en cambio, supiéramos ambos</p>
<p><span class="math display">\[
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}]
\]</span></p>
<p>y</p>
<p><span class="math display">\[
1 - p({\bf x}) = P[Y = 0 \mid {\bf X} = {\bf x}].
\]</span></p>
<p>Con esta información, clasificar las observaciones basadas en los valores de los predictores es realmente muy fácil. Simplemente clasifique una observación en la clase (<span class="math inline">\(0\)</span> o <span class="math inline">\(1\)</span>) con mayor probabilidad. En general, este resultado se denomina <strong>Clasificador de Bayes</strong>,</p>
<p><span class="math display">\[
C^B({\bf x}) = \underset{k}{\mathrm{argmax}} \ P[Y = k \mid {\bf X = x}].
\]</span></p>
<p>Para una respuesta binaria, es decir,</p>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; p({\bf x}) &gt; 0.5 \\
      0 &amp; p({\bf x}) \leq 0.5 
\end{cases}
\]</span></p>
<p>En pocas palabras, el clasificador de Bayes (que no debe confundirse con el clasificador Naive Bayes) minimiza la probabilidad de clasificación errónea al clasificar cada observación en la clase con la probabilidad más alta. Desafortunadamente, en la práctica, no conoceremos las probabilidades necesarias para usar directamente el clasificador de Bayes. En su lugar, tendremos que usar probabilidades estimadas. Entonces, para crear un clasificador que busque minimizar las clasificaciones erróneas, usaríamos,</p>
<p><span class="math display">\[
\hat{C}({\bf x}) = \underset{k}{\mathrm{argmax}} \ \hat{P}[Y = k \mid {\bf X = x}].
\]</span></p>
<p>En el caso de una respuesta binaria desde <span class="math inline">\(\hat{p}({\bf x}) = 1 - \hat{p}({\bf x})\)</span>, esto se convierte en</p>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; \hat{p}({\bf x}) &gt; 0.5 \\
      0 &amp; \hat{p}({\bf x}) \leq 0.5 
\end{cases}
\]</span></p>
<p>Usando esta simple regla de clasificación, podemos convertir la regresión logística en un clasificador. Para usarla en la clasificación, primero usamos la regresión logística para obtener probabilidades estimadas, <span class="math inline">\(\hat{p}({\bf x})\)</span>, luego las usamos junto con la regla de clasificación anterior.</p>
<p>La regresión logística es solo una de las muchas formas en que se pueden estimar estas probabilidades. En un curso completamente centrado en el aprendizaje automático, aprenderá muchas formas adicionales de hacer esto, así como métodos para realizar clasificaciones directamente sin necesidad de estimar las probabilidades primero. Pero como ya habíamos introducido la regresión logística, tiene sentido discutirla en el contexto de la clasificación.</p>
<div id="ejemplo-spam" class="section level3" number="17.4.1">
<h3><span class="header-section-number">17.4.1</span> Ejemplo <code>spam</code></h3>
<p>Para ilustrar el uso de la regresión logística como clasificador, usaremos el conjunto de datos <code>spam</code> del paquete <code>kernlab</code>.</p>
<div class="sourceCode" id="cb1471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1471-1"><a href="regresión-logística.html#cb1471-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;kernlab&quot;)</span></span>
<span id="cb1471-2"><a href="regresión-logística.html#cb1471-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;kernlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     alpha</code></pre>
<div class="sourceCode" id="cb1474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1474-1"><a href="regresión-logística.html#cb1474-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;spam&quot;</span>)</span>
<span id="cb1474-2"><a href="regresión-logística.html#cb1474-2" aria-hidden="true" tabindex="-1"></a>tibble<span class="sc">::</span><span class="fu">as.tibble</span>(spam)</span></code></pre></div>
<pre><code>## Warning: `as.tibble()` is deprecated as of tibble 2.0.0.
## Please use `as_tibble()` instead.
## The signature and semantics have changed, see `?as_tibble`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## # A tibble: 4,601 x 58
##     make address   all num3d   our  over remove internet order  mail receive
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1  0       0.64  0.64     0  0.32  0      0        0     0     0       0   
##  2  0.21    0.28  0.5      0  0.14  0.28   0.21     0.07  0     0.94    0.21
##  3  0.06    0     0.71     0  1.23  0.19   0.19     0.12  0.64  0.25    0.38
##  4  0       0     0        0  0.63  0      0.31     0.63  0.31  0.63    0.31
##  5  0       0     0        0  0.63  0      0.31     0.63  0.31  0.63    0.31
##  6  0       0     0        0  1.85  0      0        1.85  0     0       0   
##  7  0       0     0        0  1.92  0      0        0     0     0.64    0.96
##  8  0       0     0        0  1.88  0      0        1.88  0     0       0   
##  9  0.15    0     0.46     0  0.61  0      0.3      0     0.92  0.76    0.76
## 10  0.06    0.12  0.77     0  0.19  0.32   0.38     0     0.06  0       0   
## # ... with 4,591 more rows, and 47 more variables: will &lt;dbl&gt;, people &lt;dbl&gt;,
## #   report &lt;dbl&gt;, addresses &lt;dbl&gt;, free &lt;dbl&gt;, business &lt;dbl&gt;, email &lt;dbl&gt;,
## #   you &lt;dbl&gt;, credit &lt;dbl&gt;, your &lt;dbl&gt;, font &lt;dbl&gt;, num000 &lt;dbl&gt;, money &lt;dbl&gt;,
## #   hp &lt;dbl&gt;, hpl &lt;dbl&gt;, george &lt;dbl&gt;, num650 &lt;dbl&gt;, lab &lt;dbl&gt;, labs &lt;dbl&gt;,
## #   telnet &lt;dbl&gt;, num857 &lt;dbl&gt;, data &lt;dbl&gt;, num415 &lt;dbl&gt;, num85 &lt;dbl&gt;,
## #   technology &lt;dbl&gt;, num1999 &lt;dbl&gt;, parts &lt;dbl&gt;, pm &lt;dbl&gt;, direct &lt;dbl&gt;,
## #   cs &lt;dbl&gt;, meeting &lt;dbl&gt;, original &lt;dbl&gt;, project &lt;dbl&gt;, re &lt;dbl&gt;,
## #   edu &lt;dbl&gt;, table &lt;dbl&gt;, conference &lt;dbl&gt;, charSemicolon &lt;dbl&gt;,
## #   charRoundbracket &lt;dbl&gt;, charSquarebracket &lt;dbl&gt;, charExclamation &lt;dbl&gt;,
## #   charDollar &lt;dbl&gt;, charHash &lt;dbl&gt;, capitalAve &lt;dbl&gt;, capitalLong &lt;dbl&gt;,
## #   capitalTotal &lt;dbl&gt;, type &lt;fct&gt;</code></pre>
<p>Este conjunto de datos, creado a finales de la década de 1990 en Hewlett-Packard Labs, contiene 4601 correos electrónicos, de los cuales 1813 se consideran spam. El resto no es spam. (Que para simplificar, podríamos llamar ham.) Se pueden obtener detalles adicionales usando <code>?Spam</code> o visitando <a href="https://archive.ics.uci.edu/ml/datasets/spambase" target="_blank">UCI Machine Learning Repository</a>. .</p>
<p>La variable de respuesta, <code>type</code>, es un <strong>factor</strong> con niveles que etiquetan cada correo electrónico como <code>spam</code> o <code>nonspam</code>. Al ajustar los modelos, <code>nonspam</code> será el nivel de referencia, <span class="math inline">\(Y=0\)</span>, ya que aparece primero en orden alfabético.</p>
<div class="sourceCode" id="cb1477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1477-1"><a href="regresión-logística.html#cb1477-1" aria-hidden="true" tabindex="-1"></a><span class="fu">is.factor</span>(spam<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb1479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1479-1"><a href="regresión-logística.html#cb1479-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(spam<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] &quot;nonspam&quot; &quot;spam&quot;</code></pre>
<p>Muchos de los predictores (a menudo llamados características en el aprendizaje automático) se diseñan en función de los correos electrónicos. Por ejemplo, <code>charDollar</code> es el número de veces que un correo electrónico contiene el carácter <code>$</code>. Algunas variables son muy específicas de este conjunto de datos, por ejemplo, <code>george</code> y <code>num650</code>. (El nombre y código de área de uno de los investigadores cuyos correos electrónicos se utilizaron). Debemos tener en cuenta que este conjunto de datos se creó a partir de correos electrónicos enviados a un investigador de tipo académico en la década de 1990. Es probable que los resultados que obtengamos no se generalicen a los correos electrónicos modernos para el público en general.</p>
<p>Para comenzar, primero dividiremos los datos en entrenamiento-prueba (test-train).</p>
<div class="sourceCode" id="cb1481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1481-1"><a href="regresión-logística.html#cb1481-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb1481-2"><a href="regresión-logística.html#cb1481-2" aria-hidden="true" tabindex="-1"></a><span class="co"># spam_idx = sample(nrow(spam), round(nrow(spam) / 2))</span></span>
<span id="cb1481-3"><a href="regresión-logística.html#cb1481-3" aria-hidden="true" tabindex="-1"></a>spam_idx <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(spam), <span class="dv">1000</span>)</span>
<span id="cb1481-4"><a href="regresión-logística.html#cb1481-4" aria-hidden="true" tabindex="-1"></a>spam_trn <span class="ot">=</span> spam[spam_idx, ]</span>
<span id="cb1481-5"><a href="regresión-logística.html#cb1481-5" aria-hidden="true" tabindex="-1"></a>spam_tst <span class="ot">=</span> spam[<span class="sc">-</span>spam_idx, ]</span></code></pre></div>
<p>Hemos utilizado un conjunto de entrenamiento algo pequeño en relación con el tamaño total del conjunto de datos. En la práctica, probablemente debería ser más grande, pero esto es simplemente para reducir el tiempo de entrenamiento para la ilustración y reproducción de este documento.</p>
<div class="sourceCode" id="cb1482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1482-1"><a href="regresión-logística.html#cb1482-1" aria-hidden="true" tabindex="-1"></a>fit_caps <span class="ot">=</span> <span class="fu">glm</span>(type <span class="sc">~</span> capitalTotal, </span>
<span id="cb1482-2"><a href="regresión-logística.html#cb1482-2" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> spam_trn, <span class="at">family =</span> binomial)</span>
<span id="cb1482-3"><a href="regresión-logística.html#cb1482-3" aria-hidden="true" tabindex="-1"></a>fit_selected <span class="ot">=</span> <span class="fu">glm</span>(type <span class="sc">~</span> edu <span class="sc">+</span> money <span class="sc">+</span> capitalTotal <span class="sc">+</span> charDollar, </span>
<span id="cb1482-4"><a href="regresión-logística.html#cb1482-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> spam_trn, <span class="at">family =</span> binomial)</span>
<span id="cb1482-5"><a href="regresión-logística.html#cb1482-5" aria-hidden="true" tabindex="-1"></a>fit_additive <span class="ot">=</span> <span class="fu">glm</span>(type <span class="sc">~</span> ., </span>
<span id="cb1482-6"><a href="regresión-logística.html#cb1482-6" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> spam_trn, <span class="at">family =</span> binomial)</span>
<span id="cb1482-7"><a href="regresión-logística.html#cb1482-7" aria-hidden="true" tabindex="-1"></a>fit_over <span class="ot">=</span> <span class="fu">glm</span>(type <span class="sc">~</span> capitalTotal <span class="sc">*</span> (.), </span>
<span id="cb1482-8"><a href="regresión-logística.html#cb1482-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> spam_trn, <span class="at">family =</span> binomial, <span class="at">maxit =</span> <span class="dv">50</span>)</span></code></pre></div>
<p>Ajustaremos cuatro regresiones logísticas, cada una más compleja que la anterior. Tenga en cuenta que estamos suprimiendo dos advertencias. La primera la mencionamos anteriormente.</p>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<p>Tenga en cuenta que, cuando recibamos esta advertencia, deberíamos sospechar mucho de las estimaciones de los parámetros.</p>
<div class="sourceCode" id="cb1484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1484-1"><a href="regresión-logística.html#cb1484-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit_selected)</span></code></pre></div>
<pre><code>##   (Intercept)           edu         money  capitalTotal    charDollar 
## -1.1199744712 -1.9837988840  0.9784675298  0.0007757011 11.5772904667</code></pre>
<p>Sin embargo, el modelo aún se puede usar para crear un clasificador, y evaluaremos ese clasificador por sus propios méritos.</p>
<p>También “suprimimos” la advertencia:</p>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<p>En realidad, no lo suprimimos, sino que cambiamos <code>maxit</code> a <code>50</code>, al ajustar el modelo <code>fit_over</code>. Estas fueron suficientes iteraciones adicionales para permitir que el algoritmo de mínimos cuadrados reponderados iterativamente converja al ajustar el modelo.</p>
</div>
<div id="evaluar-clasificadores" class="section level3" number="17.4.2">
<h3><span class="header-section-number">17.4.2</span> Evaluar clasificadores</h3>
<p>La métrica que más nos interesará para evaluar el rendimiento general de un clasificador es la <strong>tasa de clasificación errónea</strong>. (A veces, en cambio, se informa la precisión, que es la proporción de clasificaciones correctas, por lo que ambas métricas tienen el mismo propósito).</p>
<p><span class="math display">\[
\text{Misclass}(\hat{C}, \text{Data}) = \frac{1}{n}\sum_{i = 1}^{n}I(y_i \neq \hat{C}({\bf x_i}))
\]</span></p>
<p><span class="math display">\[
I(y_i \neq \hat{C}({\bf x_i})) = 
\begin{cases} 
  0 &amp; y_i = \hat{C}({\bf x_i}) \\
  1 &amp; y_i \neq \hat{C}({\bf x_i}) \\
\end{cases}
\]</span></p>
<p>Al usar esta métrica en los datos de entrenamiento, tendrá los mismos problemas que RSS para la regresión lineal ordinaria, es decir, solo disminuirá.</p>
<div class="sourceCode" id="cb1487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1487-1"><a href="regresión-logística.html#cb1487-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tasa de clasificación errónea de entrenamiento</span></span>
<span id="cb1487-2"><a href="regresión-logística.html#cb1487-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_caps) <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;spam&quot;</span>, <span class="st">&quot;nonspam&quot;</span>) <span class="sc">!=</span> spam_trn<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] 0.339</code></pre>
<div class="sourceCode" id="cb1489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1489-1"><a href="regresión-logística.html#cb1489-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_selected) <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;spam&quot;</span>, <span class="st">&quot;nonspam&quot;</span>) <span class="sc">!=</span> spam_trn<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] 0.224</code></pre>
<div class="sourceCode" id="cb1491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1491-1"><a href="regresión-logística.html#cb1491-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_additive) <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;spam&quot;</span>, <span class="st">&quot;nonspam&quot;</span>) <span class="sc">!=</span> spam_trn<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] 0.066</code></pre>
<div class="sourceCode" id="cb1493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1493-1"><a href="regresión-logística.html#cb1493-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_over) <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;spam&quot;</span>, <span class="st">&quot;nonspam&quot;</span>) <span class="sc">!=</span> spam_trn<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] 0.136</code></pre>
<p>Debido a esto, los datos de entrenamiento no son útiles para evaluar, ya que sugerirían que siempre deberíamos usar el modelo más grande posible, cuando en realidad, es probable que ese modelo esté sobreajustado. Recuerde, un modelo que es demasiado complejo se sobreajustará. Un modelo demasiado simple quedará bien. (Estamos buscando algo en el medio).</p>
<p>Para superar esto, usaremos la validación cruzada como hicimos con la regresión lineal ordinaria, pero esta vez validaremos de forma cruzada la tasa de clasificación errónea. Para hacerlo, usaremos la función <code>cv.glm()</code> de la libreria <code>boot</code>. Toma argumentos para los datos (en este caso entrenamiento), un modelo ajustado a través de <code>glm()</code> y <code>K</code>, el número de veces Consulte <code>?Cv.glm</code> para obtener más detalles.</p>
<p>Anteriormente, para la validación cruzada de RMSE en regresión lineal ordinaria, usamos LOOCV. Ciertamente podríamos hacer eso aquí. Sin embargo, con la regresión logística, ya no tenemos el truco inteligente que permitiría obtener una métrica LOOCV sin necesidad de ajustar el modelo <span class="math inline">\(n\)</span> veces. Entonces, en su lugar, usaremos una validación cruzada de 5 veces. (5 y 10 veces son las más comunes en la práctica). En lugar de omitir una sola observación repetidamente, omitiremos una quinta parte de los datos.</p>
<p>Básicamente, repetiremos el siguiente proceso 5 veces:</p>
<ul>
<li>Separar al azar una quinta parte de los datos (cada observación solo se retendrá una vez)</li>
<li>Modelo de entrenamiento con datos restantes</li>
<li>Evaluar la tasa de clasificación errónea de los datos retenidos</li>
</ul>
<p>La tasa de clasificación errónea con validación cruzada de 5 veces será el promedio de estas tasas de clasificación errónea. Solo necesitando reajustar el modelo 5 veces, en lugar de <span class="math inline">\(n\)</span> veces, ahorraremos mucho tiempo de cálculo.</p>
<div class="sourceCode" id="cb1495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1495-1"><a href="regresión-logística.html#cb1495-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb1495-2"><a href="regresión-logística.html#cb1495-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1495-3"><a href="regresión-logística.html#cb1495-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cv.glm</span>(spam_trn, fit_caps, <span class="at">K =</span> <span class="dv">5</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] 0.2166961</code></pre>
<div class="sourceCode" id="cb1497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1497-1"><a href="regresión-logística.html#cb1497-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cv.glm</span>(spam_trn, fit_selected, <span class="at">K =</span> <span class="dv">5</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] 0.1587043</code></pre>
<div class="sourceCode" id="cb1499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1499-1"><a href="regresión-logística.html#cb1499-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cv.glm</span>(spam_trn, fit_additive, <span class="at">K =</span> <span class="dv">5</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] 0.08684467</code></pre>
<div class="sourceCode" id="cb1501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1501-1"><a href="regresión-logística.html#cb1501-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cv.glm</span>(spam_trn, fit_over, <span class="at">K =</span> <span class="dv">5</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] 0.137</code></pre>
<p>Tenga en cuenta que estamos suprimiendo las advertencias nuevamente. (Ahora habría muchos más, ya que se ajustarían un total de 20 modelos).</p>
<p>Según estos resultados, <code>fit_caps</code> y <code>fit_selected</code> son insuficientes en relación con <code>fit_additive</code>. De manera similar, <code>fit_over</code> está sobreajustado en relación con <code>fit_additive</code>. Por lo tanto, con base en estos resultados, preferimos el clasificador creado con base al ajuste de regresión logística y almacenado en <code>fit_additive</code>.</p>
<p>En el futuro, para evaluar e informar sobre la eficacia de este clasificador, usaremos el conjunto de datos de prueba. Adoptaremos la posición de que el conjunto de datos de prueba <strong>nunca</strong> debe usarse en el entrenamiento, por lo que usamos la validación cruzada dentro del conjunto de datos de entrenamiento para seleccionar un modelo. Aunque la validación cruzada utiliza conjuntos de reserva para generar métricas, en algún momento todos los datos se utilizan para el entrenamiento.</p>
<p>Para resumir rápidamente qué tan bien funciona este clasificador, crearemos una matriz de confusión.</p>
<div class="figure">
<img src="images/confusion.png" alt="" />
<p class="caption">Matriz de confusión</p>
</div>
<p>Además, desglosa los errores de clasificación en falsos positivos y falsos negativos.</p>
<div class="sourceCode" id="cb1503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1503-1"><a href="regresión-logística.html#cb1503-1" aria-hidden="true" tabindex="-1"></a>make_conf_mat <span class="ot">=</span> <span class="cf">function</span>(predicted, actual) {</span>
<span id="cb1503-2"><a href="regresión-logística.html#cb1503-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">table</span>(<span class="at">predicted =</span> predicted, <span class="at">actual =</span> actual)</span>
<span id="cb1503-3"><a href="regresión-logística.html#cb1503-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Almacenemos explícitamente los valores predichos de nuestro clasificador en el conjunto de datos de prueba.</p>
<div class="sourceCode" id="cb1504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1504-1"><a href="regresión-logística.html#cb1504-1" aria-hidden="true" tabindex="-1"></a>spam_tst_pred <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_additive, spam_tst) <span class="sc">&gt;</span> <span class="dv">0</span>, </span>
<span id="cb1504-2"><a href="regresión-logística.html#cb1504-2" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;spam&quot;</span>, </span>
<span id="cb1504-3"><a href="regresión-logística.html#cb1504-3" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;nonspam&quot;</span>)</span>
<span id="cb1504-4"><a href="regresión-logística.html#cb1504-4" aria-hidden="true" tabindex="-1"></a>spam_tst_pred <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_additive, spam_tst, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span>, </span>
<span id="cb1504-5"><a href="regresión-logística.html#cb1504-5" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;spam&quot;</span>, </span>
<span id="cb1504-6"><a href="regresión-logística.html#cb1504-6" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;nonspam&quot;</span>)</span></code></pre></div>
<p>Las dos líneas de código anteriores producen el mismo resultado, es decir, las mismas predicciones, ya que</p>
<p><span class="math display">\[
\eta({\bf x}) = 0 \iff p({\bf x}) = 0.5
\]</span></p>
<p>Ahora usaremos estas predicciones para crear una matriz de confusión.</p>
<div class="sourceCode" id="cb1505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1505-1"><a href="regresión-logística.html#cb1505-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">conf_mat_50 =</span> <span class="fu">make_conf_mat</span>(<span class="at">predicted =</span> spam_tst_pred, <span class="at">actual =</span> spam_tst<span class="sc">$</span>type))</span></code></pre></div>
<pre><code>##          actual
## predicted nonspam spam
##   nonspam    2057  157
##   spam        127 1260</code></pre>
<p><span class="math display">\[
\text{Prev} = \frac{\text{P}}{\text{Total Obs}}= \frac{\text{TP + FN}}{\text{Total Obs}}
\]</span></p>
<div class="sourceCode" id="cb1507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1507-1"><a href="regresión-logística.html#cb1507-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(spam_tst<span class="sc">$</span>type) <span class="sc">/</span> <span class="fu">nrow</span>(spam_tst)</span></code></pre></div>
<pre><code>## 
##   nonspam      spam 
## 0.6064982 0.3935018</code></pre>
<p>Primero, tenga en cuenta que para ser un clasificador razonable, debe superar al clasificador obvio de simplemente clasificar todas las observaciones en la clase mayoritaria. En este caso, clasificar todo como no spam para una tasa de clasificación errónea de prueba de 0.3935018</p>
<p>A continuación, podemos ver que usando el clasificador creado a partir de <code>fit_additive</code>, un total de <span class="math inline">\(137 + 161 = 298\)</span> del total de 3601 correos electrónicos en el conjunto de prueba están mal clasificados. En general, la precisión en la prueba se establece</p>
<div class="sourceCode" id="cb1509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1509-1"><a href="regresión-logística.html#cb1509-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(spam_tst_pred <span class="sc">==</span> spam_tst<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] 0.921133</code></pre>
<p>En otras palabras, la clasificación errónea de la prueba es</p>
<div class="sourceCode" id="cb1511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1511-1"><a href="regresión-logística.html#cb1511-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(spam_tst_pred <span class="sc">!=</span> spam_tst<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>## [1] 0.07886698</code></pre>
<p>Esto parece un clasificador decente …</p>
<p>Sin embargo, ¿todos los errores son iguales? En este caso, absolutamente no. Los 137 correos electrónicos no spam que se marcaron como spam (falsos positivos) son un problema. No podemos permitir que información importante, digamos, una oferta de trabajo, se pierda de nuestra bandeja de entrada y sea enviada a la carpeta de correo no deseado. Por otro lado, los 161 correos electrónicos no deseados que llegarían a una bandeja de entrada (falsos negativos) se tratan fácilmente, simplemente elimínelos.</p>
<p>En lugar de simplemente evaluar un clasificador en función de su tasa de clasificación errónea (o precisión), definiremos dos métricas adicionales, sensibilidad y especificidad. Tenga en cuenta que estas son simplemente dos de muchas más métricas que se pueden considerar. La <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank">página de Wikipedia para sensibilidad y especificidad</a> detalla una gran cantidad de métricas que pueden derivarse de una matriz de confusión.</p>
<p><strong>Sensibilidad</strong> es esencialmente la tasa de verdaderos positivos. Entonces, cuando la sensibilidad es alta, el número de falsos negativos es bajo.</p>
<p><span class="math display">\[
\text{Sens} = \text{True Positive Rate} = \frac{\text{TP}}{\text{P}} = \frac{\text{TP}}{\text{TP + FN}}
\]</span></p>
<p>Tenemos una función en <code>R</code> para calcular la sensibilidad basada en la matriz de confusión. Tenga en cuenta que esta función es buena para fines ilustrativos, pero se rompe fácilmente. (Piense en lo que sucede si no se pronostican “positivos”).</p>
<div class="sourceCode" id="cb1513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1513-1"><a href="regresión-logística.html#cb1513-1" aria-hidden="true" tabindex="-1"></a>get_sens <span class="ot">=</span> <span class="cf">function</span>(conf_mat) {</span>
<span id="cb1513-2"><a href="regresión-logística.html#cb1513-2" aria-hidden="true" tabindex="-1"></a>  conf_mat[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">/</span> <span class="fu">sum</span>(conf_mat[, <span class="dv">2</span>])</span>
<span id="cb1513-3"><a href="regresión-logística.html#cb1513-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>Especificidad</strong> es esencialmente la tasa de verdaderos negativos. Entonces, cuando la especificidad es alta, el número de falsos positivos es bajo.</p>
<p><span class="math display">\[
\text{Spec} = \text{True Negative Rate} = \frac{\text{TN}}{\text{N}} = \frac{\text{TN}}{\text{TN + FP}}
\]</span></p>
<div class="sourceCode" id="cb1514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1514-1"><a href="regresión-logística.html#cb1514-1" aria-hidden="true" tabindex="-1"></a>get_spec <span class="ot">=</span>  <span class="cf">function</span>(conf_mat) {</span>
<span id="cb1514-2"><a href="regresión-logística.html#cb1514-2" aria-hidden="true" tabindex="-1"></a>  conf_mat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">/</span> <span class="fu">sum</span>(conf_mat[, <span class="dv">1</span>])</span>
<span id="cb1514-3"><a href="regresión-logística.html#cb1514-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Calculamos ambos en función de la matriz de confusión que habíamos creado para nuestro clasificador.</p>
<div class="sourceCode" id="cb1515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1515-1"><a href="regresión-logística.html#cb1515-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_sens</span>(conf_mat_50)</span></code></pre></div>
<pre><code>## [1] 0.8892025</code></pre>
<div class="sourceCode" id="cb1517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1517-1"><a href="regresión-logística.html#cb1517-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_spec</span>(conf_mat_50)</span></code></pre></div>
<pre><code>## [1] 0.9418498</code></pre>
<p>Recuerde que habíamos creado este clasificador usando una probabilidad de <span class="math inline">\(0.5\)</span> como un “límite” de cómo se deben clasificar las observaciones. Ahora modificaremos este límite. Veremos que al modificar el límite, <span class="math inline">\(c\)</span>, podemos mejorar la sensibilidad o la especificidad a expensas de la precisión general (tasa de clasificación errónea).</p>
<p><span class="math display">\[
\hat{C}(\bf x) = 
\begin{cases} 
      1 &amp; \hat{p}({\bf x}) &gt; c \\
      0 &amp; \hat{p}({\bf x}) \leq c 
\end{cases}
\]</span></p>
<p>Además, si cambiamos el límite para mejorar la sensibilidad, disminuiremos la especificidad y viceversa.</p>
<p>Primero veamos qué sucede cuando bajamos el límite de <span class="math inline">\(0.5\)</span> a <span class="math inline">\(0.1\)</span> para crear un nuevo clasificador y, por lo tanto, nuevas predicciones.</p>
<div class="sourceCode" id="cb1519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1519-1"><a href="regresión-logística.html#cb1519-1" aria-hidden="true" tabindex="-1"></a>spam_tst_pred_10 <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_additive, spam_tst, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.1</span>, </span>
<span id="cb1519-2"><a href="regresión-logística.html#cb1519-2" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;spam&quot;</span>, </span>
<span id="cb1519-3"><a href="regresión-logística.html#cb1519-3" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;nonspam&quot;</span>)</span></code></pre></div>
<p>Esto es esencialmente <em>disminuir</em> el umbral para que un correo electrónico sea etiquetado como spam, hasta ahora <em>más</em> correos electrónicos serán etiquetados como spam. Vemos eso en la siguiente matriz de confusión.</p>
<div class="sourceCode" id="cb1520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1520-1"><a href="regresión-logística.html#cb1520-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">conf_mat_10 =</span> <span class="fu">make_conf_mat</span>(<span class="at">predicted =</span> spam_tst_pred_10, <span class="at">actual =</span> spam_tst<span class="sc">$</span>type))</span></code></pre></div>
<pre><code>##          actual
## predicted nonspam spam
##   nonspam    1583   29
##   spam        601 1388</code></pre>
<p>Desafortunadamente, si bien esto reduce en gran medida los falsos negativos, los falsos positivos casi se han cuadriplicado. Vemos esto reflejado en la sensibilidad y especificidad.</p>
<div class="sourceCode" id="cb1522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1522-1"><a href="regresión-logística.html#cb1522-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_sens</span>(conf_mat_10)</span></code></pre></div>
<pre><code>## [1] 0.9795342</code></pre>
<div class="sourceCode" id="cb1524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1524-1"><a href="regresión-logística.html#cb1524-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_spec</span>(conf_mat_10)</span></code></pre></div>
<pre><code>## [1] 0.7248168</code></pre>
<p>Este clasificador, que usa <span class="math inline">\(0.1\)</span> en lugar de <span class="math inline">\(0.5\)</span> tiene una mayor sensibilidad, pero una especificidad mucho menor. Claramente, deberíamos haber movido el límite en la otra dirección. Probemos <span class="math inline">\(0.9\)</span>.</p>
<div class="sourceCode" id="cb1526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1526-1"><a href="regresión-logística.html#cb1526-1" aria-hidden="true" tabindex="-1"></a>spam_tst_pred_90 <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit_additive, spam_tst, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.9</span>, </span>
<span id="cb1526-2"><a href="regresión-logística.html#cb1526-2" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;spam&quot;</span>, </span>
<span id="cb1526-3"><a href="regresión-logística.html#cb1526-3" aria-hidden="true" tabindex="-1"></a>                          <span class="st">&quot;nonspam&quot;</span>)</span></code></pre></div>
<p>Esto es esencialmente <em>aumentar</em> el umbral para que un correo electrónico sea etiquetado como spam, hasta ahora <em>menos</em> correos electrónicos serán etiquetados como spam. Nuevamente, vemos eso en la siguiente matriz de confusión.</p>
<div class="sourceCode" id="cb1527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1527-1"><a href="regresión-logística.html#cb1527-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">conf_mat_90 =</span> <span class="fu">make_conf_mat</span>(<span class="at">predicted =</span> spam_tst_pred_90, <span class="at">actual =</span> spam_tst<span class="sc">$</span>type))</span></code></pre></div>
<pre><code>##          actual
## predicted nonspam spam
##   nonspam    2136  537
##   spam         48  880</code></pre>
<p>Este es el resultado que buscamos. Tenemos muchos menos falsos positivos. Si bien la sensibilidad se reduce considerablemente, la especificidad ha aumentado.</p>
<div class="sourceCode" id="cb1529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1529-1"><a href="regresión-logística.html#cb1529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_sens</span>(conf_mat_90)</span></code></pre></div>
<pre><code>## [1] 0.6210303</code></pre>
<div class="sourceCode" id="cb1531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1531-1"><a href="regresión-logística.html#cb1531-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_spec</span>(conf_mat_90)</span></code></pre></div>
<pre><code>## [1] 0.978022</code></pre>
<p>Si bien se trata de muchos menos falsos positivos, ¿es aceptable? Probablemente todavía no. Además, no olvide que este sería un terrible detector de spam hoy en día, ya que se basa en datos de una era de Internet muy diferente, para un grupo de personas muy específico. ¡El spam ha cambiado mucho desde los 90! (Irónicamente, el aprendizaje automático es probablemente parcialmente el culpable).</p>
<p>Este capítulo ha proporcionado una introducción bastante rápida a la clasificación y, por tanto, al aprendizaje automático. Para obtener una cobertura más completa del aprendizaje automático, <a href="http://www-bcf.usc.edu/~gareth/ISL/" target="_blank">An Introduction to Statistical Learning</a> es un recurso muy recomendable. Además, <code>R</code> for Statistical Learning] se ha escrito como un suplemento que proporciona detalles adicionales sobre cómo realizar estos métodos utilizando <code>R</code>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="selección-de-variables-y-construcción-de-modelos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="más-allá.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
