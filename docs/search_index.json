[["index.html", "Estadística aplicada con R Capítulo 1 Introducción 1.1 Convenciones", " Estadística aplicada con R 2021-05-24 Capítulo 1 Introducción ¡Bienvenido a Estadísticas aplicadas con R! Adapatado de David Dalpiaz, Applied Statistics 1.1 Convenciones El código R se compondrá con una fuente monoespaciada con sintaxis resaltada. a = 3 b = 4 sqrt(a ^ 2 + b ^ 2) Las líneas de salida R, que aparecerían en la consola, comenzarán con ##. Por lo general, no se resaltarán la sintaxis. ## [1] 5 Usamos la cantidad \\(p\\) para referirnos al número de parámetros \\(\\beta\\) en un modelo lineal, no al número de predictores. ¡No se preocupe si aún no sabe lo que esto significa! "],["introducción-a-r.html", "Capítulo 2 Introducción a R 2.1 Primeros pasos 2.2 Cálculos básicos 2.3 Obteniendo ayuda 2.4 Instalación de paquetes", " Capítulo 2 Introducción a R 2.1 Primeros pasos R es un lenguaje de programación y un entorno de software para la computación estadística, que es gratuito y de código abierto. Para comenzar, necesitará instalar dos piezas de software: R, el lenguaje de programación actual. Elija su sistema operativo y seleccione la versión más reciente, 4.0.5. RStudio, un excelente entorno de desarrollo integrado para trabajar con R. Nota, debe tener R instalado para usar RStudio. RStudio es simplemente una interfaz utilizada para interactuar con R. La popularidad de R va en aumento y cada día se convierte en una mejor herramienta para el análisis estadístico. ¡Incluso generó este libro! (Una habilidad que aprenderá en este curso). Hay muchos buenos recursos para aprender R. Los siguientes capítulos servirán como una rápida introducción a R. De ninguna manera pretenden ser una referencia completa para el lenguaje R, sino simplemente una introducción a los conceptos básicos que necesitaremos en el camino. Se volverán a enfatizar varios de los temas más importantes, ya que son realmente necesarios para los análisis. Estos capítulos introductorios a R pueden parecer una cantidad abrumadora de información. No se espera que recoja todo la primera vez. Debe probar todo el código de estos capítulos y luego volver a ellos varias veces a medida que regrese a los conceptos al realizar análisis. R se utiliza tanto para el desarrollo de software como para el análisis de datos. Operaremos en un área gris, en algún lugar entre estas dos tareas. Nuestro principal objetivo será analizar datos, pero también realizaremos ejercicios de programación que ayuden a ilustrar ciertos conceptos. RStudio tiene una gran cantidad de atajos de teclado útiles. Se puede encontrar una lista de estos usando un atajo de teclado, el atajo de teclado para gobernarlos a todos: En Windows: Alt + Shift + K En Mac: Option + Shift + K El equipo de RStudio ha desarrollado una serie de hojas de trucos para trabajar con R y RStudio. Esta hoja de referencia en particular para R Base resumirá muchos de los conceptos de este documento. (R Base es un nombre que se usa para diferenciar la práctica de usar funcionesR integradas, en contraposición a usar funciones de paquetes externos, en particular, las de tidyverse. Más sobre esto más adelante.) Al programar, a menudo es una buena práctica seguir una guía de estilo. (¿A dónde van los espacios? ¿Tabuladores o espacios? ¿Guiones bajos o CamelCase al nombrar variables?) Ninguna guía de estilo es correcta, pero ayuda a estar al tanto de lo que hacen los demás. Lo más importante es ser coherente con su propio código. Guía de estilo de Hadley Wickham from R avanzado Guía de estilo de Google Para este curso, nuestra principal desviación de estas dos guías es el uso de = en lugar de &lt;-. (Más sobre eso más adelante). 2.2 Cálculos básicos Para comenzar, usaremos R como una simple calculadora. Suma, resta, multiplicación y división Matemáticas R Resultado \\(3 + 2\\) 3 + 2 5 \\(3 - 2\\) 3 - 2 1 \\(3 \\cdot2\\) 3 * 2 6 \\(3 / 2\\) 3 / 2 1.5 Exponentes Matemáticas R Resultado \\(3^2\\) 3 ^ 2 9 \\(2^{(-3)}\\) 2 ^ (-3) 0.125 \\(100^{1/2}\\) 100 ^ (1 / 2) 10 \\(\\sqrt{100}\\) sqrt(100) 10 Constantes Matemáticas Matemáticas R Resultado \\(\\pi\\) pi 3.1415927 \\(e\\) exp(1) 2.7182818 Logaritmos Tenga en cuenta que usaremos \\(\\ln\\) y \\(\\log\\) indistintamente para significar el logaritmo natural. No hay ln() en R, en su lugar usalog ()para significar el logaritmo natural. Matemáticas R Resultado \\(\\log(e)\\) log(exp(1)) 1 \\(\\log_{10}(1000)\\) log10(1000) 3 \\(\\log_{2}(8)\\) log2(8) 3 \\(\\log_{4}(16)\\) log(16, base = 4) 2 Trigonometría Matemáticas R Resultado \\(\\sin(\\pi / 2)\\) sin(pi / 2) 1 \\(\\cos(0)\\) cos(0) 1 2.3 Obteniendo ayuda Al usar R como calculadora, hemos visto varias funciones:sqrt(),exp(),log()ysin(). Para obtener documentación sobre una función en R, simplemente coloque un signo de interrogación delante del nombre de la función y RStudio mostrará la documentación, por ejemplo: ?log ?sin ?paste ?lm Con frecuencia, una de las cosas más difíciles de hacer al aprender R es pedir ayuda. Primero, debe decidir pedir ayuda, luego necesita saber cómo pedir ayuda. Su primera línea de defensa debe ser buscar en Google su mensaje de error o una breve descripción de su problema. (La capacidad de resolver problemas utilizando este método se está convirtiendo rápidamente en una habilidad extremadamente valiosa). Si eso falla, y eventualmente lo hará, debe pedir ayuda. Hay una serie de cosas que debe incluir al enviar un correo electrónico a un instructor o al publicar en un sitio web de ayuda como Stack Exchange. Describa qué espera que haga el código. Indique el objetivo final que está tratando de lograr. (A veces, lo que espera que haga el código no es lo que realmente quiere hacer). Proporcione el texto completo de los errores que haya recibido. Proporcione suficiente código para recrear el error. A menudo, para el propósito de este curso, simplemente puede enviar por correo electrónico todo el archivo .R o.Rmd. A veces también es útil incluir una captura de pantalla de toda la ventana de RStudio cuando se produce el error. Si sigue estos pasos, su problema se resolverá mucho más rápido y posiblemente aprenderá más en el proceso. No se desanime al encontrarse con errores y dificultades al aprender R. (O cualquier habilidad técnica). Es simplemente parte del proceso de aprendizaje. 2.4 Instalación de paquetes R viene con una serie de funciones integradas y conjuntos de datos, pero una de las principales fortalezas deR como proyecto de código abierto es su sistema de paquetes. Los paquetes agregan funciones y datos adicionales. Con frecuencia, si desea hacer algo en R, y no está disponible de forma predeterminada, es muy probable que haya un paquete que satisfaga sus necesidades. Para instalar un paquete, use la función install.packages(). Piense en esto como comprar un libro de recetas en la tienda, llevarlo a casa y ponerlo en su estante. install.packages(&quot;ggplot2&quot;) Una vez que se instala un paquete, debe cargarse en su sesión actual de R antes de ser utilizado. Piense en esto como sacar el libro del estante y abrirlo para leer. library(ggplot2) Una vez que cierra R, todos los paquetes se cierran y se vuelven a colocar en el estante imaginario. La próxima vez que abra R, no es necesario que vuelva a instalar el paquete, pero debe cargar los paquetes que desee utilizar invocandolibrary(). "],["datos-y-programación.html", "Capítulo 3 Datos y programación 3.1 Tipos de datos 3.2 Estructuras de datos 3.3 Conceptos básicos de programación", " Capítulo 3 Datos y programación 3.1 Tipos de datos R tiene varios tipos de datos básicos. Numérico También conocido como Double. El tipo predeterminado cuando se trata de números. Ejemplos: 1, 1.0, 42.5 Entero Ejemplos: 1L, 2L, 42L Complejo Ejemplos: 4 + 2i Lógico Dos valores posibles: TRUE y FALSE También puedes usar T y F, pero esto no se recomienda. NA También se considera lógico. caracteres Ejemplos: \"a\", \"Estadísticas\", \"1 más 2.\" 3.2 Estructuras de datos R también tiene varias estructuras de datos básicas. Una estructura de datos es homogénea (todos los elementos son del mismo tipo de datos) o heterogénea (los elementos pueden ser de más de un tipo de datos). Dimensión Homogénea Heterogénea 1 Vector Lista 2 Matriz marco de datos 3+ Array 3.2.1 Vectores Muchas operaciones en R hacen un uso intensivo de vectores. Los vectores en R se indexan comenzando en 1. Eso es lo que indica el [1] en la salida, que el primer elemento de la fila que se muestra es el primer elemento del vector. Los vectores más grandes comenzarán filas adicionales con [*] donde * es el índice del primer elemento de la fila. Posiblemente la forma más común de crear un vector en R es usando la función c(), que es la abreviatura de combinar. Como sugiere el nombre, combina una lista de elementos separados por comas. c(1, 3, 5, 7, 8, 9) ## [1] 1 3 5 7 8 9 Aquí R simplemente genera este vector. Si quisiéramos almacenar este vector en una variable podemos hacerlo con el operador de asignación =. En este caso, la variable x ahora contiene el vector que acabamos de crear, y podemos acceder al vector escribiendo x. x = c(1, 3, 5, 7, 8, 9) x ## [1] 1 3 5 7 8 9 Como acotación al margen, hay una larga historia del operador de asignación en R, en parte debido a las teclas disponibles en los teclados de los creadores del lenguaje S. (Que precedió a R.) Para simplificar, usaremos =, pero sepa que a menudo verá &lt;- como el operador de asignación. Los pros y los contras de estos dos están mucho más allá del alcance de este libro, pero sepa que para nuestros propósitos no tendrá ningún problema si simplemente usa =. Si está interesado en los casos extraños en los que la diferencia importa, consulte The R Inferno. Si desea usar &lt;-, aún necesitará usar =, sin embargo, solo para pasar argumentos. A algunos usuarios les gusta mantener la asignación (&lt;-) y el paso de argumentos (=) separados. No importa lo que elija, lo más importante es que se mantenga constante. Además, si trabaja en un proyecto colaborativo más grande, debe usar cualquier estilo que ya esté implementado. Debido a que los vectores deben contener elementos que sean todos del mismo tipo,R automáticamente coaccionará a un solo tipo cuando intente crear un vector que combine varios tipos. c(42, &quot;Statistics&quot;, TRUE) ## [1] &quot;42&quot; &quot;Statistics&quot; &quot;TRUE&quot; c(42, TRUE) ## [1] 42 1 Con frecuencia, es posible que desee crear un vector basado en una secuencia de números. La forma más rápida y sencilla de hacer esto es con el operador :, que crea una secuencia de enteros entre dos enteros especificados. (y = 1:100) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 Aquí vemos a R etiquetando las filas después de la primera ya que este es un vector grande. Además, vemos que al poner paréntesis alrededor de la asignación, R almacena el vector en una variable llamaday y automáticamente envía y a la consola. Tenga en cuenta que los escalares no existen en R. Son simplemente vectores de longitud 1. 2 ## [1] 2 Si queremos crear una secuencia que no se limite a números enteros y que aumente de uno en uno, podemos usar la función seq (). seq(from = 1.5, to = 4.2, by = 0.1) ## [1] 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 ## [20] 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 Discutiremos las funciones en detalle más adelante, pero tenga en cuenta que las etiquetas de entrada from,to y by son opcionales. seq(1.5, 4.2, 0.1) ## [1] 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 ## [20] 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 Otra operación común para crear un vector es rep (), que puede repetir un solo valor varias veces. rep(&quot;A&quot;, times = 10) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; La función rep () se puede usar para repetir un vector varias veces. rep(x, times = 3) ## [1] 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 Ahora hemos visto cuatro formas diferentes de crear vectores: c() : seq() rep() Hasta ahora los hemos usado principalmente de forma aislada, pero a menudo se usan juntos. c(x, rep(seq(1, 9, 2), 3), c(1, 2, 3), 42, 2:4) ## [1] 1 3 5 7 8 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 2 3 42 ## [26] 2 3 4 La longitud de un vector se puede obtener con la función length(). length(x) ## [1] 6 length(y) ## [1] 100 3.2.1.1 Subconjunto Para crear un subconjunto de un vector, usamos corchetes, []. x ## [1] 1 3 5 7 8 9 x[1] ## [1] 1 x[3] ## [1] 5 Vemos que x[1] devuelve el primer elemento, y x[3] devuelve el tercer elemento. x[-2] ## [1] 1 5 7 8 9 También podemos excluir ciertos índices, en este caso el segundo elemento. x[1:3] ## [1] 1 3 5 x[c(1,3,4)] ## [1] 1 5 7 Por último, vemos que podemos crear subconjuntos basados en un vector de índices. Todo lo anterior es un subconjunto de un vector usando un vector de índices. (Recuerde que un solo número sigue siendo un vector). En su lugar, podríamos usar un vector de valores lógicos. z = c(TRUE, TRUE, FALSE, TRUE, TRUE, FALSE) z ## [1] TRUE TRUE FALSE TRUE TRUE FALSE x[z] ## [1] 1 3 7 8 3.2.2 Vectorización Una de las mayores fortalezas de R es su uso de operaciones vectorizadas. (Con frecuencia, la falta de comprensión de este concepto conduce a la creencia de que R es lento. R no es el lenguaje más rápido, pero tiene la reputación de ser más lento de lo que realmente es). x = 1:10 x + 1 ## [1] 2 3 4 5 6 7 8 9 10 11 2 * x ## [1] 2 4 6 8 10 12 14 16 18 20 2 ^ x ## [1] 2 4 8 16 32 64 128 256 512 1024 sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 ## [8] 2.0794415 2.1972246 2.3025851 Vemos que cuando se llama a una función como log() en un vector x, se devuelve un vector que ha aplicado la función a cada elemento del vector x. 3.2.3 Operadores logicos Operador Resumen Ejemplo Resultado x &lt; y x menor a y 3 &lt; 42 TRUE x &gt; y x mayor a y 3 &gt; 42 FALSE x &lt;= y x menor o igual ay 3 &lt;= 42 TRUE x &gt;= y x mayor o igual a y 3 &gt;= 42 FALSE x == y xigual a y 3 == 42 FALSE x != y x diferente a y 3 != 42 TRUE !x diferente a x !(3 &gt; 42) TRUE x | y x o y (3 &gt; 42) | TRUE TRUE x &amp; y x y y (3 &lt; 4) &amp; ( 42 &gt; 13) TRUE En R, los operadores lógicos están vectorizados. x = c(1, 3, 5, 7, 8, 9) x &gt; 3 ## [1] FALSE FALSE TRUE TRUE TRUE TRUE x &lt; 3 ## [1] TRUE FALSE FALSE FALSE FALSE FALSE x == 3 ## [1] FALSE TRUE FALSE FALSE FALSE FALSE x != 3 ## [1] TRUE FALSE TRUE TRUE TRUE TRUE x == 3 &amp; x != 3 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE x == 3 | x != 3 ## [1] TRUE TRUE TRUE TRUE TRUE TRUE Esto es extremadamente útil para crear subconjuntos. x[x &gt; 3] ## [1] 5 7 8 9 x[x != 3] ## [1] 1 5 7 8 9 TODO: coercion sum(x &gt; 3) ## [1] 4 as.numeric(x &gt; 3) ## [1] 0 0 1 1 1 1 Aquí vemos que el uso de la función sum() en un vector de valores lógicos TRUE yFALSE que es el resultado de x&gt;3 da como resultado un resultado numérico. R primero está convirtiendo automáticamente lo lógico en numérico, donde TRUE es 1 y FALSE es 0. Esta coerción de lógica a numérica ocurre para la mayoría de las operaciones matemáticas. which(x &gt; 3) ## [1] 3 4 5 6 x[which(x &gt; 3)] ## [1] 5 7 8 9 max(x) ## [1] 9 which(x == max(x)) ## [1] 6 which.max(x) ## [1] 6 3.2.4 Más vectorización x = c(1, 3, 5, 7, 8, 9) y = 1:100 x + 2 ## [1] 3 5 7 9 10 11 x + rep(2, 6) ## [1] 3 5 7 9 10 11 x &gt; 3 ## [1] FALSE FALSE TRUE TRUE TRUE TRUE x &gt; rep(3, 6) ## [1] FALSE FALSE TRUE TRUE TRUE TRUE x + y ## Warning in x + y: longitud de objeto mayor no es múltiplo de la longitud de uno ## menor ## [1] 2 5 8 11 13 15 8 11 14 17 19 21 14 17 20 23 25 27 ## [19] 20 23 26 29 31 33 26 29 32 35 37 39 32 35 38 41 43 45 ## [37] 38 41 44 47 49 51 44 47 50 53 55 57 50 53 56 59 61 63 ## [55] 56 59 62 65 67 69 62 65 68 71 73 75 68 71 74 77 79 81 ## [73] 74 77 80 83 85 87 80 83 86 89 91 93 86 89 92 95 97 99 ## [91] 92 95 98 101 103 105 98 101 104 107 length(x) ## [1] 6 length(y) ## [1] 100 length(y) / length(x) ## [1] 16.66667 (x + y) - y ## Warning in x + y: longitud de objeto mayor no es múltiplo de la longitud de uno ## menor ## [1] 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 ## [38] 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 ## [75] 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 8 9 1 3 5 7 y = 1:60 x + y ## [1] 2 5 8 11 13 15 8 11 14 17 19 21 14 17 20 23 25 27 20 23 26 29 31 33 26 ## [26] 29 32 35 37 39 32 35 38 41 43 45 38 41 44 47 49 51 44 47 50 53 55 57 50 53 ## [51] 56 59 61 63 56 59 62 65 67 69 length(y) / length(x) ## [1] 10 rep(x, 10) + y ## [1] 2 5 8 11 13 15 8 11 14 17 19 21 14 17 20 23 25 27 20 23 26 29 31 33 26 ## [26] 29 32 35 37 39 32 35 38 41 43 45 38 41 44 47 49 51 44 47 50 53 55 57 50 53 ## [51] 56 59 61 63 56 59 62 65 67 69 all(x + y == rep(x, 10) + y) ## [1] TRUE identical(x + y, rep(x, 10) + y) ## [1] TRUE # ?any # ?all.equal 3.2.5 Matrices R también se puede utilizar para cálculos de matriz. Las matrices tienen filas y columnas que contienen un solo tipo de datos. En una matriz, el orden de filas y columnas es importante. (Esto no es cierto para Marcos de datos, que veremos más adelante). Las matrices se pueden crear usando la función matrix. x = 1:9 x ## [1] 1 2 3 4 5 6 7 8 9 X = matrix(x, nrow = 3, ncol = 3) X ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Observe aquí que estamos usando dos variables diferentes: x minúscula, que almacena un vector y X mayúscula, que almacena una matriz. (Siguiendo la convención matemática habitual). Podemos hacer esto porque R distingue entre mayúsculas y minúsculas. Por defecto, la función matrix reordena un vector en columnas, pero también podemos decirle aR que use filas en su lugar. Y = matrix(x, nrow = 3, ncol = 3, byrow = TRUE) Y ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 También podemos crear una matriz de una dimensión específica donde todos los elementos son iguales, en este caso 0. Z = matrix(0, 2, 4) Z ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 Al igual que los vectores, las matrices se pueden subdividir utilizando corchetes, []. Sin embargo, dado que las matrices son bidimensionales, necesitamos especificar tanto una fila como una columna al crear subconjuntos . X ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 X[1, 2] ## [1] 4 Aquí accedimos al elemento de la primera fila y la segunda columna. También podríamos crear subconjuntos de una fila o columna completa. X[1, ] ## [1] 1 4 7 X[, 2] ## [1] 4 5 6 También podemos usar vectores para crear subconjuntos de más de una fila o columna a la vez. Aquí creamos un subconjunto de la primera y tercera columna de la segunda fila. X[2, c(1, 3)] ## [1] 2 8 Las matrices también se pueden crear combinando vectores como columnas, usando cbind, o combinando vectores como filas, usandorbind. x = 1:9 rev(x) ## [1] 9 8 7 6 5 4 3 2 1 rep(1, 9) ## [1] 1 1 1 1 1 1 1 1 1 rbind(x, rev(x), rep(1, 9)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## x 1 2 3 4 5 6 7 8 9 ## 9 8 7 6 5 4 3 2 1 ## 1 1 1 1 1 1 1 1 1 cbind(col_1 = x, col_2 = rev(x), col_3 = rep(1, 9)) ## col_1 col_2 col_3 ## [1,] 1 9 1 ## [2,] 2 8 1 ## [3,] 3 7 1 ## [4,] 4 6 1 ## [5,] 5 5 1 ## [6,] 6 4 1 ## [7,] 7 3 1 ## [8,] 8 2 1 ## [9,] 9 1 1 Cuando use rbind ycbind, puede especificar nombres de argumentos que se usarán como nombres de columna. R se puede utilizar para realizar cálculos matriciales. x = 1:9 y = 9:1 X = matrix(x, 3, 3) Y = matrix(y, 3, 3) X ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Y ## [,1] [,2] [,3] ## [1,] 9 6 3 ## [2,] 8 5 2 ## [3,] 7 4 1 X + Y ## [,1] [,2] [,3] ## [1,] 10 10 10 ## [2,] 10 10 10 ## [3,] 10 10 10 X - Y ## [,1] [,2] [,3] ## [1,] -8 -2 4 ## [2,] -6 0 6 ## [3,] -4 2 8 X * Y ## [,1] [,2] [,3] ## [1,] 9 24 21 ## [2,] 16 25 16 ## [3,] 21 24 9 X / Y ## [,1] [,2] [,3] ## [1,] 0.1111111 0.6666667 2.333333 ## [2,] 0.2500000 1.0000000 4.000000 ## [3,] 0.4285714 1.5000000 9.000000 Tenga en cuenta que X*Y no es una multiplicación de matrices. Es una multiplicación elemento por elemento. (Lo mismo para X/Y). En cambio, la multiplicación de matrices usa %*%. Otras funciones matriciales incluyen t() que da la transpuesta de una matriz y solve() que devuelve la inversa de una matriz cuadrada si es invertible. X %*% Y ## [,1] [,2] [,3] ## [1,] 90 54 18 ## [2,] 114 69 24 ## [3,] 138 84 30 t(X) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 Z = matrix(c(9, 2, -3, 2, 4, -2, -3, -2, 16), 3, byrow = TRUE) Z ## [,1] [,2] [,3] ## [1,] 9 2 -3 ## [2,] 2 4 -2 ## [3,] -3 -2 16 solve(Z) ## [,1] [,2] [,3] ## [1,] 0.12931034 -0.05603448 0.01724138 ## [2,] -0.05603448 0.29094828 0.02586207 ## [3,] 0.01724138 0.02586207 0.06896552 Para verificar que solve(Z) devuelve el inverso, lo multiplicamos por Z. Es de esperar que esto devuelva la matriz de identidad, sin embargo, vemos que este no es el caso debido a algunos problemas de cálculo. Sin embargo, R también tiene la función all.equal() que verifica la igualdad, con una pequeña tolerancia que explica algunos problemas de cálculo. La función identical() se usa para verificar la igualdad exacta. solve(Z) %*% Z ## [,1] [,2] [,3] ## [1,] 1.000000e+00 -6.245005e-17 0.000000e+00 ## [2,] 8.326673e-17 1.000000e+00 5.551115e-17 ## [3,] 2.775558e-17 0.000000e+00 1.000000e+00 diag(3) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 all.equal(solve(Z) %*% Z, diag(3)) ## [1] TRUE R tiene una serie de funciones específicas de matriz para obtener información de dimensión y resumen. X = matrix(1:6, 2, 3) X ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 dim(X) ## [1] 2 3 rowSums(X) ## [1] 9 12 colSums(X) ## [1] 3 7 11 rowMeans(X) ## [1] 3 4 colMeans(X) ## [1] 1.5 3.5 5.5 La función diag() se puede utilizar de varias formas. Podemos extraer la diagonal de una matriz. diag(Z) ## [1] 9 4 16 O crear una matriz con elementos específicos de la diagonal. (Y 0 en lo que esta fuera de las diagonales). diag(1:5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 2 0 0 0 ## [3,] 0 0 3 0 0 ## [4,] 0 0 0 4 0 ## [5,] 0 0 0 0 5 O, por último, crear una matriz cuadrada de cierta dimensión con 1 para cada elemento de la diagonal y 0 para cada elemento fuera de la diagonal. diag(5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 Cálculos con vectores y matrices Ciertas operaciones en R, por ejemplo %*% tiene un comportamiento diferente en vectores y matrices. Para ilustrar esto, primero crearemos dos vectores. a_vec = c(1, 2, 3) b_vec = c(2, 2, 2) Tenga en cuenta que estos son de hecho vectores. No son matrices. c(is.vector(a_vec), is.vector(b_vec)) ## [1] TRUE TRUE c(is.matrix(a_vec), is.matrix(b_vec)) ## [1] FALSE FALSE Cuando este es el caso, el operador % *% se usa para calcular el producto escalar, también conocido como el producto interno de los dos vectores. El producto escalar de los vectores \\(\\boldsymbol{a} = \\lbrack a_1, a_2, \\cdots a_n \\rbrack\\) y \\(\\boldsymbol{b} = \\lbrack b_1, b_2, \\cdots b_n \\rbrack\\) esta definido por \\[ \\boldsymbol{a} \\cdot \\boldsymbol{b} = \\sum_{i = 1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \\cdots a_n b_n. \\] a_vec %*% b_vec # inner product ## [,1] ## [1,] 12 a_vec %o% b_vec # outer product ## [,1] [,2] [,3] ## [1,] 2 2 2 ## [2,] 4 4 4 ## [3,] 6 6 6 El operador %o% se utiliza para calcular el producto exterior de los dos vectores. Cuando los vectores se ven obligados a convertirse en matrices, son vectores columna. Entonces, un vector de longitud \\(n\\) se convierte en una matriz $n $ después de la coerción. as.matrix(a_vec) ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 Si usamos el operador %*% en matrices, `%*%`` vuelve a realizar la multiplicación de matrices esperada. Por lo tanto, es de esperar que lo siguiente produzca un error, porque las dimensiones son incorrectas. as.matrix(a_vec) %*% b_vec ## [,1] [,2] [,3] ## [1,] 2 2 2 ## [2,] 4 4 4 ## [3,] 6 6 6 A simple vista, esta es una matriz de \\(3\\times1\\), multiplicada por una matriz de \\(3\\times1\\). Sin embargo, cuando b_vec se convierte automáticamente en una matriz,R decidió convertirlo en un vector de fila, una matriz de $1 $, de modo que la multiplicación tenga una dimensión conformable. Si hubiéramos coaccionado a ambos, entonces R produciría un error. as.matrix(a_vec) %*% as.matrix(b_vec) Otra forma de calcular un * producto escalar * es con la función crossprod(). Dados dos vectores, la función crossprod() calcula su producto escalar. La función tiene un nombre bastante engañoso. crossprod(a_vec, b_vec) # inner product ## [,1] ## [1,] 12 tcrossprod(a_vec, b_vec) # outer product ## [,1] [,2] [,3] ## [1,] 2 2 2 ## [2,] 4 4 4 ## [3,] 6 6 6 Estas funciones podrían resultar muy útiles más adelante. Cuando se usa con matrices \\(X\\) y \\(Y\\) como argumentos, calcula \\[ X^\\top Y. \\] Cuando se trata de modelos lineales, el cálculo \\[ X^\\top X \\] se utiliza repetidamente. C_mat = matrix(c(1, 2, 3, 4, 5, 6), 2, 3) D_mat = matrix(c(2, 2, 2, 2, 2, 2), 2, 3) Esto es útil como atajo para un cálculo frecuente y como una implementación más eficiente que usar t() y %*%. crossprod(C_mat, D_mat) ## [,1] [,2] [,3] ## [1,] 6 6 6 ## [2,] 14 14 14 ## [3,] 22 22 22 t(C_mat) %*% D_mat ## [,1] [,2] [,3] ## [1,] 6 6 6 ## [2,] 14 14 14 ## [3,] 22 22 22 all.equal(crossprod(C_mat, D_mat), t(C_mat) %*% D_mat) ## [1] TRUE crossprod(C_mat, C_mat) ## [,1] [,2] [,3] ## [1,] 5 11 17 ## [2,] 11 25 39 ## [3,] 17 39 61 t(C_mat) %*% C_mat ## [,1] [,2] [,3] ## [1,] 5 11 17 ## [2,] 11 25 39 ## [3,] 17 39 61 all.equal(crossprod(C_mat, C_mat), t(C_mat) %*% C_mat) ## [1] TRUE 3.2.6 Listas Una lista es una estructura de datos heterogénea unidimensional. Por lo tanto, está indexado como un vector con un solo valor entero, pero cada elemento puede contener un elemento de cualquier tipo. # creación list(42, &quot;Hola&quot;, TRUE) ## [[1]] ## [1] 42 ## ## [[2]] ## [1] &quot;Hola&quot; ## ## [[3]] ## [1] TRUE ex_list = list( a = c(1, 2, 3, 4), b = TRUE, c = &quot;Hola!&quot;, d = function(arg = 42) {print(&quot;Hola mundo!&quot;)}, e = diag(5) ) Las listas pueden ser subconjuntos usando dos sintaxis, el operador $ y corchetes []. El operador $ devuelve un elemento nombrado de una lista. La sintaxis [] devuelve una lista, mientras que [[]] devuelve un elemento de una lista. ex_list[1] devuelve una lista que contiene el primer elemento. ex_list[[1]] devuelve el primer elemento de la lista, en este caso, un vector. # subconjunto ex_list$e ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 ex_list[1:2] ## $a ## [1] 1 2 3 4 ## ## $b ## [1] TRUE ex_list[1] ## $a ## [1] 1 2 3 4 ex_list[[1]] ## [1] 1 2 3 4 ex_list[c(&quot;e&quot;, &quot;a&quot;)] ## $e ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 ## ## $a ## [1] 1 2 3 4 ex_list[&quot;e&quot;] ## $e ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 ex_list[[&quot;e&quot;]] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 ex_list$d ## function(arg = 42) {print(&quot;Hola mundo!&quot;)} ex_list$d(arg = 1) ## [1] &quot;Hola mundo!&quot; 3.2.7 Marcos de datos Anteriormente hemos visto vectores y matrices para almacenar datos. Ahora presentaremos un marco de datos que será la forma más común de almacenar e interactuar con los datos en este curso. example_data = data.frame(x = c(1, 3, 5, 7, 9, 1, 3, 5, 7, 9), y = c(rep(&quot;Hola&quot;, 9), &quot;Adiós&quot;), z = rep(c(TRUE, FALSE), 5)) A diferencia de una matriz, que se puede considerar como un vector reorganizado en filas y columnas, no se requiere que un marco de datos tenga el mismo tipo de datos para cada elemento. Un marco de datos es una lista de vectores. Entonces, cada vector debe contener el mismo tipo de datos, pero los diferentes vectores pueden almacenar diferentes tipos de datos. example_data ## x y z ## 1 1 Hola TRUE ## 2 3 Hola FALSE ## 3 5 Hola TRUE ## 4 7 Hola FALSE ## 5 9 Hola TRUE ## 6 1 Hola FALSE ## 7 3 Hola TRUE ## 8 5 Hola FALSE ## 9 7 Hola TRUE ## 10 9 Adiós FALSE A diferencia de una lista que tiene más flexibilidad, los elementos de un marco de datos deben ser todos vectores y tener la misma longitud. example_data$x ## [1] 1 3 5 7 9 1 3 5 7 9 all.equal(length(example_data$x), length(example_data$y), length(example_data$z)) ## [1] TRUE str(example_data) ## &#39;data.frame&#39;: 10 obs. of 3 variables: ## $ x: num 1 3 5 7 9 1 3 5 7 9 ## $ y: chr &quot;Hola&quot; &quot;Hola&quot; &quot;Hola&quot; &quot;Hola&quot; ... ## $ z: logi TRUE FALSE TRUE FALSE TRUE FALSE ... nrow(example_data) ## [1] 10 ncol(example_data) ## [1] 3 dim(example_data) ## [1] 10 3 La función data.frame() es una forma de crear un marco de datos. También podemos importar datos de varios tipos de archivos en R, así como utilizar datos almacenados en paquetes. Los datos de ejemplo anteriores también se pueden encontrar aquí como un archivo .csv. Para leer estos datos en R, usaríamos la funciónread_csv()del paquete readr. Tenga en cuenta que R tiene una función incorporada read.csv()que opera de manera muy similar. La función read_csv() de readr tiene varias ventajas. Por ejemplo, es mucho más rápido para leer datos grandes. library(readr) example_data_from_csv = read_csv(&quot;data/example-data.csv&quot;) Esta línea de código en particular asume que el archivo example_data.csv existe en una carpeta llamada data en su directorio de trabajo actual. example_data_from_csv ## # A tibble: 10 x 3 ## x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 1 &quot;Hola&quot; TRUE ## 2 3 &quot;Hola&quot; FALSE ## 3 5 &quot;Hola&quot; TRUE ## 4 7 &quot;Hola&quot; FALSE ## 5 9 &quot;Hola&quot; TRUE ## 6 1 &quot;Hola&quot; FALSE ## 7 3 &quot;Hola&quot; TRUE ## 8 5 &quot;Hola&quot; FALSE ## 9 7 &quot;Hola&quot; TRUE ## 10 9 &quot;Adi\\xf3s&quot; FALSE Un tibble es simplemente un marco de datos que se imprime con cordura. Observe en el resultado anterior que se nos proporciona información adicional, como dimensión y tipo de variable. La función as_tibble() se puede utilizar para convertir un marco de datos regular en un tibble. library(tibble) example_data = as_tibble(example_data) example_data ## # A tibble: 10 x 3 ## x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 1 Hola TRUE ## 2 3 Hola FALSE ## 3 5 Hola TRUE ## 4 7 Hola FALSE ## 5 9 Hola TRUE ## 6 1 Hola FALSE ## 7 3 Hola TRUE ## 8 5 Hola FALSE ## 9 7 Hola TRUE ## 10 9 Adiós FALSE Alternativamente, podríamos usar la función Import Dataset en RStudio que se puede encontrar en la ventana del entorno. (De forma predeterminada, el panel superior derecho de RStudio). Una vez completado, este proceso generará automáticamente el código para importar un archivo. El código resultante se mostrará en la ventana de la consola. En versiones recientes de RStudio, read_csv() se usa por defecto, por lo tanto, se lee en un tibble. Anteriormente analizamos la instalación de paquetes, en particular el paquete ggplot2. (Un paquete para visualización. Si bien no es necesario para este curso, su popularidad está creciendo rápidamente). library(ggplot2) Dentro del paquete ggplot2 hay un conjunto de datos llamado mpg. Al cargar el paquete usando la función library(), ahora podemos acceder a mpg. Cuando usamos datos desde dentro de un paquete, hay tres cosas que generalmente nos gustaría hacer: Mira los datos sin procesar. Comprender los datos. (¿De dónde vino? ¿Cuáles son las variables? Etc.) Visualiza los datos. Para ver los datos, tenemos dos comandos útiles: head() y str(). head(mpg, n = 10) ## # A tibble: 10 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~ ## 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~ ## 3 audi a4 2 2008 4 manual~ f 20 31 p comp~ ## 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~ ## 5 audi a4 2.8 1999 6 auto(l~ f 16 26 p comp~ ## 6 audi a4 2.8 1999 6 manual~ f 18 26 p comp~ ## 7 audi a4 3.1 2008 6 auto(a~ f 18 27 p comp~ ## 8 audi a4 quat~ 1.8 1999 4 manual~ 4 18 26 p comp~ ## 9 audi a4 quat~ 1.8 1999 4 auto(l~ 4 16 25 p comp~ ## 10 audi a4 quat~ 2 2008 4 manual~ 4 20 28 p comp~ La función head() mostrará las primeras n observaciones del marco de datos. La función head() era más útil antes que tibbles. Observe que mpg ya es un tibble, por lo que la salida de head() indica que solo hay 10 observaciones. Tenga en cuenta que esto se aplica a head (mpg, n = 10) y no a mpg en sí. También tenga en cuenta que tibbles imprime un número limitado de filas y columnas de forma predeterminada. La última línea de la salida impresa indica qué filas y columnas se omitieron. mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l~ f 18 29 p comp~ ## 2 audi a4 1.8 1999 4 manual~ f 21 29 p comp~ ## 3 audi a4 2 2008 4 manual~ f 20 31 p comp~ ## 4 audi a4 2 2008 4 auto(a~ f 21 30 p comp~ ## 5 audi a4 2.8 1999 6 auto(l~ f 16 26 p comp~ ## 6 audi a4 2.8 1999 6 manual~ f 18 26 p comp~ ## 7 audi a4 3.1 2008 6 auto(a~ f 18 27 p comp~ ## 8 audi a4 quat~ 1.8 1999 4 manual~ 4 18 26 p comp~ ## 9 audi a4 quat~ 1.8 1999 4 auto(l~ 4 16 25 p comp~ ## 10 audi a4 quat~ 2 2008 4 manual~ 4 20 28 p comp~ ## # ... with 224 more rows La función str() mostrará la estructura del marco de datos. Mostrará el número de observaciones y variables, enumerará las variables, dará el tipo de cada variable y mostrará algunos elementos de cada variable. Esta información también se puede encontrar en la ventana Entorno de RStudio. str(mpg) ## tibble [234 x 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... Es importante notar que mientras las matrices tienen filas y columnas,los marcos de datos (tibbles) en cambio tienen observaciones y variables. Cuando se muestra en la consola o el visor, cada fila es una observación y cada columna es una variable. Sin embargo, en términos generales, su orden no importa, es simplemente un efecto secundario de cómo se ingresaron o almacenaron los datos. En este conjunto de datos, una observación es para un modelo-año particular de un automóvil, y las variables describen atributos del automóvil, por ejemplo, su eficiencia de combustible en la carretera. Para comprender más sobre el conjunto de datos, usamos el operador ? Para abrir la documentación de los datos. ?mpg R tiene una serie de funciones para trabajar rápidamente y extraer información básica de Marcos de datos. Para obtener rápidamente un vector de los nombres de las variables, usamos la función names(). names(mpg) ## [1] &quot;manufacturer&quot; &quot;model&quot; &quot;displ&quot; &quot;year&quot; &quot;cyl&quot; ## [6] &quot;trans&quot; &quot;drv&quot; &quot;cty&quot; &quot;hwy&quot; &quot;fl&quot; ## [11] &quot;class&quot; Para acceder a una de las variables como vector, usamos el operador $. mpg$year ## [1] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 2008 ## [16] 1999 2008 2008 2008 2008 2008 1999 2008 1999 1999 2008 2008 2008 2008 2008 ## [31] 1999 1999 1999 2008 1999 2008 2008 1999 1999 1999 1999 2008 2008 2008 1999 ## [46] 1999 2008 2008 2008 2008 1999 1999 2008 2008 2008 1999 1999 1999 2008 2008 ## [61] 2008 1999 2008 1999 2008 2008 2008 2008 2008 2008 1999 1999 2008 1999 1999 ## [76] 1999 2008 1999 1999 1999 2008 2008 1999 1999 1999 1999 1999 2008 1999 2008 ## [91] 1999 1999 2008 2008 1999 1999 2008 2008 2008 1999 1999 1999 1999 1999 2008 ## [106] 2008 2008 2008 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 2008 ## [121] 2008 2008 2008 2008 1999 1999 2008 2008 2008 2008 1999 2008 2008 1999 1999 ## [136] 1999 2008 1999 2008 2008 1999 1999 1999 2008 2008 2008 2008 1999 1999 2008 ## [151] 1999 1999 2008 2008 1999 1999 1999 2008 2008 1999 1999 2008 2008 2008 2008 ## [166] 1999 1999 1999 1999 2008 2008 2008 2008 1999 1999 1999 1999 2008 2008 1999 ## [181] 1999 2008 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 1999 1999 ## [196] 1999 2008 2008 1999 2008 1999 1999 2008 1999 1999 2008 2008 1999 1999 2008 ## [211] 2008 1999 1999 1999 1999 2008 2008 2008 2008 1999 1999 1999 1999 1999 1999 ## [226] 2008 2008 1999 1999 2008 2008 1999 1999 2008 mpg$hwy ## [1] 29 29 31 30 26 26 27 26 25 28 27 25 25 25 25 24 25 23 20 15 20 17 17 26 23 ## [26] 26 25 24 19 14 15 17 27 30 26 29 26 24 24 22 22 24 24 17 22 21 23 23 19 18 ## [51] 17 17 19 19 12 17 15 17 17 12 17 16 18 15 16 12 17 17 16 12 15 16 17 15 17 ## [76] 17 18 17 19 17 19 19 17 17 17 16 16 17 15 17 26 25 26 24 21 22 23 22 20 33 ## [101] 32 32 29 32 34 36 36 29 26 27 30 31 26 26 28 26 29 28 27 24 24 24 22 19 20 ## [126] 17 12 19 18 14 15 18 18 15 17 16 18 17 19 19 17 29 27 31 32 27 26 26 25 25 ## [151] 17 17 20 18 26 26 27 28 25 25 24 27 25 26 23 26 26 26 26 25 27 25 27 20 20 ## [176] 19 17 20 17 29 27 31 31 26 26 28 27 29 31 31 26 26 27 30 33 35 37 35 15 18 ## [201] 20 20 22 17 19 18 20 29 26 29 29 24 44 29 26 29 29 29 29 23 24 44 41 29 26 ## [226] 28 29 29 29 28 29 26 26 26 Podemos utilizar las funciones dim(), nrow() y ncol() para obtener información sobre la dimensión del marco de datos. dim(mpg) ## [1] 234 11 nrow(mpg) ## [1] 234 ncol(mpg) ## [1] 11 Aquí nrow() es también el número de observaciones, que en la mayoría de los casos es el tamaño de la muestra. Subconjuntos del marcos de datos puede funcionar de manera muy similar a subconjuntos de matrices usando corchetes, [,]. Aquí, encontramos vehículos de bajo consumo de combustible que recorren más de 35 millas por galón y solo se muestra manufacturer, model y year. mpg[mpg$hwy &gt; 35, c(&quot;manufacturer&quot;, &quot;model&quot;, &quot;year&quot;)] ## # A tibble: 6 x 3 ## manufacturer model year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 honda civic 2008 ## 2 honda civic 2008 ## 3 toyota corolla 2008 ## 4 volkswagen jetta 1999 ## 5 volkswagen new beetle 1999 ## 6 volkswagen new beetle 1999 Una alternativa sería usar la función subset(), que tiene una sintaxis mucho más legible. subset(mpg, subset = hwy &gt; 35, select = c(&quot;manufacturer&quot;, &quot;model&quot;, &quot;year&quot;)) Por último, podríamos usar las funciones filter y select del paquete dplyr que introduce el operador %&gt;% del paquete magrittr. Esto no es necesario para este curso, sin embargo, el paquete dplyr es algo que se debe tener en cuenta ya que se está convirtiendo en una herramienta popular en el mundoR. library(dplyr) mpg %&gt;% filter(hwy &gt; 35) %&gt;% select(manufacturer, model, year) Los tres enfoques producen los mismos resultados. Lo que utilice se basará en gran medida en una situación determinada, así como en las preferencias del usuario. Al crear un subconjunto de un marco de datos, tenga en cuenta lo que se devuelve, ya que a veces puede ser un vector en lugar de un marco de datos. También tenga en cuenta que existen diferencias entre subconjuntos de un marco de datos y un tibble. Un marco de datos opera más como una matriz donde es posible reducir el subconjunto a un vector. Un tibble funciona más como una lista donde siempre se subconjunta a otro tibble. 3.3 Conceptos básicos de programación 3.3.1 Flujo de control En R, la sintaxis if/else es: if (...) { some R code } else { more R code } Por ejemplo, x = 1 y = 3 if (x &gt; y) { z = x * y print(&quot;x es mayor que y&quot;) } else { z = x + 5 * y print(&quot;x es menor o igual que y&quot;) } ## [1] &quot;x es menor o igual que y&quot; z ## [1] 16 R también tiene una función especial ifelse() que es muy útil. Devuelve uno de los dos valores especificados en función de una declaración condicional. ifelse(4 &gt; 3, 1, 0) ## [1] 1 El verdadero poder de ifelse() proviene de su capacidad para ser aplicado a vectores. fib = c(1, 1, 2, 3, 5, 8, 13, 21) ifelse(fib &gt; 6, &quot;Foo&quot;, &quot;Bar&quot;) ## [1] &quot;Bar&quot; &quot;Bar&quot; &quot;Bar&quot; &quot;Bar&quot; &quot;Bar&quot; &quot;Foo&quot; &quot;Foo&quot; &quot;Foo&quot; Ahora un ejemplo de bucle for, x = 11:15 for (i in 1:5) { x[i] = x[i] * 2 } x ## [1] 22 24 26 28 30 Tenga en cuenta que este bucle for es muy normal en muchos lenguajes de programación, pero no en R. En R no usaríamos un bucle, sino que simplemente usaríamos una operación vectorizada. x = 11:15 x = x * 2 x ## [1] 22 24 26 28 30 3.3.2 Funciones Hasta ahora hemos estado usando funciones, pero en realidad no hemos discutido algunos de sus detalles. function_name(arg1 = 10, arg2 = 20) Para usar una función, simplemente escriba su nombre, seguido de un paréntesis abierto, luego especifique los valores de sus argumentos y luego termine con un paréntesis de cierre. Un argumento es una variable que se usa en el cuerpo de la función. Especificar los valores de los argumentos es esencialmente proporcionar las entradas a la función. También podemos escribir nuestras propias funciones en R. Por ejemplo, a menudo nos gusta estandarizar las variables, es decir, restar la media de la muestra y dividir por la desviación estándar de la muestra. \\[ \\frac{x - \\bar{x}}{s} \\] En R escribiríamos una función para hacer esto. Al escribir una función, hay tres cosas que debe hacer. Asigne un nombre a la función. Preferiblemente algo que sea breve, pero descriptivo. Especifique los argumentos usando function() Escriba el cuerpo de la función entre llaves, {}. standardize = function(x) { m = mean(x) std = sd(x) result = (x - m) / std result } Aquí el nombre de la función es standardize, y la función tiene un solo argumento x que se usa en el cuerpo de la función. Tenga en cuenta que la salida de la línea final del cuerpo es lo que devuelve la función. En este caso, la función devuelve el vector almacenado en la variable result. Para probar nuestra función, tomaremos una muestra aleatoria de tamaño n = 10 de una distribución normal con una media de 2 y una desviación estándar de 5. (test_sample = rnorm(n = 10, mean = 2, sd = 5)) ## [1] 3.9365267 -2.6220147 8.6811189 0.6163524 0.2819761 0.7472954 ## [7] 6.4412168 3.0082455 0.8218932 2.0838996 standardize(x = test_sample) ## [1] 0.46880427 -1.53179482 1.91608135 -0.54397240 -0.64596962 -0.50402993 ## [7] 1.23282791 0.18564398 -0.48127482 -0.09631592 Esta función podría escribirse de manera mucho más corta, simplemente realizando todas las operaciones en una línea y devolviendo inmediatamente el resultado, sin almacenar ninguno de los resultados intermedios. standardize = function(x) { (x - mean(x)) / sd(x) } Al especificar argumentos, puede proporcionar argumentos predeterminados. power_of_num = function(num, power = 2) { num ^ power } Veamos varias formas en las que podríamos ejecutar esta función para realizar la operación \\(10^2\\) que da como resultado 100. power_of_num(10) ## [1] 100 power_of_num(10, 2) ## [1] 100 power_of_num(num = 10, power = 2) ## [1] 100 power_of_num(power = 2, num = 10) ## [1] 100 Tenga en cuenta que sin utilizar los nombres de los argumentos, el orden es importante. El siguiente código no se evaluará con el mismo resultado que el ejemplo anterior. power_of_num(2, 10) ## [1] 1024 Además, la siguiente línea de código produciría un error ya que se deben especificar argumentos sin un valor predeterminado. power_of_num(power = 5) Para ilustrar más una función con un argumento predeterminado, escribiremos una función que calcula la varianza muestral de dos formas. De forma predeterminada, calculará la estimación imparcial de \\(\\sigma^2\\), que llamaremos \\(s^2\\). \\[ s^2 = \\frac{1}{n - 1}\\sum_{i=1}^{n}(x - \\bar{x})^2 \\] También tendrá la capacidad de devolver la estimación sesgada (basada en la máxima probabilidad) que llamaremos \\(\\hat{\\sigma}^2\\). \\[ \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x - \\bar{x})^2 \\] get_var = function(x, biased = FALSE) { n = length(x) - 1 * !biased (1 / n) * sum((x - mean(x)) ^ 2) } get_var(test_sample) ## [1] 10.74718 get_var(test_sample, biased = FALSE) ## [1] 10.74718 var(test_sample) ## [1] 10.74718 Vemos que la función está funcionando como se esperaba, y cuando devuelve la estimación no sesgada coincide con la función incorporada de R var(). Finalmente, examinemos la estimación sesgada de \\(\\sigma^2\\). get_var(test_sample, biased = TRUE) ## [1] 9.672459 "],["resumen-de-datos.html", "Capítulo 4 Resumen de datos 4.1 Resumen estadístico 4.2 Graficas", " Capítulo 4 Resumen de datos 4.1 Resumen estadístico R tiene funciones integradas para una gran cantidad de estadísticas resumidas. Para las variables numéricas, podemos resumir los datos con las medidas de tendencia central y de dispersión Veremos nuevamente el conjunto de datos mpg del paqueteggplot2. library(ggplot2) Tendencia central Medida R Resultado Promedio mean(mpg$cty) 16.8589744 Mediana median(mpg$cty) 17 Dispersión Measure R Result Varianza var(mpg$cty) 18.1130736 Desviación Estándar sd(mpg$cty) 4.2559457 IQR IQR(mpg$cty) 5 Mínimo min(mpg$cty) 9 Máximo max(mpg$cty) 35 Rango range(mpg$cty) 9, 35 Categórica Para las variables categóricas, se pueden utilizar recuentos y porcentajes para el resumen. table(mpg$drv) ## ## 4 f r ## 103 106 25 table(mpg$drv) / nrow(mpg) ## ## 4 f r ## 0.4401709 0.4529915 0.1068376 4.2 Graficas Ahora que tenemos algunos datos con que trabajar y hemos aprendido sobre los datos en el nivel más básico, nuestra próxima tarea es visualizar los datos. A menudo, una visualización adecuada puede iluminar las características de los datos que pueden informar más análisis. Veremos cuatro métodos de visualización de datos que usaremos a lo largo del curso: Histogramas Gráfico de barras Gráfico de cajas Gráfico de dispersión 4.2.1 Histogramas Al visualizar una sola variable numérica, un histograma será nuestra herramienta de referencia, que se puede crear en R usando la función hist(). hist(mpg$cty) La función de histograma tiene una serie de parámetros que se pueden cambiar para que nuestro gráfico se vea mucho mejor. Utilice el operador ? Para leer la documentación de hist() para ver una lista completa de estos parámetros. hist(mpg$cty, xlab = &quot;Millas por galón (Ciudad)&quot;, main = &quot;Histograma de MPG (Ciudad)&quot;, breaks = 12, col = &quot;dodgerblue&quot;, border = &quot;darkorange&quot;) Es importante destacar que siempre debe asegurarse de etiquetar sus ejes y darle un título a la gráfica El argumento breaks es específico de hist(). Ingresar un número entero le dará una sugerencia a R de cuántas barras usar para el histograma. Por defecto, R intentará adivinar inteligentemente un buen número de breaks, pero como podemos ver aquí, a veces es útil modificarlo usted mismo. 4.2.2 Gráfico de barras Es algo similar a un histograma, un gráfico de barras puede proporcionar un resumen visual de una variable categórica o una variable numérica con un número finito de valores, como una clasificación del 1 al 10. barplot(table(mpg$drv)) barplot(table(mpg$drv), xlab = &quot;Drivetrain (f = FWD, r = RWD, 4 = 4WD)&quot;, ylab = &quot;Frecuencia&quot;, main = &quot;Drivetrains&quot;, col = &quot;dodgerblue&quot;, border = &quot;darkorange&quot;) 4.2.3 Diagramas de cajas Para visualizar la relación entre una variable numérica y categórica, usaremos un gráfico de cajas. En el conjunto de datos mpg, la variable drv toma un pequeño número finito de valores. Un automóvil solo puede ser de tracción delantera, 4 ruedas motrices o tracción trasera. unique(mpg$drv) ## [1] &quot;f&quot; &quot;4&quot; &quot;r&quot; En primer lugar, tenga en cuenta que podemos utilizar una gráfica de caja única como alternativa a un histograma para visualizar una única variable numérica. Para hacerlo en R, usamos la funciónboxplot(). boxplot(mpg$hwy) Sin embargo, más a menudo usaremos gráficas de cajas para comparar una variable numérica para diferentes valores de una variable categórica. boxplot(hwy ~ drv, data = mpg) Aquí usamos el comando boxplot() para crear gráficas de cajas una al lado de la otra. Sin embargo, dado que ahora estamos tratando con dos variables, la sintaxis ha cambiado. La sintaxis en R hwy ~ drv, data = mpg dice Trace la variable hwy contra la variable drv usando el conjunto de datos mpg. Vemos el uso de un ~ (que especifica una fórmula) y también un argumento data =. Esta será una sintaxis común a muchas funciones que usaremos en este curso. boxplot(hwy ~ drv, data = mpg, xlab = &quot;Drivetrain (f = FWD, r = RWD, 4 = 4WD)&quot;, ylab = &quot;Millas por galón (Carretera)&quot;, main = &quot;MPG (Carretera) vs Drivetrain&quot;, pch = 20, cex = 2, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;) Nuevamente, boxplot() tiene una serie de argumentos adicionales que tienen la capacidad de hacer que nuestra gráfica sea más atractiva visualmente. 4.2.4 Gráfico de dispersión Por último, para visualizar la relación entre dos variables numéricas usaremos un diagrama de dispersión. Esto se puede hacer con la función plot() y la sintaxis ~ que acabamos de usar con una gráfica de cajas. (La función plot() también se puede utilizar de forma más general; consulte la documentación para obtener más detalles.) plot(hwy ~ displ, data = mpg) plot(hwy ~ displ, data = mpg, xlab = &quot;Desplazamiento del motor (en litros)&quot;, ylab = &quot;Millas por galón (Carretera)&quot;, main = &quot;MPG (Carretera) vs Desplazamiento del motor&quot;, pch = 20, cex = 2, col = &quot;dodgerblue&quot;) "],["probabilidad-y-estadística-en-r.html", "Capítulo 5 Probabilidad y estadística en R 5.1 Probabilidad en R 5.2 Pruebas de hipótesis en R 5.3 Simulación", " Capítulo 5 Probabilidad y estadística en R 5.1 Probabilidad en R 5.1.1 Distribuciones Cuando trabajamos con diferentes distribuciones estadísticas, a menudo queremos hacer declaraciones probabilísticas basadas en la distribución. Por lo general, queremos saber una de cuatro cosas: La densidad (pdf) a un valor particular. La distribución (cdf) a un valor particular. El valor de un cuantil correspondiente a una probabilidad particular. Una extracción aleatoria de valores de una distribución particular. Esto solía hacerse con tablas estadísticas impresas en la parte posterior de los libros de texto. Ahora, R tiene funciones para obtener densidad, distribución, cuantiles y valores aleatorios. La estructura general de nomenclaturas de las funciones relevantes de R: dname calcula la densidad (pdf) en la entrada x. pname calcula la distribución (cdf) en la entrada x. qname calcula el cuantil con una probabilidad de entrada. rname genera valores aleatorios de una distribución particular. Tenga en cuenta que name representa el nombre de la distribución dada. Por ejemplo, considere una variable aleatoria \\(X\\) que es \\(N(\\mu = 2, \\sigma^2 = 25)\\). (Tenga en cuenta que estamos parametrizando usando la varianza \\(\\sigma^2\\). Sin embargo, R usa la desviación estándar). Para calcular el valor del pdf en x = 3, es decir, la altura de la curva en x = 3, utilice: dnorm(x = 3, mean = 2, sd = 5) ## [1] 0.07820854 Para calcular el valor de la cdf en x = 3, es decir, \\(P(X\\leq3)\\), la probabilidad de que \\(X\\) sea menor o igual que 3, utilice: pnorm(q = 3, mean = 2, sd = 5) ## [1] 0.5792597 O, para calcular el cuantil de probabilidad 0.975, use: qnorm(p = 0.975, mean = 2, sd = 5) ## [1] 11.79982 Por último, para generar una muestra aleatoria de tamaño n = 10, utilice: rnorm(n = 10, mean = 2, sd = 5) ## [1] 7.3697049 0.7003923 -3.1230623 2.9342622 8.3390906 0.8830426 ## [7] 1.9641743 1.4362463 2.8994311 1.8262463 Estas funciones existen para muchas otras distribuciones, que incluyen, entre otras: Command Distribution *binom Binomial *t t *pois Poisson *f F *chisq Chi-cuadrado Donde * puede ser d,p, q yr. Cada distribución tendrá su propio conjunto de parámetros que deben pasarse a las funciones como argumentos. Por ejemplo, dbinom() no tendría argumentos para mean ysd, ya que no son parámetros de la distribución. En cambio, una distribución binomial suele estar parametrizada por \\(n\\) y \\(p\\), sin embargo, R elige llamarlos de otra manera. Para encontrar los nombres que usa R, usaríamos ?dbinom y veremos que R en su lugar llama a los argumentossize y prob. Por ejemplo: dbinom(x = 6, size = 10, prob = 0.75) ## [1] 0.145998 También tenga en cuenta que, cuando se utilizan las funciones dname con distribuciones discretas, son las pmf de la distribución. Por ejemplo, el comando anterior es \\(P(Y = 6)\\) si \\(Y\\sim b(n = 10, p = 0,75)\\). (La probabilidad de lanzar una moneda injusta 10 veces y ver 6 caras, si la probabilidad de que salga cara es 0.75). 5.2 Pruebas de hipótesis en R Un requisito previo es la comprensión de los conceptos básicos de la prueba de hipótesis. Recuerde la estructura básica de las pruebas de hipótesis: Se realiza un modelo general y los supuestos relacionados. (Las más comunes son las observaciones que siguen una distribución normal). Se especifican las hipótesis nula (\\(H_ {0}\\)) y alternativa (\\(H_ {1}\\) o \\(H_{A}\\)). Por lo general, la nula especifica un valor particular de un parámetro. Con los datos dados, se calcula el valor del estadístico de prueba. Bajo los supuestos generales, además de asumir que la hipótesis nula es cierta, se conoce la distribución del estadístico de prueba. Dada la distribución y el valor del estadístico de la prueba, así como la forma de la hipótesis alternativa, podemos calcular un valor p de la prueba. Basándonos en el valor p y el nivel de significancia preespecificado, tomamos una decisión: No rechazar la hipótesis nula. Rechazar la hipótesis nula. Haremos una revisión rápida de dos de las pruebas más comunes para mostrar cómo se realizan usando R. 5.2.1 Prueba t de una muestra: revisión Suponer que \\(x_{i} \\sim \\mathrm{N}(\\mu,\\sigma^{2})\\) Y nosotros queremos probar \\(H_{0}: \\mu = \\mu_{0}\\) versus \\(H_{1}: \\mu \\neq \\mu_{0}.\\) Asumiendo que \\(\\sigma\\) es desconocido, usamos la estadística de prueba \\(t\\) de Student de una muestra: \\[ t = \\frac{\\bar{x}-\\mu_{0}}{s/\\sqrt{n}} \\sim t_{n-1}, \\] donde \\(\\bar{x} = \\displaystyle\\frac{\\sum_{i=1}^{n}x_{i}}{n}\\) y \\(s = \\sqrt{\\displaystyle\\frac{1}{n - 1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\). un intervalo de confianza al \\(100(1 - \\alpha)\\)% para \\(\\mu\\) esta dado por, \\[ \\bar{x} \\pm t_{n-1}(\\alpha/2)\\frac{s}{\\sqrt{n}} \\] donde \\(t_{n-1}(\\alpha/2)\\) es el valor crítico tal que \\(P\\left(t&gt;t_{n-1}(\\alpha/2)\\right) = \\alpha/2\\) con \\(n-1\\) grados de libertad. 5.2.2 Prueba t de una muestra: ejemplo Suponga que una tienda vende cajas de 16 onzas de cereal Captain Crisp. Se tomó y pesó una muestra aleatoria de 9 cajas. El peso en onzas se almacena en el marco de datos capt_crisp. capt_crisp = data.frame(weight = c(15.5, 16.2, 16.1, 15.8, 15.6, 16.0, 15.8, 15.9, 16.2)) La compañía que fabrica el cereal Captain Crisp afirma que el peso promedio de una caja es de al menos 16 onzas. Supondremos que el peso del cereal en una caja se distribuye normalmente y usaremos un nivel de significancia de 0.05 para probar la afirmación de la compañía. Probar \\(H_{0}: \\mu \\geq 16\\) versus \\(H_{1}: \\mu &lt; 16\\), el estadístico de prueba es \\[ t = \\frac{\\bar{x} - \\mu_{0}}{s / \\sqrt{n}} \\] La media muestral \\(\\bar{x}\\) y la desviación estándar muestral \\(s\\) pueden calcularse fácilmente usando R. También creamos variables que almacenan la media hipotética y el tamaño de la muestra. x_bar = mean(capt_crisp$weight) s = sd(capt_crisp$weight) mu_0 = 16 n = 9 Entonces podemos calcular fácilmente la estadística de prueba. t = (x_bar - mu_0) / (s / sqrt(n)) t ## [1] -1.2 Bajo la hipótesis nula, el estadístico de prueba tiene una distribución \\(t\\) con \\(n - 1\\) grados de libertad, en este caso 8. Para completar la prueba, necesitamos obtener el valor p de la prueba. Dado que esta es una prueba unilateral con una alternativa menor que, necesitamos el área a la izquierda de -1.2 para una distribución \\(t\\) con 8 grados de libertad. Es decir, \\[ P(t_{8} &lt; -1.2) \\] pt(t, df = n - 1) ## [1] 0.1322336 Ahora tenemos el valor p de nuestra prueba, que es mayor que nuestro nivel de significancia (0.05), por lo que no rechazamos la hipótesis nula. Alternativamente, todo este proceso podría haberse completado usando una línea de código en R. t.test(x = capt_crisp$weight, mu = 16, alternative = c(&quot;less&quot;), conf.level = 0.95) ## ## One Sample t-test ## ## data: capt_crisp$weight ## t = -1.2, df = 8, p-value = 0.1322 ## alternative hypothesis: true mean is less than 16 ## 95 percent confidence interval: ## -Inf 16.05496 ## sample estimates: ## mean of x ## 15.9 Proporcionamos a R los datos, el valor hipotético de \\(\\mu\\), la alternativa y el nivel de confianza. R luego devuelve una gran cantidad de información que incluye: El valor del estadístico de prueba. Los grados de libertad de la distribución bajo la hipótesis nula. El valor p de la prueba. El intervalo de confianza que corresponde a la prueba. Una estimación de \\(\\mu\\). Dado que la prueba fue unilateral, R arrojó un intervalo de confianza unilateral. Si por el contrario quisiéramos un intervalo de dos caras para el peso medio de las cajas de cereal Captain Crisp, podríamos modificar nuestro código. capt_test_results = t.test(capt_crisp$weight, mu = 16, alternative = c(&quot;two.sided&quot;), conf.level = 0.95) Esta vez hemos almacenado los resultados. Al hacerlo, podemos acceder directamente a partes de la salida de t.test(). Para ver qué información está disponible usamos la función names(). names(capt_test_results) ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; ## [6] &quot;null.value&quot; &quot;stderr&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; Estamos interesados en el intervalo de confianza que se almacena en conf.int. capt_test_results$conf.int ## [1] 15.70783 16.09217 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 Comprobemos este intervalo a mano. La única información que nos falta es el valor crítico, \\(t_{n-1}(\\alpha/2) = t_{8}(0.025)\\), que se puede calcular en R usando la función qt(). qt(0.975, df = 8) ## [1] 2.306004 Entonces, el IC del 95% para el peso medio de una caja de cereal se calcula ingresando a la fórmula, \\[ \\bar{x} \\pm t_{n-1}(\\alpha/2) \\frac{s}{\\sqrt{n}} \\] c(mean(capt_crisp$weight) - qt(0.975, df = 8) * sd(capt_crisp$weight) / sqrt(9), mean(capt_crisp$weight) + qt(0.975, df = 8) * sd(capt_crisp$weight) / sqrt(9)) ## [1] 15.70783 16.09217 5.2.3 Prueba t de dos muestras: revisión Suponer que \\(x_{i} \\sim \\mathrm{N}(\\mu_{x}, \\sigma^{2})\\) y \\(y_{i} \\sim \\mathrm{N}(\\mu_{y}, \\sigma^{2}).\\) Se quiere probar \\(H_{0}: \\mu_{x} - \\mu_{y} = \\mu_{0}\\) versus \\(H_{1}: \\mu_{x} - \\mu_{y} \\neq \\mu_{0}.\\) Asumiendo que \\(\\sigma\\) es desconocido, use la estadística de prueba \\(t\\) de Student de dos muestras: \\[ t = \\frac{(\\bar{x} - \\bar{y})-\\mu_{0}}{s_{p}\\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim t_{n+m-2}, \\] donde \\(\\displaystyle\\bar{x}=\\frac{\\sum_{i=1}^{n}x_{i}}{n}\\), \\(\\displaystyle\\bar{y}=\\frac{\\sum_{i=1}^{m}y_{i}}{m}\\), y \\(s_p^2 = \\displaystyle\\frac{(n-1)s_x^2+(m-1)s_y^2}{n+m-2}\\). Un IC al \\(100(1-\\alpha)\\)% para \\(\\mu_{x}-\\mu_{y}\\) esta dado por \\[ (\\bar{x} - \\bar{y}) \\pm t_{n+m-2}(\\alpha/2) \\left(s_{p}\\textstyle\\sqrt{\\frac{1}{n}+\\frac{1}{m}}\\right), \\] donde \\(t_{n+m-2}(\\alpha/2)\\) es el valor crítico tal que \\(P\\left(t&gt;t_{n+m-2}(\\alpha/2)\\right)=\\alpha/2\\). 5.2.4 Prueba t de dos muestras: Ejemplo Suponga que las distribuciones de \\(X\\) y \\(Y\\) son \\(\\mathrm{N}(\\mu_{1},\\sigma^{2})\\) y \\(\\mathrm{N}(\\mu_{2},\\sigma^{2})\\), respectivamente. Dadas las \\(n = 6\\) observaciones de \\(X\\), x = c(70, 82, 78, 74, 94, 82) n = length(x) y las \\(m = 8\\) observaciones de \\(Y\\), y = c(64, 72, 60, 76, 72, 80, 84, 68) m = length(y) Probaremos \\(H_{0}: \\mu_{1} = \\mu_{2}\\) versus \\(H_{1}: \\mu_{1} &gt; \\mu_{2}\\). Primero, tenga en cuenta que podemos calcular las medias muestrales y las desviaciones estándar. x_bar = mean(x) s_x = sd(x) y_bar = mean(y) s_y = sd(y) Luego podemos calcular la desviación estándar combinada. \\[ s_{p} = \\sqrt{\\frac{(n-1)s_{x}^{2}+(m-1)s_{y}^{2}}{n+m-2}} \\] s_p = sqrt(((n - 1) * s_x ^ 2 + (m - 1) * s_y ^ 2) / (n + m - 2)) Por lo tanto, el estadístico de prueba \\(t\\) relevante viene dado por \\[ t = \\frac{(\\bar{x}-\\bar{y})-\\mu_{0}}{s_{p}\\sqrt{\\frac{1}{n}+\\frac{1}{m}}}. \\] t = ((x_bar - y_bar) - 0) / (s_p * sqrt(1 / n + 1 / m)) t ## [1] 1.823369 Tenga en cuenta que \\(t \\sim t_{n + m - 2} = t_{12}\\), para que podamos calcular el valor p, que es \\[ P(t_{12} &gt; 1.8233692). \\] 1 - pt(t, df = n + m - 2) ## [1] 0.04661961 Pero, de nuevo, podríamos haber realizado simplemente esta prueba en una línea de R. t.test(x, y, alternative = c(&quot;greater&quot;), var.equal = TRUE) ## ## Two Sample t-test ## ## data: x and y ## t = 1.8234, df = 12, p-value = 0.04662 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.1802451 Inf ## sample estimates: ## mean of x mean of y ## 80 72 Recuerde que una prueba \\(t\\) de dos muestras se puede realizar con o sin un supuesto de varianza igual. Aquí, var.equal = TRUE le dice aR que nos gustaría realizar la prueba bajo el supuesto de varianza igual. Arriba llevamos a cabo el análisis usando dos vectores x y y. En general, tendremos preferencia por usar marcos de datos. t_test_data = data.frame(values = c(x, y), group = c(rep(&quot;A&quot;, length(x)), rep(&quot;B&quot;, length(y)))) Ahora tenemos los datos almacenados en una sola variable (values) y hemos creado una segunda variable (group) que indica a qué muestra pertenece el valor. t_test_data ## values group ## 1 70 A ## 2 82 A ## 3 78 A ## 4 74 A ## 5 94 A ## 6 82 A ## 7 64 B ## 8 72 B ## 9 60 B ## 10 76 B ## 11 72 B ## 12 80 B ## 13 84 B ## 14 68 B Ahora, para realizar la prueba, todavía usamos la función t.test() pero con la sintaxis ~ y un argumento data. t.test(values ~ group, data = t_test_data, alternative = c(&quot;greater&quot;), var.equal = TRUE) ## ## Two Sample t-test ## ## data: values by group ## t = 1.8234, df = 12, p-value = 0.04662 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.1802451 Inf ## sample estimates: ## mean in group A mean in group B ## 80 72 5.3 Simulación La simulación y el ajuste del modelo son procesos relacionados pero opuestos. En simulación, se conoce el proceso de generación de datos. Conoceremos la forma del modelo así como el valor de cada uno de los parámetros. En particular, a menudo controlaremos la distribución y los parámetros que definen la aleatoriedad o el ruido en los datos. En ajuste de modelo, se conocen los datos. Luego asumiremos una cierta forma de modelo y encontraremos los mejores valores posibles de los parámetros dados los datos observados. Esencialmente buscamos descubrir la verdad. A menudo, intentaremos ajustarnos a muchos modelos y aprenderemos métricas para evaluar qué modelo encaja mejor. Simulación vs modelado A menudo, simularemos datos de acuerdo con un proceso que decidamos, posteriormente usaremos un método de modelado visto. Luego podemos verificar qué tan bien funciona el método, ya que conocemos el proceso de generación de datos. Una de las mayores fortalezas de R es su capacidad para realizar simulaciones utilizando funciones integradas para generar muestras aleatorias a partir de ciertas distribuciones. Veremos dos ejemplos muy simples aquí. 5.3.1 Diferencias emparejadas Consider the model: \\[ \\begin{split} X_{11}, X_{12}, \\ldots, X_{1n} \\sim N(\\mu_1,\\sigma^2)\\\\ X_{21}, X_{22}, \\ldots, X_{2n} \\sim N(\\mu_2,\\sigma^2) \\end{split} \\] Se asume que \\(\\mu_1 = 6\\), \\(\\mu_2 = 5\\), \\(\\sigma^2 = 4\\) y \\(n = 25\\). \\[ \\begin{aligned} \\bar{X}_1 &amp;= \\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}X_{1i}\\\\ \\bar{X}_2 &amp;= \\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}X_{2i}\\\\ D &amp;= \\bar{X}_1 - \\bar{X}_2. \\end{aligned} \\] Supongamos que nos gustaría calcular \\(P(0&lt;D&lt;2)\\). Primero necesitaremos obtener la distribución de \\(D\\). Ahora, \\[ \\bar{X}_1 \\sim N\\left(\\mu_1,\\frac{\\sigma^2}{n}\\right) \\] y \\[ \\bar{X}_2 \\sim N\\left(\\mu_2,\\frac{\\sigma^2}{n}\\right). \\] Luego, \\[ D = \\bar{X}_1 - \\bar{X}_2 \\sim N\\left(\\mu_1-\\mu_2, \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n}\\right) = N\\left(6-5, \\frac{4}{25} + \\frac{4}{25}\\right). \\] Entnonces, \\[ D \\sim N(\\mu = 1, \\sigma^2 = 0.32). \\] Thus, \\[ P(0 &lt; D &lt; 2) = P(D &lt; 2) - P(D &lt; 0). \\] Esto se puede calcular usando R sin necesidad de estandarizar primero o usar una tabla. pnorm(2, mean = 1, sd = sqrt(0.32)) - pnorm(0, mean = 1, sd = sqrt(0.32)) ## [1] 0.9229001 Un enfoque alternativo sería simular una gran cantidad de observaciones de \\(D\\) y luego usar la distribución empírica para calcular la probabilidad. Nuestra estrategia será: Generar una muestra de 25 observaciones aleatorias a partir de \\(N(\\mu_1 = 6,\\sigma^2 = 4)\\). Llamar a la media de esta muestra \\(\\bar {x}_{1s}\\). Generar una muestra de 25 observaciones aleatorias a partir de \\(N(\\mu_1 = 5,\\sigma^2 = 4)\\). Llamar a la media de esta muestra ${x}_{2s} $. Calcular las diferencias de las medias, \\(d_s=\\bar{x}_{1s}-\\bar{x}_{2s}\\). Repetir el proceso un gran número de veces. Luego usar la distribución de las observaciones simuladas de \\(d_s\\) como una estimación de la verdadera distribución de \\(D\\). set.seed(42) num_samples = 10000 differences = rep(0, num_samples) Antes de iniciar el ciclo for para realizar la operación, establecer una semilla para la reproducibilidad, crear y configurar una variable num_samples que definirá el número de repeticiones y, por último, crear una variable differences que almacenará los valores simulados, \\(d_s\\). Usando set.seed() podemos reproducir los resultados aleatorios de rnorm() cada vez que comience desde esa línea. for (s in 1:num_samples) { x1 = rnorm(n = 25, mean = 6, sd = 2) x2 = rnorm(n = 25, mean = 5, sd = 2) differences[s] = mean(x1) - mean(x2) } Para estimar $P(0&lt;D&lt;2) $ encontrar la proporción de valores de \\(d_s\\) (entre los 10^{4} valores de \\(d_s\\) generados) que están entre 0 y 2. mean(0 &lt; differences &amp; differences &lt; 2) ## [1] 0.9222 Recuerde que arriba la distribución de \\(D\\) es \\(N(\\mu=1,\\sigma^2=0.32)\\) Si miramos un histograma de las diferencias, encontramos que se parece mucho a una distribución normal. hist(differences, breaks = 20, main = &quot;Distribución empírica de D&quot;, xlab = &quot;Valores simulados de D&quot;, col = &quot;dodgerblue&quot;, border = &quot;darkorange&quot;) Además, la media y la varianza de la muestra están muy cerca de lo que cabría esperar. mean(differences) ## [1] 1.001423 var(differences) ## [1] 0.3230183 También podríamos haber logrado esta tarea con una sola línea de R más idiomática. set.seed(42) diffs = replicate(10000, mean(rnorm(25, 6, 2)) - mean(rnorm(25, 5, 2))) Use ?Replicate para echar un vistazo a la documentación de la función replicate y vea si puede entender cómo esta línea realiza las mismas operaciones que ejecutó nuestro ciclo for anterior. mean(differences == diffs) ## [1] 1 Vemos que al establecer la misma semilla para la aleatorización, ¡obtenemos resultados idénticos! 5.3.2 Distribución de una media muestral Para otro ejemplo de simulación, simularemos observaciones de una distribución de Poisson y examinaremos la distribución empírica de la media muestral de estas observaciones. Recuerde, si \\[ X \\sim Pois(\\mu) \\] luego \\[ E[X] = \\mu \\] y \\[ Var[X] = \\mu. \\] Además, recuerde que para una variable aleatoria \\(X\\) con media finita \\(\\mu\\) y varianza finita \\(\\sigma^2\\), el teorema del límite central nos dice que la media, \\(\\bar{X}\\) de una muestra aleatoria de tamaño \\(n\\) es aproximadamente normal para valores grandes de \\(n\\). Específicamente, como $n$, \\[ \\bar{X} \\overset{d}{\\to} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right). \\] Lo siguiente verifica este resultado para una distribución de Poisson con \\(\\mu=10\\) y un tamaño de muestra de \\(n=50\\). set.seed(1337) mu = 10 sample_size = 50 samples = 100000 x_bars = rep(0, samples) for(i in 1:samples){ x_bars[i] = mean(rpois(sample_size, lambda = mu)) } x_bar_hist = hist(x_bars, breaks = 50, main = &quot;Histograma de medias muestrales&quot;, xlab = &quot;Medias muestrales&quot;) Ahora compararemos las estadísticas de muestra de la distribución empírica con sus valores conocidos basados en la distribución principal. c(mean(x_bars), mu) ## [1] 10.00008 10.00000 c(var(x_bars), mu / sample_size) ## [1] 0.1989732 0.2000000 c(sd(x_bars), sqrt(mu) / sqrt(sample_size)) ## [1] 0.4460641 0.4472136 Y aquí, calcularemos la proporción de medias muestrales que están dentro de 2 desviaciones estándar de la media poblacional. mean(x_bars &gt; mu - 2 * sqrt(mu) / sqrt(sample_size) &amp; x_bars &lt; mu + 2 * sqrt(mu) / sqrt(sample_size)) ## [1] 0.95429 Este último histograma utiliza un truco para sombrear aproximadamente las barras que están dentro de dos desviaciones estándar de la media). shading = ifelse(x_bar_hist$breaks &gt; mu - 2 * sqrt(mu) / sqrt(sample_size) &amp; x_bar_hist$breaks &lt; mu + 2 * sqrt(mu) / sqrt(sample_size), &quot;darkorange&quot;, &quot;dodgerblue&quot;) x_bar_hist = hist(x_bars, breaks = 50, col = shading, main = &quot;Histograma de medias muestrales, Dos desviaciones estándar&quot;, xlab = &quot;Medias muestrales&quot;) "],["recursos-r.html", "Capítulo 6 Recursos R 6.1 Referencias y tutoriales para principiantes 6.2 Referencias intermedias 6.3 Referencias avanzadas 6.4 Comparaciones rápidas con otros lenguajes 6.5 Vídeos de RStudio y RMarkdown", " Capítulo 6 Recursos R Hasta ahora, hemos visto una gran cantidad de R y rápidamente. Nuevamente, los capítulos anteriores no pretendían ser de ninguna manera una referencia completa para el lenguaje R, sino más bien una introducción a muchos de los conceptos que necesitaremos en este texto. Los siguientes recursos no son necesarios para el resto de este texto, pero pueden resultarle útiles si desea una comprensión más profunda de R: 6.1 Referencias y tutoriales para principiantes Try R from Code School. Una introducción interactiva a los conceptos básicos de R. Útil para ponerse al día con la sintaxis de R. Quick-R by Robert Kabacoff. Una buena referencia para los conceptos básicos de R. R Tutorial by Chi Yau. Una combinación de referencia y tutorial para los conceptos básicos de R. R Programming for Data Science by Roger Peng Un gran texto para principiantes en programación con R. Muestra R desde cero, destacando detalles de programación que quizás no discutimos. 6.2 Referencias intermedias R for Data Science by Hadley Wickham and Garrett Grolemund. Similar a la R avanzado, pero se enfoca más en el análisis de datos, al mismo tiempo que introduce conceptos de programación. Especialmente útil para trabajar en tidyverse. The Art of R Programming by Norman Matloff. Suave introducción al lado de la programación de R. (Mientras que nos centraremos más en el análisis de datos). una versión electrónica gratuita está disponible a través de la biblioteca de Illinois. 6.3 Referencias avanzadas Advanced R by Hadley Wickham. Del autor de varios paquetes R extremadamente populares. Buen seguimiento de El arte de la programación R. (Y material más actualizado). The R Inferno by Patrick Burns. Se compara el aprendizaje de los trucos de R con descender a través de los niveles del infierno. Material muy avanzado, pero puede ser importante si R se convierte en parte de su caja de herramientas diaria. Efficient R Programming by Colin Gillespie and Robin Lovelace Analiza los programas eficientes de R, así como la programación enR de manera eficiente. 6.4 Comparaciones rápidas con otros lenguajes Aquellos que estén familiarizados con otros idiomas pueden encontrar útiles las siguientes hojas de trucos para realizar la transición a la R. MATLAB, NumPy, Julia Stata 6.5 Vídeos de RStudio y RMarkdown Las siguientes listas de reproducción de videos se crearon como una introducción a R, RStudio y RMarkdown. R and RStudio Data in R RMarkdown Tenga en cuenta que RStudio y RMarkdown reciben constantemente un excelente soporte y actualizaciones, por lo que estos videos pueden contener información desactualizada. RStudio proporciona su propio tutorial para RMarkdown. Ellos tambien tienen una excelente Hojas de trucos de RStudio que identifica visualmente muchas de las funciones disponibles en el IDE. "],["regresión-lineal-simple.html", "Capítulo 7 Regresión lineal simple 7.1 Modelado 7.2 Enfoque de mínimos cuadrados 7.3 Descomposición de variación 7.4 La función lm 7.5 Enfoque de estimación de máxima verosimilitud (MLE) 7.6 Simulando SLR 7.7 Historia", " Capítulo 7 Regresión lineal simple Todos los modelos son incorrectos, pero algunos son útiles..  George E. P. Box Después de leer este capítulo, podrá: Comprender el concepto de modelo. Describir dos formas en las que se derivan los coeficientes de regresión. Estimar y visualizar un modelo de regresión usando R. Interpretar coeficientes de regresión y estadística en el contexto de problemas del mundo real. Utilizar un modelo de regresión para realizar predicciones. 7.1 Modelado Consideremos un ejemplo simple de cómo la velocidad de un automóvil afecta su distancia de frenado, es decir, qué tan lejos avanza antes de detenerse. Para examinar esta relación, usaremos el conjunto de datos cars que es un conjunto de datos predeterminado de R. Por lo tanto, no necesitamos cargar un paquete; está disponible de inmediato. Para echar un primer vistazo a los datos, puede usar la función View() dentro de RStudio. View(cars) También podríamos echar un vistazo a los nombres de las variables, la dimensión del marco de datos y algunas observaciones de muestra con str(). str(cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... Como hemos visto antes con los marcos de datos, hay una serie de funciones adicionales para acceder directamente a parte de esta información. dim(cars) ## [1] 50 2 nrow(cars) ## [1] 50 ncol(cars) ## [1] 2 Aparte de los dos nombres de variables y el número de observaciones, estos datos siguen siendo solo un montón de números, por lo que probablemente deberíamos obtener algo de contexto. ?cars Al leer la documentación, nos enteramos de que se trata de datos recopilados durante la década de 1920 sobre la velocidad de los automóviles y la distancia resultante que tarda el automóvil en detenerse. La tarea interesante es determinar qué tan lejos viaja un automóvil antes de detenerse, cuando viaja a cierta velocidad. Entonces, primero graficaremos la distancia de frenado contra la velocidad. plot(dist ~ speed, data = cars, xlab = &quot;Velocidad (en millas por hora)&quot;, ylab = &quot;Distancia de frenado (en pies)&quot;, main = &quot;Distancia de frenado vs velocidad&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) Definamos ahora algo de terminología. Tenemos pares de datos, \\((x_i, y_i)\\), para \\(i = 1, 2, \\ldots n\\), donde \\(n\\) es el tamaño de muestra del conjunto de datos. Usamos \\(i\\) como índice, simplemente como notación. Usamos \\(x_i\\) como la variable predictora (explicativa). La variable predictiva se utiliza para ayudar a predecir o explicar la variable respuesta (objetivo, resultado), \\(y_i\\). Otros textos pueden usar el término variable independiente en lugar de predictor y variable dependiente en lugar de respuesta. Sin embargo, esos apodos implican características matemáticas que podrían no ser ciertas. Si bien estos otros términos no son incorrectos, la independencia ya es un concepto estrictamente definido en probabilidad. Por ejemplo, al intentar predecir el peso de una persona dada su altura, ¿sería correcto decir que la altura es independiente del peso? Ciertamente no, pero eso es una implicación involuntaria de decir variable independiente. Preferimos alejarnos de esta nomenclatura. En el ejemplo de cars, estamos interesados en usar la variable predictora speed para predecir y explicar la variable respuesta dist. En términos generales, nos gustaría modelar la relación entre \\(X\\) y \\(Y\\) usando la forma \\[ Y = f(X) + \\epsilon. \\] La función \\(f\\) describe la relación funcional entre las dos variables, y el término \\(\\epsilon\\) se usa para dar cuenta del error. Esto indica que si ingresamos un valor dado de \\(X\\) como entrada, nuestra salida es un valor de \\(Y\\), dentro de un cierto rango de error. Puedes pensar en esto de varias maneras: Respuesta = Predicción + Error Respuesta = Señal + Ruido Respuesta = Modelo + Inexplicable Respuesta = determinista + aleatoria Respuesta = Explicable + Inexplicable ¿Qué tipo de función deberíamos usar para \\(f(X)\\) para los datos cars? Podríamos intentar modelar los datos con una línea horizontal. Es decir, el modelo para \\(y\\) no depende del valor de \\(x\\). (Alguna función \\(f(X) = c\\).) En la gráfica de abajo, vemos que esto no parece funcionar muy bien. Muchos de los puntos de datos están muy lejos de la línea naranja que representa \\(c\\). Este es un ejemplo de desajuste. La solución obvia es hacer que la función \\(f(X)\\) realmente dependa de \\(x\\). También podríamos intentar modelar los datos con una función muy ondulada que intenta pasar por tantos puntos de datos como sea posible. Esto tampoco parece funcionar muy bien. ¡La distancia de frenado para una velocidad de 5 mph no debería estar fuera de la tabla! (Incluso en 1920). Este es un ejemplo de sobreajuste. (Tenga en cuenta que en este ejemplo ninguna función pasará por todos los puntos, ya que hay algunos valores \\(x\\) que tienen varios valores \\(y\\) posibles en los datos). Por último, podríamos intentar modelar los datos eligiendo bien una línea en lugar de uno de los dos extremos que se intentaron anteriormente. La línea del siguiente gráfico parece resumir bastante bien la relación entre la distancia de frenado y la velocidad. A medida que aumenta la velocidad, aumenta la distancia necesaria para detenerse. Todavía hay alguna variación en esta línea, pero parece capturar la tendencia general. Con esto en mente, nos gustaría restringir nuestra elección de\\(f(X)\\) a funciones lineales de \\(X\\). Escribiremos nuestro modelo usando \\(\\beta_1\\) para la pendiente y \\(\\beta_0\\) para la intersección, \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon. \\] 7.1.1 Modelo de regresión lineal simple Ahora definimos lo que llamaremos el modelo de regresión lineal simple, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] donde \\[ \\epsilon_i \\sim N(0, \\sigma^2). \\] Es decir, los \\(\\epsilon_i\\) son variables aleatorias normales independientes e idénticamente distribuidas (iid) con media \\(0\\) y varianza \\(\\sigma^2\\). Este modelo tiene tres parámetros para estimar: \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma^2\\), que son constantes fijas pero desconocidas. Hemos modificado ligeramente nuestra notación. Ahora estamos usando \\(Y_i\\) y \\(x_i\\), ya que ajustaremos este modelo a un conjunto de \\(n\\) puntos de datos, para \\(i = 1, 2, \\ldots n\\). Recuerde que usamos \\(Y\\) mayúscula para indicar una variable aleatoria y \\(y\\) minúscula para denotar un valor potencial de la variable aleatoria. Como tendremos \\(n\\) observaciones, tenemos \\(n\\) variables aleatorias \\(Y_i\\) y sus posibles valores \\(y_i\\). En el modelo de regresión lineal simple, se supone que \\(x_i\\) son constantes fijas conocidas y, por lo tanto, se anotan con una variable en minúscula. La respuesta \\(Y_i\\) sigue siendo una variable aleatoria debido al comportamiento aleatorio de la variable de error, \\(\\epsilon_i\\). Es decir, cada respuesta \\(Y_i\\) está vinculada a un \\(x_i\\) observable y un \\(\\epsilon_i\\) aleatorio, no observable. Esencialmente, podríamos pensar explícitamente que \\(Y_i\\) tiene una distribución diferente para cada \\(X_i\\). En otras palabras, \\(Y_i\\) tiene una distribución condicional que depende del valor de \\(X_i\\), escrito \\(x_i\\). Al hacerlo, todavía no hacemos suposiciones de distribución de \\(X_i\\), ya que solo estamos interesados en la distribución de \\(Y_i\\) para un valor particular \\(x_i\\). \\[ Y_i \\mid X_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) \\] Los \\(Y_i\\) aleatorios son una función de \\(x_i\\), por lo que podemos escribir su media como una función de \\(x_i\\), \\[ \\text{E}[Y_i \\mid X_i = x_i] = \\beta_0 + \\beta_1 x_i. \\] Sin embargo, su varianza permanece constante para cada \\(x_i\\), \\[ \\text{Var}[Y_i \\mid X_i = x_i] = \\sigma^2. \\] Esto se muestra visualmente en la siguiente imagen, Vemos que para cualquier valor \\(x\\), el valor esperado de \\(Y\\) es \\(\\beta_0 + \\beta_1 x\\). En cada valor de \\(x\\), \\(Y\\) tiene la misma varianza \\(\\sigma^2\\). Simple Linear Regression Model Introductory Statistics (Shafer and Zhang), UC Davis Stat Wiki A menudo, hablamos directamente sobre las suposiciones que hace este modelo. Lineal. La relación entre \\(Y\\) y \\(x\\) es lineal, de la forma \\(\\beta_0 + \\beta_1 x\\). Independiente. Los errores \\(\\epsilon\\) son independientes. Normal. Los errores \\(\\epsilon\\) se distribuyen normalmente. Ese es el error alrededor de la línea, sigue una distribución normal. Igualdad de varianza. En cada valor de \\(x\\), la varianza de \\(Y\\) es la misma, \\(\\sigma^2\\). También asumimos que los valores de \\(x\\) son fijos, es decir, no aleatorios. No hacemos una suposición distributiva sobre la variable predictora. Como nota al margen, a menudo nos referiremos a la regresión lineal simple como SLR, por sus siglas en inglés. Una explicación del nombre SLR: Simple se refiere al hecho de que estamos utilizando una única variable predictora. Posteriormente usaremos múltiples variables predictoras. Linear nos dice que nuestro modelo para \\(Y\\) es una combinación lineal de los predictores \\(X\\). (En este caso solo uno.) Ahora mismo, esto siempre da como resultado un modelo que es una línea, pero más adelante veremos cómo no siempre es así. Regression simplemente significa que estamos intentando medir la relación entre una variable respuesta y (una o más) variables predictoras. En el caso de SLR, tanto la respuesta como el predictor son variables numéricas. Entonces, SLR modela \\(Y\\) como una función lineal de \\(X\\), pero ¿cómo definimos realmente una buena línea? Hay un número infinito de líneas que podríamos usar, por lo que intentaremos encontrar una con pequeños errores. Esa es una línea con tantos puntos como sea posible. La pregunta ahora es, ¿cómo encontramos esa línea? Hay muchos enfoques que podríamos tomar. Podríamos encontrar la línea que tiene la distancia máxima más pequeña desde cualquiera de los puntos a la línea. Es decir, \\[ \\underset{\\beta_0, \\beta_1}{\\mathrm{argmin}} \\max|y_i - (\\beta_0 + \\beta_1 x_i)|. \\] Podríamos encontrar la línea que minimiza la suma de todas las distancias desde los puntos a la línea. Es decir, \\[ \\underset{\\beta_0, \\beta_1}{\\mathrm{argmin}} \\sum_{i = 1}^{n}|y_i - (\\beta_0 + \\beta_1 x_i)|. \\] Podríamos encontrar la línea que minimiza la suma de todas las distancias al cuadrado desde los puntos hasta la línea. Es decir, \\[ \\underset{\\beta_0, \\beta_1}{\\mathrm{argmin}} \\sum_{i = 1}^{n}(y_i - (\\beta_0 + \\beta_1 x_i))^2. \\] Esta última opción se llama método de mínimos cuadrados. Es esencialmente el método de hecho para ajustar una línea a los datos. (Es posible que lo haya visto antes en un curso de álgebra lineal). Su popularidad se debe en gran parte al hecho de que es matemáticamente fácil. (Lo cual fue importante históricamente, ya que las computadoras son un artilugio moderno). También es muy popular porque muchas relaciones están bien aproximadas por una función lineal. 7.2 Enfoque de mínimos cuadrados Dadas las observaciones \\((x_i, y_i)\\), para \\(i = 1, 2, \\ldots n\\), queremos encontrar valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimicen \\[ f(\\beta_0, \\beta_1) = \\sum_{i = 1}^{n}(y_i - (\\beta_0 + \\beta_1 x_i))^2 = \\sum_{i = 1}^{n}(y_i - \\beta_0 - \\beta_1 x_i)^2. \\] Llamaremos a estos valores \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\). Primero, tomamos una derivada parcial con respecto a \\(\\beta_0\\) y \\(\\beta_1\\). \\[ \\begin{aligned} \\frac{\\partial f}{\\partial \\beta_0} &amp;= -2 \\sum_{i = 1}^{n}(y_i - \\beta_0 - \\beta_1 x_i) \\\\ \\frac{\\partial f}{\\partial \\beta_1} &amp;= -2 \\sum_{i = 1}^{n}(x_i)(y_i - \\beta_0 - \\beta_1 x_i) \\end{aligned} \\] Luego igualamos a cero cada una de las derivadas parciales y resolvemos el sistema de ecuaciones resultante. \\[ \\begin{aligned} \\sum_{i = 1}^{n}(y_i - \\beta_0 - \\beta_1 x_i) &amp;= 0 \\\\ \\sum_{i = 1}^{n}(x_i)(y_i - \\beta_0 - \\beta_1 x_i) &amp;= 0 \\end{aligned} \\] Al resolver el sistema de ecuaciones, un reordenamiento algebraico común da como resultado las ecuaciones normales. \\[ \\begin{aligned} n \\beta_0 + \\beta_1 \\sum_{i = 1}^{n} x_i &amp;= \\sum_{i = 1}^{n} y_i\\\\ \\beta_0 \\sum_{i = 1}^{n} x_i + \\beta_1 \\sum_{i = 1}^{n} x_i^2 &amp;= \\sum_{i = 1}^{n} x_i y_i \\end{aligned} \\] Finalmente, terminamos de resolver el sistema de ecuaciones. \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i = 1}^{n} x_i y_i - \\frac{(\\sum_{i = 1}^{n} x_i)(\\sum_{i = 1}^{n} y_i)}{n}}{\\sum_{i = 1}^{n} x_i^2 - \\frac{(\\sum_{i = 1}^{n} x_i)^2}{n}} = \\frac{S_{xy}}{S_{xx}}\\\\ \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{aligned} \\] Aquí, hemos definido una notación para la expresión que hemos obtenido. Tenga en cuenta que tienen formas alternativas con las que es mucho más fácil trabajar. (No lo haremos aquí, pero puede intentar demostrar las equivalencias a continuación por su cuenta, por diversión). Usamos la letra mayúscula \\(S\\) para denotar suma que reemplaza a la letra mayúscula \\(\\Sigma\\) cuando calculamos estos valores basados en datos observados, \\((x_i ,y_i)\\). Los subíndices como \\(xy\\) denotan sobre qué variables se aplica la función \\((z - \\bar{z})\\). \\[ \\begin{aligned} S_{xy} &amp;= \\sum_{i = 1}^{n} x_i y_i - \\frac{(\\sum_{i = 1}^{n} x_i)(\\sum_{i = 1}^{n} y_i)}{n} = \\sum_{i = 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\\\\ S_{xx} &amp;= \\sum_{i = 1}^{n} x_i^2 - \\frac{(\\sum_{i = 1}^{n} x_i)^2}{n} = \\sum_{i = 1}^{n}(x_i - \\bar{x})^2\\\\ S_{yy} &amp;= \\sum_{i = 1}^{n} y_i^2 - \\frac{(\\sum_{i = 1}^{n} y_i)^2}{n} = \\sum_{i = 1}^{n}(y_i - \\bar{y})^2 \\end{aligned} \\] Tenga en cuenta que estas sumas \\(S\\) no deben confundirse con la desviación estándar muestral \\(s\\). Al usar las expresiones alternativas anteriores para \\(S_{xy}\\) y \\(S_{xx}\\), llegamos a una expresión más limpia y útil para \\(\\hat{\\beta}_1\\). \\[ \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i = 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2} \\] Tradicionalmente, calcularíamos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) a mano para el conjunto de datos cars. Sin embargo, debido a que vivimos en el siglo XXI y somos inteligentes (o perezosos o eficientes, según su perspectiva), utilizaremos R para hacer el cálculo numérico por nosotros. Para mantener alguna notación consistente con las matemáticas anteriores, almacenaremos la variable de respuesta como y y la variable predictora como x. x = cars$speed y = cars$dist Luego calculamos las tres sumas de cuadrados definidos anteriormente. Sxy = sum((x - mean(x)) * (y - mean(y))) Sxx = sum((x - mean(x)) ^ 2) Syy = sum((y - mean(y)) ^ 2) c(Sxy, Sxx, Syy) ## [1] 5387.40 1370.00 32538.98 Luego, finalmente calculamos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\). beta_1_hat = Sxy / Sxx beta_0_hat = mean(y) - beta_1_hat * mean(x) c(beta_0_hat, beta_1_hat) ## [1] -17.579095 3.932409 ¿Qué nos dicen estos valores sobre nuestro conjunto de datos? El parámetro de pendiente \\(\\beta_1\\) nos dice que para un aumento en la velocidad de una milla por hora, la distancia media de frenado aumenta en \\(\\beta_1\\). Es importante precisar que estamos hablando de la media. Recuerde que \\(\\beta_0 + \\beta_1 x\\) es la media de \\(Y\\), en este caso la distancia de frenado, para un valor particular de \\(x\\). (la velocidad). Entonces \\(\\beta_1\\) nos dice cómo la media de \\(Y\\) se ve afectada por un cambio en \\(x\\). De manera similar, la estimación \\(\\hat{\\beta}_1 = 3.93\\) nos dice que para un aumento en la velocidad de una milla por hora, la distancia media estimada de frenado aumenta en \\(3.93\\) pies. Debemos asegurarnos de especificar que estamos discutiendo una cantidad estimada. Recuerde que \\(\\hat{y}\\) es la media estimada de \\(Y\\), por lo que \\(\\hat{\\beta}_1\\) nos dice cómo la media estimada de \\(Y\\) se ve afectada al cambiar \\(x\\). El parámetro de intercepción \\(\\beta_0\\) nos dice la distancia media de frenado para un automóvil que viaja a cero millas por hora. (No se mueve). La estimación \\(\\hat{\\beta}_0 = -17.58\\) nos dice que la distancia de frenado media estimada para un automóvil que viaja a cero millas por hora es \\(-17.58\\) pies. Entonces, cuando aplica los frenos a un automóvil que no se está moviendo, ¿se mueve hacia atrás? Esto no parece correcto. (Extrapolación, es la cuestión aquí que veremos más adelante.). 7.2.1 Haciendo predicciones Ahora podemos escribir la línea ajustada o estimada, \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x. \\] En este caso, \\[ \\hat{y} = -17.58 + 3.93 x. \\] Ahora podemos usar esta línea para hacer predicciones. Primero, veamos los posibles valores \\(x\\) en el conjunto de datos cars. Dado que algunos valores \\(x\\) pueden aparecer más de una vez, usamos unique() para devolver cada valor único solo una vez. unique(cars$speed) ## [1] 4 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 Hagamos una predicción de la distancia de frenado de un automóvil que viaja a 8 millas por hora. \\[ \\hat{y} = -17.58 + 3.93 \\times 8 % = 13.88 \\] beta_0_hat + beta_1_hat * 8 ## [1] 13.88018 Esto nos dice que la distancia media estimada de frenado de un automóvil que viaja a 8 millas por hora es \\(13.88\\). Ahora hagamos una predicción de la distancia de frenado de un automóvil que viaja a 21 millas por hora. Esto se considera interpolación ya que 21 no es un valor observado de \\(x\\). (Pero está en el rango de datos). Podemos usar el operador especial %in% para verificar rápidamente en R. 8 %in% unique(cars$speed) ## [1] TRUE 21 %in% unique(cars$speed) ## [1] FALSE min(cars$speed) &lt; 21 &amp; 21 &lt; max(cars$speed) ## [1] TRUE \\[ \\hat{y} = -17.58 + 3.93 \\times 21 % = 65 \\] beta_0_hat + beta_1_hat * 21 ## [1] 65.00149 Por último, podemos hacer una predicción de la distancia de frenado de un automóvil que viaja a 50 millas por hora. Esto se considera extrapolación ya que 50 no es un valor observado de \\(x\\) y está fuera del rango de datos. Deberíamos tener menos confianza en predicciones de este tipo. range(cars$speed) ## [1] 4 25 range(cars$speed)[1] &lt; 50 &amp; 50 &lt; range(cars$speed)[2] ## [1] FALSE \\[ \\hat{y} = -17.58 + 3.93 \\times 50 % = 179.04 \\] beta_0_hat + beta_1_hat * 50 ## [1] 179.0413 Los automóviles viajan a 50 millas por hora con bastante facilidad hoy en día, ¡pero no en la década de 1920! Este también es un problema que vimos al interpretar \\(\\hat{\\beta}_0 = -17.58\\), que es equivalente a hacer una predicción en \\(x = 0\\). No debemos confiar en la relación lineal estimada fuera del rango de datos que hemos observado. 7.2.2 Residuales Si pensamos en nuestro modelo como Respuesta = Predicción + Error, podemos escribirlo como \\[ y = \\hat{y} + e. \\] Luego definimos un residual como el valor observado menos el valor predicho. \\[ e_i = y_i - \\hat{y}_i \\] Calculemos el residuo de la predicción que hicimos para un automóvil que viaja a 8 millas por hora. Primero, necesitamos obtener el valor observado de \\(y\\) para este valor de \\(x\\). which(cars$speed == 8) ## [1] 5 cars[5, ] ## speed dist ## 5 8 16 cars[which(cars$speed == 8), ] ## speed dist ## 5 8 16 Entonces podemos calcular el residual. \\[ e = 16 - 13.88 = 2.12 \\] 16 - (beta_0_hat + beta_1_hat * 8) ## [1] 2.119825 El valor residual positivo indica que la distancia de frenado observada es en realidad 2.12 pies más de lo que se predijo. 7.2.3 Estimación de la varianza Ahora usaremos los residuos de cada uno de los puntos para crear una estimación de la varianza, \\(\\sigma^2\\). Recordemos que, \\[ \\text{E}[Y_i \\mid X_i = x_i] = \\beta_0 + \\beta_1 x_i. \\] Entonces, \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] es una estimación natural de la media de \\(Y_i\\) para un valor dado de \\(x_i\\). Además, recuerde que cuando especificamos el modelo, teníamos tres parámetros desconocidos; \\(\\beta_0\\), \\(\\beta_1\\), y \\(\\sigma^2\\). El método de mínimos cuadrados nos dio estimaciones de \\(\\beta_0\\) y \\(\\beta_1\\), sin embargo, todavía tenemos que ver una estimación de \\(\\sigma^2\\). Ahora definiremos \\(s_e^2\\) que será una estimación de \\(\\sigma^2\\). \\[ \\begin{aligned} s_e^2 &amp;= \\frac{1}{n - 2} \\sum_{i = 1}^{n}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2 \\\\ &amp;= \\frac{1}{n - 2} \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2 \\\\ &amp;= \\frac{1}{n - 2} \\sum_{i = 1}^{n} e_i^2 \\end{aligned} \\] Esto probablemente parece una estimación natural, aparte del uso de \\(n - 2\\), que pospondremos para explicar hasta el próximo capítulo. En realidad, debería verse bastante similar a algo que hemos visto antes. \\[ s^2 = \\frac{1}{n - 1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\] Aquí, \\(s^2\\) es la estimación de \\(\\sigma^2\\) cuando tenemos una sola variable aleatoria \\(X\\). En este caso, \\(\\bar{x}\\) es una estimación de \\(\\mu\\) que se supone que es la misma para cada \\(x\\). Ahora, en el caso de regresión, con \\(s_e^2\\) cada \\(y\\) tiene una media diferente debido a la relación con \\(x\\). Por lo tanto, para cada \\(y_i\\), usamos una estimación diferente de la media, que es \\(\\hat{y}_i\\). y_hat = beta_0_hat + beta_1_hat * x e = y - y_hat n = length(e) s2_e = sum(e^2) / (n - 2) s2_e ## [1] 236.5317 Al igual que con la medida univariada de varianza, este valor de 236.53 no tiene una interpretación práctica en términos de distancia de frenado. Sin embargo, al tomar la raíz cuadrada se calcula la desviación estándar de los residuos, también conocida como error residual estándar. s_e = sqrt(s2_e) s_e ## [1] 15.37959 Esto nos dice que nuestras estimaciones de la distancia media de frenado están típicamente desviadas en 15.38 pies. 7.3 Descomposición de variación Podemos volver a expresar \\(y_i - \\bar{y}\\), que mide la desviación de una observación de la media muestral, de la siguiente manera, \\[ y_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y}). \\] Este es el truco matemático común de sumar cero. En este caso sumamos y restamos \\(\\hat{y}_i\\). Aquí, \\(y_i - \\hat{y}_i\\) mide la desviación de una observación de la línea de regresión ajustada y \\(\\hat{y}_i - \\bar{y}\\) mide la desviación de la línea de regresión ajustada de la media muestral . Si elevamos al cuadrado y luego sumamos ambos lados de la ecuación anterior, podemos obtener lo siguiente, \\[ \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2. \\] Esto debería ser algo alarmante o sorprendente. ¿Cómo es esto cierto? Por ahora dejaremos estas preguntas sin respuesta. (Piense en esto, y tal vez intente demostrarlo). Ahora definiremos tres de las cantidades que se ven en esta ecuación. Suma de cuadrados del total \\[ \\text{SST} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 \\] La cantidad Suma de cuadrados del total, o \\(\\text{SST}\\), representa la variación total de los valores de \\(y\\) observados. Esta debería ser una expresión familiar. Tenga en cuenta que, \\[ s ^ 2 = \\frac{1}{n - 1}\\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\frac{1}{n - 1} \\text{SST}. \\] Suma de cuadrados de la regresión \\[ \\text{SSReg} = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2 \\] La magnitud Suma de cuadrados de la regresión, \\(\\text{SSReg}\\), representa la variación explicada de los valores observados \\(y\\). Suma de cuadrados del error \\[ \\text{SSE} = \\text{RSS} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\] La magnitud Suma de cuadrados del error, \\(\\text{SSE}\\), representa la variación no explicada de los valores observados \\(y\\). A menudo verá \\(\\text{SSE}\\) escrito como \\(\\text{RSS}\\), o Suma de cuadrados residual. SST = sum((y - mean(y)) ^ 2) SSReg = sum((y_hat - mean(y)) ^ 2) SSE = sum((y - y_hat) ^ 2) c(SST = SST, SSReg = SSReg, SSE = SSE) ## SST SSReg SSE ## 32538.98 21185.46 11353.52 Tenga en cuenta que, \\[ s_e^2 = \\frac{\\text{SSE}}{n - 2}. \\] SSE / (n - 2) ## [1] 236.5317 Podemos usar R para verificar que esto coincide con nuestro cálculo anterior de \\(s_e^2\\). s2_e == SSE / (n - 2) ## [1] TRUE Estas tres medidas tampoco tienen una interpretación práctica importante individualmente. Pero juntas, están a punto de revelar una nueva estadística para ayudar a medir la solidez de un modelo SLR. 7.3.1 Coeficiente de determinación El coeficiente de determinación, \\(R^2\\), se define como \\[ \\begin{aligned} R^2 &amp;= \\frac{\\text{SSReg}}{\\text{SST}} = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} \\\\[2.5ex] &amp;= \\frac{\\text{SST} - \\text{SSE}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}} \\\\[2.5ex] &amp;= 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} = 1 - \\frac{\\sum_{i = 1}^{n}e_i^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} \\end{aligned} \\] El coeficiente de determinación se interpreta como la proporción de variación observada en \\(y\\) que puede explicarse mediante el modelo de regresión lineal simple. R2 = SSReg / SST R2 ## [1] 0.6510794 Para el ejemplo de cars, calculamos \\(R^2 = 0.65\\). Luego decimos que \\(65\\%\\) de la variabilidad observada en la distancia de frenado se explica por la relación lineal con la velocidad. Las siguientes gráficas demuestran visualmente las tres sumas de cuadrados para un conjunto de datos simulado que tiene \\(R^2 = 0.92\\) que es un valor algo alto. Observe en el gráfico final que las flechas naranjas representan una proporción mayor de la flecha total. Las siguientes gráficas nuevamente demuestran visualmente las tres sumas de cuadrados, esta vez para un conjunto de datos simulado que tiene \\(R^2 = 0.19\\). Observe en el gráfico final, que ahora las flechas azules representan una proporción mayor de la flecha total. 7.4 La función lm Hasta ahora hemos hecho la regresión derivando las estimaciones de mínimos cuadrados y luego escribiendo comandos simples en R para realizar los cálculos necesarios. Dado que esta es una tarea tan común, esta es una funcionalidad que se construye directamente en R a través del comando lm(). El comando lm() se usa para ajustar modelos lineales que en realidad representan una clase más amplia de modelos que la regresión lineal simple, pero usaremos SLR como nuestra primera demostración de lm(). La función lm() será una de nuestras herramientas más utilizadas, por lo que es posible que desee echar un vistazo a la documentación usando ?lm. Notará que hay mucha información allí, pero comenzaremos con lo más básico. Esta es la documentación a la que querrá volver a menudo. Continuaremos usando los datos cars, y esencialmente usaremos la función lm() para verificar el trabajo que habíamos hecho anteriormente. stop_dist_model = lm(dist ~ speed, data = cars) Esta línea de código se ajusta a nuestro primer modelo lineal. La sintaxis debería resultar familiar. Usamos la sintaxis dist ~ speed para decirle a R que nos gustaría modelar la variable de respuesta dist como una función lineal de la variable predictora speed. En general, debería pensar en la sintaxis como respuesta ~ predictor. El argumento data = cars le dice a R que las variables dist y speed son del conjunto de datos cars. Luego almacenamos este resultado en la variable stop_dist_model. La variable stop_dist_model ahora contiene una gran cantidad de información, y ahora veremos cómo extraer y usar esa información. Lo primero que haremos es simplemente generar lo que esté almacenado inmediatamente en la variable stop_dist_model. stop_dist_model ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Vemos que primero nos dice la fórmula que ingresamos en R, que es lm(formula = dist ~ speed, data = cars). También vemos los coeficientes del modelo. Podemos comprobar que estos son los que habíamos calculado previamente. (Menos algunos redondeos que realiza R al mostrar los resultados. Se almacenan con total precisión). c(beta_0_hat, beta_1_hat) ## [1] -17.579095 3.932409 A continuación, sería bueno agregar la línea ajustada al diagrama de dispersión. Para hacerlo usaremos la función abline(). plot(dist ~ speed, data = cars, xlab = &quot;Velocidad (en millas por hora)&quot;, ylab = &quot;Distancia de frenado (en pies)&quot;, main = &quot;Distancia de frenado vs velocidad&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) abline(stop_dist_model, lwd = 3, col = &quot;darkorange&quot;) La función abline() se usa para agregar líneas de la forma \\(a+bx\\) a un gráfico. (De ahí abline.) Cuando le damos stop_dist_model como argumento, automáticamente extrae las estimaciones del coeficiente de regresión (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)) y los usa como la pendiente e intersección de la línea. También usamos lwd para modificar el ancho de la línea, así como col para modificar el color de la línea. La cosa que devuelve la función lm() es en realidad un objeto de la clase lm que es una lista. Los detalles exactos de esto no son importantes a menos que esté seriamente interesado en el funcionamiento interno de R, pero sepa que podemos determinar los nombres de los elementos de la lista usando el comando names(). names(stop_dist_model) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Luego podemos usar esta información para, por ejemplo, acceder a los residuales usando el operador $. stop_dist_model$residuals ## 1 2 3 4 5 6 7 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 ## 8 9 10 11 12 13 14 ## 4.255007 12.255007 -8.677401 2.322599 -15.609810 -9.609810 -5.609810 ## 15 16 17 18 19 20 21 ## -1.609810 -7.542219 0.457781 0.457781 12.457781 -11.474628 -1.474628 ## 22 23 24 25 26 27 28 ## 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 33 34 35 ## -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 ## 36 37 38 39 40 41 42 ## -21.136672 -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 ## 43 44 45 46 47 48 49 ## 2.930920 -2.933898 -18.866307 -6.798715 15.201285 16.201285 43.201285 ## 50 ## 4.268876 Otra forma de acceder a la información almacenada en stop_dist_model son las funciones coef(), resid() y fit(). Estas devuelven los coeficientes, los residuos y los valores ajustados, respectivamente. coef(stop_dist_model) ## (Intercept) speed ## -17.579095 3.932409 resid(stop_dist_model) ## 1 2 3 4 5 6 7 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 ## 8 9 10 11 12 13 14 ## 4.255007 12.255007 -8.677401 2.322599 -15.609810 -9.609810 -5.609810 ## 15 16 17 18 19 20 21 ## -1.609810 -7.542219 0.457781 0.457781 12.457781 -11.474628 -1.474628 ## 22 23 24 25 26 27 28 ## 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 33 34 35 ## -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 ## 36 37 38 39 40 41 42 ## -21.136672 -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 ## 43 44 45 46 47 48 49 ## 2.930920 -2.933898 -18.866307 -6.798715 15.201285 16.201285 43.201285 ## 50 ## 4.268876 fitted(stop_dist_model) ## 1 2 3 4 5 6 7 8 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 ## 9 10 11 12 13 14 15 16 ## 21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 ## 17 18 19 20 21 22 23 24 ## 33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 ## 25 26 27 28 29 30 31 32 ## 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 ## 33 34 35 36 37 38 39 40 ## 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 ## 49 50 ## 76.798715 80.731124 Una función de R que es útil en muchas situaciones es summary(). Vemos que cuando se llama en nuestro modelo, devuelve una gran cantidad de información. Al final del curso, sabrá para qué se utiliza cada valor aquí. Por ahora, debería notar inmediatamente las estimaciones de los coeficientes y puede reconocer el valor del \\(R^2\\) que vimos antes. summary(stop_dist_model) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 El comando summary() también devuelve una lista, y podemos usar nuevamente names() para saber qué hay sobre los elementos de esta lista. names(summary(stop_dist_model)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; Entonces, por ejemplo, si quisiéramos acceder directamente al valor de \\(R^2\\), en lugar de copiarlo y pegarlo fuera de la declaración impresa de summary(), podríamos hacerlo. summary(stop_dist_model)$r.squared ## [1] 0.6510794 Otro valor al que podemos querer acceder es \\(s_e\\), que R llama sigma. summary(stop_dist_model)$sigma ## [1] 15.37959 Tenga en cuenta que este es el mismo resultado visto anteriormente como s_e. También puede notar que este valor se mostró arriba como resultado del comando summary(), que R etiquetó como Residual Standard Error. \\[ s_e = \\text{RSE} = \\sqrt{\\frac{1}{n - 2}\\sum_{i = 1}^n e_i^2} \\] A menudo es útil hablar de \\(s_e\\) (o RSE) en lugar de \\(s_e^2\\) debido a sus unidades. Las unidades de \\(s_e\\) en el ejemplo de cars son pies, mientras que las unidades de \\(s_e^2\\) son pies al cuadrado. Otra función útil, que usaremos casi tan a menudo como lm() es la función predict(). predict(stop_dist_model, newdata = data.frame(speed = 8)) ## 1 ## 13.88018 El código anterior dice predice la distancia de frenado de un automóvil que viaja a 8 millas por hora usando el stop_dist_model. Es importante destacar que el segundo argumento para predecir() es un marco de datos que creamos en su lugar. Hacemos esto para que podamos especificar que 8 es un valor de speed, para que predict sepa cómo usarlo con el modelo almacenado en stop_dist_model. Vemos que este resultado es el que habíamos calculado a mano anteriormente. También podríamos predecir varios valores a la vez. predict(stop_dist_model, newdata = data.frame(speed = c(8, 21, 50))) ## 1 2 3 ## 13.88018 65.00149 179.04134 \\[ \\begin{aligned} \\hat{y} &amp;= -17.58 + 3.93 \\times 8 = 13.88 \\\\ \\hat{y} &amp;= -17.58 + 3.93 \\times 21 = 65 \\\\ \\hat{y} &amp;= -17.58 + 3.93 \\times 50 = 179.04 \\end{aligned} \\] O podríamos calcular el valor ajustado para cada uno de los puntos de datos originales. Simplemente podemos suministrar el marco de datos original, cars, ya que contiene una variable llamada speed que tiene los valores que nos gustaría predecir. predict(stop_dist_model, newdata = cars) ## 1 2 3 4 5 6 7 8 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 ## 9 10 11 12 13 14 15 16 ## 21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 ## 17 18 19 20 21 22 23 24 ## 33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 ## 25 26 27 28 29 30 31 32 ## 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 ## 33 34 35 36 37 38 39 40 ## 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 ## 49 50 ## 76.798715 80.731124 # predict(stop_dist_model, newdata = data.frame(speed = cars$speed)) En realidad, esto es equivalente a simplemente llamar a predict() en stop_dist_model sin un segundo argumento. predict(stop_dist_model) ## 1 2 3 4 5 6 7 8 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 ## 9 10 11 12 13 14 15 16 ## 21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 ## 17 18 19 20 21 22 23 24 ## 33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 ## 25 26 27 28 29 30 31 32 ## 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 ## 33 34 35 36 37 38 39 40 ## 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 ## 49 50 ## 76.798715 80.731124 Tenga en cuenta que, en este caso, esto es lo mismo que usar fitted(). fitted(stop_dist_model) ## 1 2 3 4 5 6 7 8 ## -1.849460 -1.849460 9.947766 9.947766 13.880175 17.812584 21.744993 21.744993 ## 9 10 11 12 13 14 15 16 ## 21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 ## 17 18 19 20 21 22 23 24 ## 33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 ## 25 26 27 28 29 30 31 32 ## 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 ## 33 34 35 36 37 38 39 40 ## 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 ## 41 42 43 44 45 46 47 48 ## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 ## 49 50 ## 76.798715 80.731124 7.5 Enfoque de estimación de máxima verosimilitud (MLE) Recuerde el modelo, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Entonces podemos encontrar la media y la varianza de cada \\(Y_i\\). \\[ \\text{E}[Y_i \\mid X_i = x_i] = \\beta_0 + \\beta_1 x_i \\] y \\[ \\text{Var}[Y_i \\mid X_i = x_i] = \\sigma^2. \\] Además, el \\(Y_i\\) sigue una distribución normal condicionada al \\(x_i\\). \\[ Y_i \\mid X_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2) \\] Recuerde que la pdf de una variable aleatoria \\(X \\sim N(\\mu, \\sigma^2)\\) viene dado por \\[ f_{X}(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left[-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\\right]}. \\] Entonces podemos escribir la pdf de cada uno de los \\(Y_i\\) como \\[ f_{Y_i}(y_i; x_i, \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left[-\\frac{1}{2}\\left(\\frac{y_i - (\\beta_0 + \\beta_1 x_i)}{\\sigma}\\right)^2\\right]}. \\] Dados \\(n\\) puntos de datos \\((x_i, y_i)\\) podemos escribir la probabilidad, que es una función de los tres parámetros \\(\\beta_0\\), \\(\\beta_1\\), y \\(\\sigma^2\\). Dado que se han observado los datos, usamos \\(y_i\\) minúsculas para indicar que estos valores ya no son aleatorios. \\[ L(\\beta_0, \\beta_1, \\sigma^2) = \\prod_{i = 1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left[-\\frac{1}{2}\\left(\\frac{y_i - \\beta_0 - \\beta_1 x_i}{\\sigma}\\right)^2\\right]} \\] Nuestro objetivo es encontrar valores de \\(\\beta_0\\), \\(\\beta_1\\), y \\(\\sigma^2\\) que maximicen esta función, que es un sencillo problema de cálculo multivariante. Comenzaremos haciendo un poco de reorganización para facilitar nuestra tarea. \\[ L(\\beta_0, \\beta_1, \\sigma^2) = \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\right)^n \\exp{\\left[-\\frac{1}{2 \\sigma^2} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\\right]} \\] Entonces, como suele ser el caso al encontrar MLE, por conveniencia matemática tomaremos el logaritmo natural de la función de verosimilitud, ya que log es una función que aumenta monótonamente. Luego procederemos a maximizar la probabilidad logarítmica, y las estimaciones resultantes serán las mismas que si no hubiéramos tomado el logaritmo. \\[ \\log L(\\beta_0, \\beta_1, \\sigma^2) = -\\frac{n}{2}\\log(2 \\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\] Tenga en cuenta que usamos \\(\\log\\) para referirnos al logaritmo natural. Ahora tomamos una derivada parcial con respecto a cada uno de los parámetros. \\[ \\begin{aligned} \\frac{\\partial \\log L(\\beta_0, \\beta_1, \\sigma^2)}{\\partial \\beta_0} &amp;= \\frac{1}{\\sigma^2} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)\\\\ \\frac{\\partial \\log L(\\beta_0, \\beta_1, \\sigma^2)}{\\partial \\beta_1} &amp;= \\frac{1}{\\sigma^2} \\sum_{i = 1}^{n}(x_i)(y_i - \\beta_0 - \\beta_1 x_i) \\\\ \\frac{\\partial \\log L(\\beta_0, \\beta_1, \\sigma^2)}{\\partial \\sigma^2} &amp;= -\\frac{n}{2 \\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 \\end{aligned} \\] Luego igualamos a cero cada una de las derivadas parciales y resolvemos el sistema de ecuaciones resultante. \\[ \\begin{aligned} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1 x_i) &amp;= 0\\\\ \\sum_{i = 1}^{n}(x_i)(y_i - \\beta_0 - \\beta_1 x_i) &amp;= 0\\\\ -\\frac{n}{2 \\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i = 1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 &amp;= 0 \\end{aligned} \\] Puede notar que las dos primeras ecuaciones también aparecen en el método de mínimos cuadrados. Luego, omitiendo la cuestión de verificar realmente si hemos encontrado un máximo, llegamos a nuestras estimaciones. A estas estimaciones las denominamos estimaciones de máxima verosimilitud. \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i = 1}^{n} x_i y_i - \\frac{(\\sum_{i = 1}^{n} x_i)(\\sum_{i = 1}^{n} y_i)}{n}}{\\sum_{i = 1}^{n} x_i^2 - \\frac{(\\sum_{i = 1}^{n} x_i)^2}{n}} = \\frac{S_{xy}}{S_{xx}}\\\\ \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\\\ \\hat{\\sigma}^2 &amp;= \\frac{1}{n} \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2 \\end{aligned} \\] Tenga en cuenta que \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son las mismas que las estimaciones de mínimos cuadrados. Sin embargo, ahora tenemos una nueva estimación de \\(\\sigma^2\\), que es \\(\\hat{\\sigma}^2\\). Entonces ahora tenemos dos estimaciones diferentes de \\(\\sigma^2\\). \\[ \\begin{aligned} s_e^2 &amp;= \\frac{1}{n - 2} \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n - 2} \\sum_{i = 1}^{n}e_i^2 &amp; \\text{Least Squares}\\\\ \\hat{\\sigma}^2 &amp;= \\frac{1}{n} \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i = 1}^{n}e_i^2 &amp; \\text{MLE} \\end{aligned} \\] En el próximo capítulo, discutiremos en detalle la diferencia entre estas dos estimaciones, lo que implica sesgo. 7.6 Simulando SLR Volvemos de nuevo a más ejemplos de simulación. ¡Este será un tema común! En la práctica, casi nunca tendrá un modelo verdadero y utilizará datos para intentar recuperar información sobre el modelo verdadero desconocido. Con la simulación, decidimos el modelo real y simulamos datos a partir de él. Luego, aplicamos un método a los datos, en este caso mínimos cuadrados. Ahora, dado que conocemos el modelo real, podemos evaluar qué tan bien funcionó. Para este ejemplo, simularemos \\(n=21\\) observaciones del modelo \\[ Y = 5 - 2 x + \\epsilon. \\] Eso es \\(\\beta_0 = 5\\), \\(\\beta_1 = -2\\), y sea \\(\\epsilon \\sim N(\\mu = 0, \\sigma^2 = 9)\\). O, aún más resumido, podríamos escribir \\[ Y \\mid X \\sim N(\\mu = 5 - 2 x, \\sigma^ 2 = 9). \\] Primero establecemos los verdaderos parámetros del modelo a simular. num_obs = 21 beta_0 = 5 beta_1 = -2 sigma = 3 A continuación, obtenemos valores simulados de \\(\\epsilon_i\\) después de establecer una semilla para la reproducibilidad. set.seed(1) epsilon = rnorm(n = num_obs, mean = 0, sd = sigma) Ahora, dado que los valores \\(x_i\\) en SLR se consideran fijos y conocidos, simplemente especificamos los valores 21. Otra práctica común es generarlos a partir de una distribución uniforme y luego usarlos para el resto del análisis. x_vals = seq(from = 0, to = 10, length.out = num_obs) # set.seed(1) # x_vals = runif(num_obs, 0, 10) Luego generamos los valores \\(y\\) de acuerdo con la relación funcional especificada. y_vals = beta_0 + beta_1 * x_vals + epsilon Los datos, \\((x_i, y_i)\\), representan una posible muestra de la distribución verdadera. Ahora, para comprobar qué tan bien funciona el método de mínimos cuadrados, usamos lm() para ajustar el modelo a nuestros datos simulados, luego echemos un vistazo a los coeficientes estimados. sim_fit = lm(y_vals ~ x_vals) coef(sim_fit) ## (Intercept) x_vals ## 4.832639 -1.831401 Y observe esto, ¡no están muy lejos de los verdaderos parámetros que especificamos! plot(y_vals ~ x_vals) abline(sim_fit) Deberíamos decir que estamos siendo un poco perezosos, y no el buen tipo de perezosos que podrían considerarse eficientes. Cada vez que simule datos, debe considerar hacer dos cosas: escribir una función y almacenar los datos en un marco de datos. La función de abajo, sim_slr(), puede usarse para la misma tarea que la anterior, pero es mucho más flexible. Observe que proporcionamos x a la función, en lugar de generar x dentro de la función. En el modelo SLR, los \\(x_i\\) se consideran valores conocidos. Es decir, no son aleatorios, por lo que no asumimos una distribución para \\(x_i\\). Debido a esto, usaremos repetidamente los mismos valores de x en todas las simulaciones. sim_slr = function(x, beta_0 = 10, beta_1 = 5, sigma = 1) { n = length(x) epsilon = rnorm(n, mean = 0, sd = sigma) y = beta_0 + beta_1 * x + epsilon data.frame(predictor = x, response = y) } Aquí, usamos la función para repetir el análisis anterior. set.seed(1) sim_data = sim_slr(x = x_vals, beta_0 = 5, beta_1 = -2, sigma = 3) Esta vez, las observaciones simuladas se almacenan en un marco de datos. head(sim_data) ## predictor response ## 1 0.0 3.1206386 ## 2 0.5 4.5509300 ## 3 1.0 0.4931142 ## 4 1.5 6.7858424 ## 5 2.0 1.9885233 ## 6 2.5 -2.4614052 Ahora, cuando ajustamos el modelo con lm() podemos usar el argumento data, una muy buena práctica. sim_fit = lm(response ~ predictor, data = sim_data) coef(sim_fit) ## (Intercept) predictor ## 4.832639 -1.831401 Y esta vez, haremos que la gráfica se vea mucho mejor. plot(response ~ predictor, data = sim_data, xlab = &quot;Variable predictora simulada&quot;, ylab = &quot;Variable respuesta simulada&quot;, main = &quot;Datos de regresión simulados&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) abline(sim_fit, lwd = 3, lty = 1, col = &quot;darkorange&quot;) abline(beta_0, beta_1, lwd = 3, lty = 2, col = &quot;dodgerblue&quot;) legend(&quot;topright&quot;, c(&quot;Estimate&quot;, &quot;Truth&quot;), lty = c(1, 2), lwd = 2, col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;)) 7.7 Historia Para obtener algunos antecedentes sobre la historia de la regresión lineal, consulte Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics Instructors del Journal of Statistics Education, así como la Wikipedia page on the history of regression analysis y, por último, el artículo de regression to the mean que detalla los orígenes del término regresión. "],["inferencia-para-regresión-lineal-simple.html", "Capítulo 8 Inferencia para regresión lineal simple 8.1 Teorema de Gauss-Markov 8.2 Distribuciones muestrales 8.3 Errores estándar 8.4 Intervalos de confianza para pendiente e Intercepto 8.5 Pruebas de hipótesis 8.6 Ejemplo cars 8.7 Intervalo de confianza para la respuesta Promedio 8.8 Intervalo de predicción para nuevas observaciones 8.9 Bandas de confianza y predicción 8.10 Significancia de la regresión, prueba F", " Capítulo 8 Inferencia para regresión lineal simple Hay tres tipos de mentiras: mentiras, malditas mentiras y estadísticas.  Benjamin Disraeli Después de leer este capítulo, podrá: Comprender las distribuciones de las estimaciones de regresión. Crear estimaciones de intervalo para parámetros de regresión, respuesta media y predicciones. Probar la significancia de la regresión. En el capítulo anterior definimos el modelo de regresión lineal simple, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). luego usamos observaciones \\((x_i, y_i)\\), con \\(i = 1, 2, \\ldots n\\), para encontrar valores de \\(\\beta_0\\) y \\(\\beta_1\\) que minimizan \\[ f(\\beta_0, \\beta_1) = \\sum_{i = 1}^{n}(y_i - (\\beta_0 + \\beta_1 x_i))^2. \\] Llamamos a estos valores \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\), que encontramos \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i = 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2}\\\\ \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x}. \\end{aligned} \\] También estimamos \\(\\sigma^2\\) usando \\(s_e^2\\). En otras palabras, encontramos que \\(s_e\\) es una estimación de \\(\\sigma\\), donde; \\[ s_e = \\text{RSE} = \\sqrt{\\frac{1}{n - 2}\\sum_{i = 1}^n e_i^2} \\] que también llamamos \\(\\text{RSE}\\), para Error estándar residual. Cuando se aplica a los datos cars, obtenemos los siguientes resultados: stop_dist_model = lm(dist ~ speed, data = cars) summary(stop_dist_model) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 El último capítulo, solo discutimos sobre los valores de Estimación, Error estándar residual, y R cuadrado múltiple. En este capítulo, discutiremos toda la información acerca de los Coeficientes así como también de el Estadístico F. plot(dist ~ speed, data = cars, xlab = &quot;Velocidad (en millas por hora)&quot;, ylab = &quot;Distancia de frenado (en pies)&quot;, main = &quot;Distancia de frenado vs velocidad&quot;, pch = 20, cex = 2, col = &quot;grey&quot;) abline(stop_dist_model, lwd = 5, col = &quot;darkorange&quot;) Para comenzar, notaremos que hay otra expresión equivalente para \\(S_{xy}\\) que no vimos en el último capítulo, \\[ S_{xy}= \\sum_{i = 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i = 1}^{n}(x_i - \\bar{x}) y_i. \\] Ésta puede ser una equivalencia sorprendente. (Tal vez intente probarlo). Sin embargo, será útil para ilustrar conceptos en este capítulo. Tenga en cuenta que \\(\\hat{\\beta}_1\\) es una estadística muestral cuando se calcula con los datos observados como está escrito anteriormente, al igual que \\(\\hat{\\beta}_0\\). Sin embargo, en este capítulo a menudo será conveniente utilizar tanto \\(\\hat{\\beta}_1\\) como \\(\\hat{\\beta}_0\\) como variables aleatorias, es decir, aún no hemos observado los valores por cada \\(Y_i\\). Cuando este sea el caso, usaremos una notación ligeramente diferente, sustituyendo en mayúsculas \\(Y_i\\) por minúsculas \\(y_i\\). \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i = 1}^{n}(x_i - \\bar{x}) Y_i}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2} \\\\ \\hat{\\beta}_0 &amp;= \\bar{Y} - \\hat{\\beta}_1 \\bar{x} \\end{aligned} \\] En el último capítulo argumentamos que estas estimaciones de parámetros de modelo desconocidos \\(\\beta_0\\) y \\(\\beta_1\\) eran buenas porque las obtuvimos minimizando los errores. Ahora discutiremos el teorema de Gauss-Markov que lleva esta idea más allá, mostrando que estas estimaciones son en realidad las mejores estimaciones, desde cierto punto de vista. 8.1 Teorema de Gauss-Markov El teorema de Gauss-Markov nos dice que al estimar los parámetros del modelo de regresión lineal simple \\(\\beta_0\\) y \\(\\beta_1\\), los \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) que se obtienen son las mejores estimaciones lineales insesgadas, o BLUE para abreviar. (Las condiciones reales del teorema de Gauss-Markov son más flexibles que las del modelo SLR). Ahora discutiremos lineal, insesgado y mejor en lo que se refiere a estas estimaciones. Lineal Recuerde, en la definición de SLR que los valores \\(x_i\\) se consideran cantidades fijas y conocidas. Entonces, una estimación lineal es aquella que se puede escribir como una combinación lineal de \\(Y_i\\). En el caso de \\(\\hat{\\beta}_1\\) vemos \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i = 1}^{n}(x_i - \\bar{x}) Y_i}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2} = \\sum_{i = 1}^n k_i Y_i = k_1 Y_1 + k_2 Y_2 + \\cdots k_n Y_n \\] donde \\(k_i = \\displaystyle\\frac{(x_i - \\bar{x})}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2}\\). De manera similar, podríamos mostrar que $_0 $ se puede escribir como una combinación lineal de \\(Y_i\\). Luego, tanto \\(\\hat{\\beta}_0\\) como \\(\\hat{\\beta}_1\\) son estimadores lineales. Insesgado Ahora que sabemos que nuestras estimaciones son lineales, ¿qué tan buenas son estas estimaciones? Una medida de la bondad de una estimación es su sesgo. Específicamente, preferimos estimaciones que sean insesgadas, lo que significa que su valor esperado es el parámetro que se estima. En el caso de las estimaciones de regresión, tenemos, \\[ \\begin{aligned} \\text{E}[\\hat{\\beta}_0] &amp;= \\beta_0 \\\\ \\text{E}[\\hat{\\beta}_1] &amp;= \\beta_1. \\end{aligned} \\] Esto nos dice que, cuando se cumplen las condiciones del modelo SLR, en promedio nuestras estimaciones serán correctas. Sin embargo, como vimos en el último capítulo al simular desde el modelo SLR, eso no significa que cada estimación individual sea correcta. Solo que, si repitiéramos el proceso un número infinito de veces, en promedio la estimación sería correcta. Mejor Ahora bien, si nos limitamos a estimaciones tanto lineales como insesgadas, ¿cómo definimos la mejor estimación? La estimación con la varianza mínima. En primer lugar, tenga en cuenta que es muy fácil crear una estimación de \\(\\beta_1\\) que tenga una varianza muy baja, pero que no sea imparcial. Por ejemplo, defina: \\[ \\hat{\\theta}_{BAD} = 5. \\] Entonces, dado que \\(\\hat{\\theta}_{BAD}\\) es un valor constante, \\[ \\text{Var}[\\hat{\\theta}_{BAD}] = 0. \\] Sin embargo, desde, \\[ \\text{E}[\\hat{\\theta}_{BAD}] = 5 \\] decimos que \\(\\hat{\\theta}_{BAD}\\) es un estimador sesgado a menos que \\(\\beta_1 = 5\\), lo cual no sabríamos de antemano. Por esta razón, es una mala estimación (a menos que por casualidad \\(\\beta_1 = 5\\)) aunque tenga la menor variación posible. Esta es parte de la razón por la que nos limitamos a estimaciones insesgadas. ¿De qué sirve una estimación, si estima la cantidad incorrecta? Entonces, ahora, la pregunta natural es, ¿cuáles son las varianzas de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\)? Estas son, \\[ \\begin{aligned} \\text{Var}[\\hat{\\beta}_0] &amp;= \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\\\ \\text{Var}[\\hat{\\beta}_1] &amp;= \\frac{\\sigma^2}{S_{xx}}. \\end{aligned} \\] cuantifican la variabilidad de las estimaciones debido a la aleatoridad durante el muestreo. ¿Son estas las mejores? ¿Son estas variaciones pequeñas tanto como es posible obtenerlas? Solo tendrá que confiar en nuestra palabra de que lo son porque demostrar que esto es cierto está más allá del alcance de este curso. 8.2 Distribuciones muestrales Ahora que hemos redefinido las estimaciones para \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) como variables aleatorias, podemos discutir su distribución muestral, que es la distribución cuando una estadística se considera una variable aleatoria.. Dado que tanto \\(\\hat{\\beta}_0\\) como \\(\\hat{\\beta}_1\\) son una combinación lineal de \\(Y_i\\) y cada \\(Y_i\\) se distribuye normalmente, entonces ambos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) también siguen una distribución normal. Luego, juntando todo lo anterior, llegamos a las distribuciones de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\). Para \\(\\hat{\\beta}_1\\) decimos, \\[ \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i = 1}^{n}(x_i - \\bar{x}) Y_i}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2} \\sim N\\left( \\beta_1, \\ \\frac{\\sigma^2}{\\sum_{i = 1}^{n}(x_i - \\bar{x})^2} \\right). \\] O más resumido, \\[ \\hat{\\beta}_1 \\sim N\\left( \\beta_1, \\frac{\\sigma^2}{S_{xx}} \\right). \\] Y para \\(\\hat{\\beta}_0\\), \\[ \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x} \\sim N\\left( \\beta_0, \\ \\frac{\\sigma^2 \\sum_{i = 1}^{n}x_i^2}{n \\sum_{i = 1}^{n}(x_i - \\bar{x})^2} \\right). \\] O más resumido, \\[ \\hat{\\beta}_0 \\sim N\\left( \\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\right) \\] En este punto nos hemos olvidado de probar algunos de estos resultados. En lugar de trabajar con las tediosas demostraciones de estas distribuciones muestrales, justificaremos estos resultados mediante la simulación. Una nota para los lectores actuales: estas demostraciones y pruebas se pueden agregar a un apéndice en un momento posterior. También puede encontrar estos resultados en casi cualquier libro de texto de regresión lineal estándar. 8.2.1 Simular distribuciones muestrales Para verificar los resultados anteriores, simularemos muestras de tamaño \\(n=100\\) del modelo \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2).\\) En este caso, se sabe que los parámetros son: \\(\\beta_0 = 3\\) \\(\\beta_1 = 6\\) \\(\\sigma^2 = 4\\) Entonces, basándonos en lo anterior, deberíamos encontrar que \\[ \\hat{\\beta}_1 \\sim N\\left( \\beta_1, \\frac{\\sigma^2}{S_{xx}} \\right) \\] y \\[ \\hat{\\beta}_0 \\sim N\\left( \\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\right). \\] Primero, debemos decidir con anticipación cuáles serán nuestros valores de \\(x\\) para esta simulación, ya que los valores de \\(x\\) en SLR también se consideran cantidades conocidas. La elección de valores \\(x\\) es arbitraria. Aquí también establecemos una semilla para la aleatorización y calculamos \\(S_{xx}\\) que necesitaremos en el futuro. set.seed(42) sample_size = 100 # esto es n x = seq(-1, 1, length = sample_size) Sxx = sum((x - mean(x)) ^ 2) También arreglamos los valores de nuestros parámetros. beta_0 = 3 beta_1 = 6 sigma = 2 Con esta información, sabemos que las distribuciones muestrales deben ser: (var_beta_1_hat = sigma ^ 2 / Sxx) ## [1] 0.1176238 (var_beta_0_hat = sigma ^ 2 * (1 / sample_size + mean(x) ^ 2 / Sxx)) ## [1] 0.04 \\[ \\hat{\\beta}_1 \\sim N( 6, 0.1176238) \\] y \\[ \\hat{\\beta}_0 \\sim N( 3, 0.04). \\] Es decir, \\[ \\begin{aligned} \\text{E}[\\hat{\\beta}_1] &amp;= 6 \\\\ \\text{Var}[\\hat{\\beta}_1] &amp;= 0.1176238 \\end{aligned} \\] y \\[ \\begin{aligned} \\text{E}[\\hat{\\beta}_0] &amp;= 3 \\\\ \\text{Var}[\\hat{\\beta}_0] &amp;= 0.04. \\end{aligned} \\] Ahora simulamos datos de este modelo 10,000 veces. Tenga en cuenta que esta puede no ser la mejor forma de realizar la simulación en R. Realizamos la simulación de esta manera en un intento de claridad. Por ejemplo, podríamos haber usado la función sim_slr() del capítulo anterior. Ahora, simplemente almacenamos variables en el entorno global en lugar de crear un marco de datos para cada nuevo conjunto de datos simulado. num_samples = 10000 beta_0_hats = rep(0, num_samples) beta_1_hats = rep(0, num_samples) for (i in 1:num_samples) { eps = rnorm(sample_size, mean = 0, sd = sigma) y = beta_0 + beta_1 * x + eps sim_model = lm(y ~ x) beta_0_hats[i] = coef(sim_model)[1] beta_1_hats[i] = coef(sim_model)[2] } Cada vez que simulamos los datos, obtuvimos valores de los coeficientes estimados. Las variables beta_0_hats y beta_1_hats ahora almacenan 10,000 valores simulados de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) respectivamente. Primero verificamos la distribución de \\(\\hat{\\beta}_1\\). mean(beta_1_hats) # media empírica ## [1] 6.001998 beta_1 # media verdadera ## [1] 6 var(beta_1_hats) # varianza empírica ## [1] 0.11899 var_beta_1_hat # varianza verdadera ## [1] 0.1176238 Vemos que las medias y varianzas empíricas y verdaderas son muy similares. También verificamos que la distribución empírica es normal. Para hacerlo, trazamos un histograma de beta_1_hats y agregamos la curva para la verdadera distribución de $_1 $. Usamos prob = TRUE para poner el histograma en la misma escala que la curva normal. # tenga en cuenta que es necesario utilizar prob = TRUE hist(beta_1_hats, prob = TRUE, breaks = 20, xlab = expression(hat(beta)[1]), main = &quot;&quot;, border = &quot;dodgerblue&quot;) curve(dnorm(x, mean = beta_1, sd = sqrt(var_beta_1_hat)), col = &quot;darkorange&quot;, add = TRUE, lwd = 3) Luego repetimos el proceso para \\(\\hat{\\beta}_0\\). mean(beta_0_hats) # media empírica ## [1] 3.001147 beta_0 # media verdadera ## [1] 3 var(beta_0_hats) # varianza empírica ## [1] 0.04017924 var_beta_0_hat # varianza verdadera ## [1] 0.04 hist(beta_0_hats, prob = TRUE, breaks = 25, xlab = expression(hat(beta)[0]), main = &quot;&quot;, border = &quot;dodgerblue&quot;) curve(dnorm(x, mean = beta_0, sd = sqrt(var_beta_0_hat)), col = &quot;darkorange&quot;, add = TRUE, lwd = 3) En este estudio de simulación, solo hemos simulado un número finito de muestras. Para verificar verdaderamente los resultados de la distribución, necesitaríamos observar un número infinito de muestras. Sin embargo, la siguiente gráfica debería dejar en claro que si continuamos simulando, los resultados empíricos se acercarían cada vez más a lo que deberíamos esperar. par(mar = c(5, 5, 1, 1)) plot(cumsum(beta_1_hats) / (1:length(beta_1_hats)), type = &quot;l&quot;, ylim = c(5.95, 6.05), xlab = &quot;Numero de simulaciones&quot;, ylab = expression(&quot;Media empírica de &quot; ~ hat(beta)[1]), col = &quot;dodgerblue&quot;) abline(h = 6, col = &quot;darkorange&quot;, lwd = 2) par(mar = c(5, 5, 1, 1)) plot(cumsum(beta_0_hats) / (1:length(beta_0_hats)), type = &quot;l&quot;, ylim = c(2.95, 3.05), xlab = &quot;Numero de simulaciones&quot;, ylab = expression(&quot;Media empírica de &quot; ~ hat(beta)[0]), col = &quot;dodgerblue&quot;) abline(h = 3, col = &quot;darkorange&quot;, lwd = 2) 8.3 Errores estándar Así que ahora creemos en los dos resultados de las distribuciones \\[ \\begin{aligned} \\hat{\\beta}_0 &amp;\\sim N\\left( \\beta_0, \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\right) \\\\ \\hat{\\beta}_1 &amp;\\sim N\\left( \\beta_1, \\frac{\\sigma^2}{S_{xx}} \\right). \\end{aligned} \\] Luego, al estandarizar estos resultados, encontramos que \\[ \\frac{\\hat{\\beta}_0 - \\beta_0}{\\text{SD}[\\hat{\\beta}_0]} \\sim N(0, 1) \\] y \\[ \\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SD}[\\hat{\\beta}_1]} \\sim N(0, 1) \\] donde \\[ \\text{SD}[\\hat{\\beta}_0] = \\sigma\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}} \\] y \\[ \\text{SD}[\\hat{\\beta}_1] = \\frac{\\sigma}{\\sqrt{S_{xx}}}. \\] Como no conocemos \\(\\sigma\\) en la práctica, tendremos que estimarlo usando \\(s_e\\), que enlazamos a nuestra expresión existente para las desviaciones estándar de nuestras estimaciones. Estas dos nuevas expresiones se denominan errores estándar que son las desviaciones estándar estimadas de las distribuciones muestrales. \\[ \\text{SE}[\\hat{\\beta}_0] = s_e\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}} \\] \\[ \\text{SE}[\\hat{\\beta}_1] = \\frac{s_e}{\\sqrt{S_{xx}}} \\] Ahora bien, si dividimos por el error estándar, en lugar de la desviación estándar, obtenemos los siguientes resultados que nos permitirán hacer intervalos de confianza y realizar pruebas de hipótesis. \\[ \\frac{\\hat{\\beta}_0 - \\beta_0}{\\text{SE}[\\hat{\\beta}_0]} \\sim t_{n-2} \\] \\[ \\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}[\\hat{\\beta}_1]} \\sim t_{n-2} \\] Para ver esto, primero tenga en cuenta que, \\[ \\frac{\\text{RSS}}{\\sigma^2} = \\frac{(n-2)s_e^2}{\\sigma^2} \\sim \\chi_{n-2}^2. \\] Recuerde también que una variable aleatoria \\(T\\) esta definida como, \\[ T = \\frac{Z}{\\sqrt{\\frac{\\chi_{d}^2}{d}}} \\] sigue una distribución \\(t\\) con \\(d\\) grados de libertad, donde \\(\\chi_{d}^2\\) es una variable aleatoria \\(\\chi^2\\) con \\(d\\) grados de libertad. escribimos, \\[ T \\sim t_d \\] es decir que la variable aleatoria \\(T\\) sigue una distribución \\(t\\) con \\(d\\) grados de libertad. Luego usamos el truco clásico de multiplicar por 1 y algunos reordenamientos para llegar a \\[ \\begin{aligned} \\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}[\\hat{\\beta}_1]} &amp;= \\frac{\\hat{\\beta}_1 - \\beta_1}{s_e / \\sqrt{S_{xx}}} \\\\ &amp;= \\frac{\\hat{\\beta}_1 - \\beta_1}{s_e / \\sqrt{S_{xx}}} \\cdot \\frac{\\sigma / \\sqrt{S_{xx}}}{\\sigma / \\sqrt{S_{xx}}} \\\\ &amp;= \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sigma / \\sqrt{S_{xx}}} \\cdot \\frac{\\sigma / \\sqrt{S_{xx}}}{s_e / \\sqrt{S_{xx}}} \\\\ &amp;= \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sigma / \\sqrt{S_{xx}}} \\bigg/ \\sqrt{\\frac{s_e^2}{\\sigma^2}} \\\\ &amp;= \\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SD}[\\hat{\\beta}_1]} \\bigg/ \\sqrt{\\frac{\\frac{(n - 2)s_e^2}{\\sigma^2}}{n - 2}} \\sim \\frac{Z}{\\sqrt{\\frac{\\chi_{n-2}^2}{n-2}}} \\sim t_{n-2} \\end{aligned} \\] donde \\(Z \\sim N(0,1)\\). Recuerde que una distribución \\(t\\) es similar a una normal estándar, pero con colas más pesadas. A medida que aumentan los grados de libertad, la distribución \\(t\\) se parece cada vez más a una normal estándar. A continuación, trazamos una distribución normal estándar, así como dos ejemplos de una distribución \\(t\\) con diferentes grados de libertad. Observe cómo la distribución \\(t\\) con los mayores grados de libertad es más similar a la curva normal estándar. # definir cuadrícula de valores x x = seq(-4, 4, length = 100) # graficar curva para normal estándar plot(x, dnorm(x), type = &quot;l&quot;, lty = 1, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;Densidad&quot;, main = &quot;Distribucion normal vs t&quot;) # agregar curvas para distribuciones t lines(x, dt(x, df = 1), lty = 3, lwd = 2, col = &quot;darkorange&quot;) lines(x, dt(x, df = 10), lty = 2, lwd = 2, col = &quot;dodgerblue&quot;) # agregar leyenda legend(&quot;topright&quot;, title = &quot;Distribuciones&quot;, legend = c(&quot;t, df = 1&quot;, &quot;t, df = 10&quot;, &quot;Normal estándar&quot;), lwd = 2, lty = c(3, 2, 1), col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;, &quot;black&quot;)) 8.4 Intervalos de confianza para pendiente e Intercepto Recuerde que los intervalos de confianza para los promedios a menudo adoptan la forma: \\[ \\text{EST} \\pm \\text{CRIT} \\cdot \\text{SE} \\] o \\[ \\text{EST} \\pm \\text{MARGIN} \\] donde \\(\\text{EST}\\) es una estimación del parámetro de interés, \\(\\text{SE}\\) es el error estándar de la estimación y \\(\\text{MARGIN} = \\text{CRIT} \\cdot \\text{SE}\\). Luego, para \\(\\beta_0\\) y \\(\\beta_1\\) podemos crear intervalos de confianza usando \\[ \\hat{\\beta}_0 \\pm t_{\\alpha/2, n - 2} \\cdot \\text{SE}[\\hat{\\beta}_0] \\quad \\quad \\quad \\hat{\\beta}_0 \\pm t_{\\alpha/2, n - 2} \\cdot s_e\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}} \\] y \\[ \\hat{\\beta}_1 \\pm t_{\\alpha/2, n - 2} \\cdot \\text{SE}[\\hat{\\beta}_1] \\quad \\quad \\quad \\hat{\\beta}_1 \\pm t_{\\alpha/2, n - 2} \\cdot \\frac{s_e}{\\sqrt{S_{xx}}} \\] donde \\(t_{\\alpha/2, n - 2}\\) es el valor crítico tal que \\(P(t_{n-2} &gt; t_{\\alpha/2, n - 2}) = \\alpha/2\\). 8.5 Pruebas de hipótesis Podemos hablar de esta hipótesis como la hipótesis nula, y debe tenerse en cuenta que la hipótesis nula nunca se prueba ni se establece, pero posiblemente sea refutada, en el curso de la experimentación  Ronald Aylmer Fisher Recuerde que una estadística de prueba (\\(\\text{TS}\\)) para las medias de prueba a menudo toma la forma: \\[ \\text{TS} = \\frac{\\text{EST} - \\text{HYP}}{\\text{SE}} \\] donde \\(\\text{EST}\\) es una estimación del parámetro de interés, \\(\\text{HYP}\\) es un valor hipotético del parámetro y \\(\\text{SE}\\) es el error estándar de la estimación. Entonces, para probar \\[ H_0: \\beta_0 = \\beta_{00} \\quad \\text{vs} \\quad H_1: \\beta_0 \\neq \\beta_{00} \\] usamos el estadístico de prueba \\[ t = \\frac{\\hat{\\beta}_0 - \\beta_{00}}{\\text{SE}[\\hat{\\beta}_0]} = \\frac{\\hat{\\beta}_0-\\beta_{00}}{s_e\\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}}} \\] que, bajo la hipótesis nula, sigue una distribución \\(t\\) con \\(n-2\\) grados de libertad. Usamos $_{00} $ para denotar el valor hipotético de \\(\\beta_0\\). Del mismo modo, para probar \\[ H_0: \\beta_1 = \\beta_{10} \\quad \\text{vs} \\quad H_1: \\beta_1 \\neq \\beta_{10} \\] usamos el estadístico de prueba \\[ t = \\frac{\\hat{\\beta}_1-\\beta_{10}}{\\text{SE}[\\hat{\\beta}_1]} = \\frac{\\hat{\\beta}_1-\\beta_{10}}{s_e / \\sqrt{S_{xx}}} \\] que de nuevo, bajo la hipótesis nula, sigue una distribución \\(t\\) con \\(n-2\\) grados de libertad. Ahora usamos \\(\\beta_{10}\\) para denotar el valor hipotético de \\(\\beta_1\\). 8.6 Ejemplo cars Ahora volvemos al ejemplo de cars del último capítulo para ilustrar estos conceptos. Primero ajustamos el modelo usando lm() y luego usamos summary() para ver los resultados con mayor detalle. stop_dist_model = lm(dist ~ speed, data = cars) summary(stop_dist_model) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 8.6.1 Pruebas en R Ahora discutiremos los resultados llamados Coefficients. Primero recuerde que podemos extraer esta información directamente. names(summary(stop_dist_model)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; summary(stop_dist_model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.579095 6.7584402 -2.601058 1.231882e-02 ## speed 3.932409 0.4155128 9.463990 1.489836e-12 La función names() nos dice qué información está disponible, y luego usamos el operador $ y Coefficients para extraer la información que nos interesa. Dos valores que deberían resultar familiares de inmediato. \\[ \\hat{\\beta}_0 = -17.5790949 \\] y \\[ \\hat{\\beta}_1 = 3.9324088 \\] que son nuestras estimaciones para los parámetros del modelo \\(\\beta_0\\) y \\(\\beta_1\\). Centrémonos ahora en la segunda fila de resultados, que es relevante para \\(\\beta_1\\). summary(stop_dist_model)$coefficients[2,] ## Estimate Std. Error t value Pr(&gt;|t|) ## 3.932409e+00 4.155128e-01 9.463990e+00 1.489836e-12 Nuevamente, el primer valor, Estimate es \\[ \\hat{\\beta}_1 = 3.9324088. \\] El segundo valor, Std. Error, es el error estándar de \\(\\hat{\\beta}_1\\), \\[ \\text{SE}[\\hat{\\beta}_1] = \\frac{s_e}{\\sqrt{S_{xx}}} = 0.4155128. \\] El tercer valor, t value, es el valor de la estadística de prueba para probar \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\), \\[ t = \\frac{\\hat{\\beta}_1-0}{\\text{SE}[\\hat{\\beta}_1]} = \\frac{\\hat{\\beta}_1-0}{s_e / \\sqrt{S_{xx}}} = 9.46399. \\] Por último, Pr(&gt;|t|), nos da el valor p de esa prueba. \\[ \\text{p-value} = 1.4898365\\times 10^{-12} \\] Tenga en cuenta que aquí estamos probando específicamente si \\(\\beta_1 = 0\\). La primera fila de salida informa los mismos valores, pero para \\(\\beta_0\\). summary(stop_dist_model)$coefficients[1,] ## Estimate Std. Error t value Pr(&gt;|t|) ## -17.57909489 6.75844017 -2.60105800 0.01231882 En resumen, el siguiente código almacena la información de summary(stop_dist_model)$coefficients en una nueva variable stop_dist_model_test_info, luego extrae cada elemento en una nueva variable que describe la información que contiene. stop_dist_model_test_info = summary(stop_dist_model)$coefficients beta_0_hat = stop_dist_model_test_info[1, 1] # Estimate beta_0_hat_se = stop_dist_model_test_info[1, 2] # Std. Error beta_0_hat_t = stop_dist_model_test_info[1, 3] # t value beta_0_hat_pval = stop_dist_model_test_info[1, 4] # Pr(&gt;|t|) beta_1_hat = stop_dist_model_test_info[2, 1] # Estimate beta_1_hat_se = stop_dist_model_test_info[2, 2] # Std. Error beta_1_hat_t = stop_dist_model_test_info[2, 3] # t value beta_1_hat_pval = stop_dist_model_test_info[2, 4] # Pr(&gt;|t|) Luego podemos verificar algunas expresiones equivalentes: la estadística de prueba \\(t\\) para \\(\\hat{\\beta}_1\\) y el valor p bilateral asociado con ese estadístico de prueba. (beta_1_hat - 0) / beta_1_hat_se ## [1] 9.46399 beta_1_hat_t ## [1] 9.46399 2 * pt(abs(beta_1_hat_t), df = length(resid(stop_dist_model)) - 2, lower.tail = FALSE) ## [1] 1.489836e-12 beta_1_hat_pval ## [1] 1.489836e-12 8.6.2 Significancia de la regresión, prueba t. Hacemos una pausa para discutir la prueba de significancia de la regresión. Primero, tenga en cuenta que, con base a los resultados de distribución anteriores, podríamos probar \\(\\beta_0\\) y \\(\\beta_1\\) contra cualquier valor en particular, y realizar pruebas de una y dos colas. Sin embargo, una prueba muy específica, \\[ H_0: \\beta_1 = 0 \\quad \\text{vs} \\quad H_1: \\beta_1 \\neq 0 \\] se utiliza con mayor frecuencia. Pensemos en esta prueba en términos del modelo de regresión lineal simple, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i. \\] Si asumimos que la hipótesis nula es cierta, entonces \\(\\beta_1=0\\) y tenemos el modelo, \\[ Y_i = \\beta_0 + \\epsilon_i. \\] En este modelo, la respuesta no depende del predictor. Entonces, podríamos pensar en esta prueba de la siguiente manera, bajo \\(H_0\\) no existe una relación lineal significativa entre \\(x\\) y \\(y\\). bajo \\(H_1\\) existe una relación lineal significativa entre \\(x\\) y \\(y\\). Para el ejemplo de cars, bajo \\(H_0\\) no existe una relación lineal significativa entre la velocidad y la distancia de frenado. bajo \\(H_1\\) Existe una relación lineal significativa entre la velocidad y la distancia de frenado. Nuevamente, esa prueba se ve en la salida de summary(), \\[ \\text{p-value} = 1.4898365\\times 10^{-12}. \\] Con este valor p extremadamente bajo, rechazaríamos la hipótesis nula en cualquier nivel \\(\\alpha\\) razonable, digamos, por ejemplo, \\(\\alpha=0.01\\). Por tanto, decimos que existe una relación lineal significativa entre la velocidad y la distancia de frenado. Observe que enfatizamos lineal. En este gráfico de datos simulados, vemos una relación clara entre \\(x\\) y \\(y\\), sin embargo, no es una relación lineal. Si ajustamos una línea a estos datos, es muy plana. La prueba resultante para \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\) da un valor p grande, en este caso \\(0.7564548\\), por lo que no podríamos rechazar y decir que no existe una relación lineal significativa entre \\(x\\) y \\(y\\). Más adelante veremos cómo ajustar una curva a estos datos usando un modelo lineal, pero por ahora, tenga en cuenta que probar \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\) solo puede detectar relaciones linéales . 8.6.3 Intervalos de confianza en R Usando R podemos obtener muy fácilmente los intervalos de confianza para \\(\\beta_0\\) y \\(\\beta_1\\). confint(stop_dist_model, level = 0.99) ## 0.5 % 99.5 % ## (Intercept) -35.706610 0.5484205 ## speed 2.817919 5.0468988 Esto calcula automáticamente intervalos de confianza del 99% para \\(\\beta_0\\) y \\(\\beta_1\\), la primera fila para \\(\\beta_0\\), la segunda fila para \\(\\beta_1\\). Para el ejemplo de cars, al interpretar estos intervalos, decimos que tenemos un 99% de confianza en que para un aumento de velocidad de 1 milla por hora, el aumento medio de la distancia de frenado está entre 2.8179187 y 5.0468988 pies, que es el intervalo para \\(\\beta_1\\). Tenga en cuenta que este intervalo de confianza del 99% no contiene el valor hipotético de 0. Dado que no contiene 0, es equivalente a rechazar la prueba de \\(H_0:\\beta_1=0\\) vs \\(H_1:\\beta_1\\neq 0\\) con \\(\\alpha=0.01\\), que habíamos visto anteriormente. Debería sospechar algo del intervalo de confianza para \\(\\beta_0\\), ya que cubre valores negativos, que corresponden a distancias de frenado negativas. Técnicamente, la interpretación sería que tenemos un 99% de confianza en que la distancia de frenado promedio de un automóvil que viaja a 0 millas por hora está entre -35.7066103 y 0.5484205 pies, pero realmente no creemos eso, ya que estamos seguros de que no sería negativo. Tenga en cuenta que podemos extraer valores específicos de esta salida de varias formas. Este código no se ejecuta y, en su lugar, debe verificar cómo se relaciona con la salida del código anterior. confint(stop_dist_model, level = 0.99)[1,] confint(stop_dist_model, level = 0.99)[1, 1] confint(stop_dist_model, level = 0.99)[1, 2] confint(stop_dist_model, parm = &quot;(Intercept)&quot;, level = 0.99) confint(stop_dist_model, level = 0.99)[2,] confint(stop_dist_model, level = 0.99)[2, 1] confint(stop_dist_model, level = 0.99)[2, 2] confint(stop_dist_model, parm = &quot;speed&quot;, level = 0.99) También podemos verificar los cálculos que está realizando R para el intervalo \\(\\beta_1\\). beta_1_hat = coef(stop_dist_model)[2] beta_1_hat_se = summary(stop_dist_model)$coefficients[2, 2] crit = qt(0.995, df = length(resid(stop_dist_model)) - 2) c(beta_1_hat - crit * beta_1_hat_se, beta_1_hat + crit * beta_1_hat_se) ## speed speed ## 2.817919 5.046899 8.7 Intervalo de confianza para la respuesta Promedio Además de los intervalos de confianza para \\(\\beta_0\\) y \\(\\beta_1\\), existen otras dos estimaciones comunes de intervalo que se utilizan con la regresión. El primero se llama intervalo de confianza para la respuesta Promedio. A menudo, nos gustaría una estimación de intervalo para la media, \\(E[Y\\mid = x]\\) para un valor particular de \\(x\\). En esta situación usamos \\(\\hat{y}(x)\\) como nuestra estimación de \\(E[Y \\mid X = x]\\). Modificamos nuestra notación ligeramente para dejar en claro que el valor predicho es una función del valor \\(x\\). \\[ \\hat{y}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] Recordar que, \\[ \\text{E}[Y \\mid X = x] = \\beta_0 + \\beta_1 x. \\] Por lo tanto, \\(\\hat{y}(x)\\) es una buena estimación ya que no tiene sesgo: \\[ \\text{E}[\\hat{y}(x)] = \\beta_0 + \\beta_1 x. \\] podríamos obtener, \\[ \\text{Var}[\\hat{y}(x)] = \\sigma^2 \\left(\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right). \\] Como las otras estimaciones que hemos visto, \\(\\hat{y}(x)\\) también sigue una distribución normal. Dado que \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son combinaciones lineales de variables aleatorias normales, \\(\\hat{y}(x)\\) también lo es. \\[ \\hat{y}(x) \\sim N \\left(\\beta_0 + \\beta_1 x, \\sigma^2 \\left(\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right) \\right) \\] Y por último, dado que necesitamos estimar esta varianza, llegamos al error estándar de nuestra estimación, \\[ \\text{SE}[\\hat{y}(x)] = s_e \\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}}. \\] Luego podemos usar esto para encontrar el intervalo de confianza para la respuesta promedio, \\[ \\hat{y}(x) \\pm t_{\\alpha/2, n - 2} \\cdot s_e\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}} \\] Para encontrar intervalos de confianza para la respuesta promedio mediante R, usamos la función predict(). ingresamos a la función nuestro modelo ajustado, así como los nuevos datos, almacenados como un marco de datos. (Esto es importante, para que R conozca el nombre de la variable predictora). Aquí, estamos encontrando el intervalo de confianza para la distancia media de frenado cuando un automóvil viaja a 5 millas por hora y cuando un automóvil viaja a 21 millas por hora. new_speeds = data.frame(speed = c(5, 21)) predict(stop_dist_model, newdata = new_speeds, interval = c(&quot;confidence&quot;), level = 0.99) ## fit lwr upr ## 1 2.082949 -10.89309 15.05898 ## 2 65.001489 56.45836 73.54462 8.8 Intervalo de predicción para nuevas observaciones Algunas veces nos gustaría una estimación de intervalo para una nueva observación, \\(Y\\), para un valor particular de \\(x\\). Esto es muy similar a un intervalo para la respuesta promedio, \\(\\text{E}[Y \\mid X = x]\\), pero diferente en una forma muy importante. Nuestra suposición para una nueva observación sigue siendo \\(\\hat{y}(x)\\). El promedio estimado sigue siendo la mejor predicción que podemos hacer. La diferencia está en la cantidad de variabilidad. Sabemos que las observaciones variarán sobre la verdadera recta de regresión de acuerdo con una distribución \\(N(0,\\sigma^2)\\). Debido a esto, agregamos un factor adicional de \\(\\sigma^2\\) a la variabilidad de nuestra estimación para tener en cuenta en las observaciones sobre la línea de regresión. \\[ \\begin{aligned} \\text{Var}[\\hat{y}(x) + \\epsilon] &amp;= \\text{Var}[\\hat{y}(x)] + \\text{Var}[\\epsilon] \\\\[2ex] &amp;= \\sigma^2 \\left(\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right) + \\sigma^2 \\\\[2ex] &amp;= \\sigma^2 \\left(1 + \\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right) \\end{aligned} \\] \\[ \\hat{y}(x) + \\epsilon \\sim N \\left(\\beta_0 + \\beta_1 x, \\ \\sigma^2 \\left(1 + \\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right) \\right) \\] \\[ \\text{SE}[\\hat{y}(x) + \\epsilon] = s_e \\sqrt{1 + \\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}} \\] Luego podemos encontrar un intervalo de predicción usando, \\[ \\hat{y}(x) \\pm t_{\\alpha/2, n - 2} \\cdot s_e\\sqrt{1 + \\frac{1}{n}+\\frac{(x-\\bar{x})^2}{S_{xx}}}. \\] Para calcular el intervalo para un conjunto de puntos en R, solo hay un pequeño cambio en la sintaxis que se usa para encontrar un intervalo de confianza para la respuesta promedio. predict(stop_dist_model, newdata = new_speeds, interval = c(&quot;prediction&quot;), level = 0.99) ## fit lwr upr ## 1 2.082949 -41.16099 45.32689 ## 2 65.001489 22.87494 107.12803 Observe que estos dos intervalos son más amplios que los intervalos de confianza correspondientes para la respuesta promedio. 8.9 Bandas de confianza y predicción A menudo nos gustaría gaficar intervalos de confianza para la respuesta promedio y los intervalos de predicción para todos los valores posibles de \\(x\\). A estas las llamamos bandas de confianza y predicción. speed_grid = seq(min(cars$speed), max(cars$speed), by = 0.01) dist_ci_band = predict(stop_dist_model, newdata = data.frame(speed = speed_grid), interval = &quot;confidence&quot;, level = 0.99) dist_pi_band = predict(stop_dist_model, newdata = data.frame(speed = speed_grid), interval = &quot;prediction&quot;, level = 0.99) plot(dist ~ speed, data = cars, xlab = &quot;Velocidad (en millas por hora)&quot;, ylab = &quot;Distancia de frenado (en pies)&quot;, main = &quot;Distancia de frenado vs velocidad&quot;, pch = 20, cex = 2, col = &quot;grey&quot;, ylim = c(min(dist_pi_band), max(dist_pi_band))) abline(stop_dist_model, lwd = 5, col = &quot;darkorange&quot;) lines(speed_grid, dist_ci_band[,&quot;lwr&quot;], col = &quot;dodgerblue&quot;, lwd = 3, lty = 2) lines(speed_grid, dist_ci_band[,&quot;upr&quot;], col = &quot;dodgerblue&quot;, lwd = 3, lty = 2) lines(speed_grid, dist_pi_band[,&quot;lwr&quot;], col = &quot;dodgerblue&quot;, lwd = 3, lty = 3) lines(speed_grid, dist_pi_band[,&quot;upr&quot;], col = &quot;dodgerblue&quot;, lwd = 3, lty = 3) points(mean(cars$speed), mean(cars$dist), pch = &quot;+&quot;, cex = 3) Algunas cosas a tener en cuenta: Usamos el argumento ylim para ajustar el eje \\(y\\) del gráfico, ya que las bandas se extienden más allá de los puntos. Agregamos un punto en \\((\\bar{x},\\bar{y})\\). Este es un punto por el que la línea de regresión siempre pasará. (Piense por qué). Este es el punto donde las bandas de confianza y predicción son más estrechas. Mire los errores estándar de ambos para entender por qué. Las bandas de predicción son menos curvas que las bandas de confianza. Este es el resultado del factor adicional de \\(\\sigma^2\\) agregado a la varianza en cualquier valor de \\(x\\). 8.10 Significancia de la regresión, prueba F En el caso de la regresión lineal simple, la prueba \\(t\\) para la significancia de la regresión es equivalente a otra prueba, la prueba \\(F\\) para la significancia de la regresión. Esta equivalencia solo será cierta para la regresión lineal simple, y en el próximo capítulo solo usaremos la prueba \\(F\\) para la significancia de la regresión. Recuerde del último capítulo la descomposición de la varianza que vimos antes de calcular el \\(R^2\\), \\[ \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2, \\] o, en resumen, \\[ \\text{SST} = \\text{SSE} + \\text{SSReg}. \\] Para desarrollar la prueba \\(F\\), organizaremos esta información en una tabla ANOVA, Fuente Suma de cuadrados Grados de libertad Cuadrado medio \\(F\\) Regression \\(\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2\\) \\(1\\) \\(\\text{SSReg} / 1\\) \\(\\text{MSReg} / \\text{MSE}\\) Error \\(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\) \\(n - 2\\) \\(\\text{SSE} / (n - 2)\\) Total \\(\\sum_{i=1}^{n}(y_i - \\bar{y})^2\\) \\(n - 1\\) ANOVA, o Análisis de varianza, será un concepto que utilizaremos a menudo en este curso. Por ahora, nos centraremos en los resultados de la tabla, que es la estadística \\(F\\), \\[ F = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2 / 1}{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 / (n - 2)} \\sim F_{1, n - 2} \\] que sigue una distribución \\(F\\) con grados de libertad \\(1\\) y \\(n - 2\\) bajo la hipótesis nula. Una distribución \\(F\\) es una distribución continua que solo toma valores positivos y tiene dos parámetros, que son los dos grados de libertad. Recuerde, en la significancia de la prueba de regresión, \\(Y\\) no depende de \\(x\\) en la hipótesis nula. \\[ H_0: \\beta_1 = 0 \\quad \\quad Y_i = \\beta_0 + \\epsilon_i \\] Mientras que en la hipótesis alternativa, \\(Y\\) puede depender de \\(x\\). \\[ H_1: \\beta_1 \\neq 0 \\quad \\quad Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] Podemos usar la estadística \\(F\\) para realizar esta prueba. \\[ F = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2 / 1}{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 / (n - 2)} \\] En particular, rechazaremos la hipótesis nula cuando el estadístico \\(F\\) sea grande, es decir, cuando haya una probabilidad baja de que las observaciones pudieran provenir del modelo nulo. Dejaremos que R calcule el valor p por nosotros. Para realizar la prueba \\(F\\) en R, puede observar la última fila de la salida de summary() llamada F-statistic que proporciona el valor de la estadística de prueba, los grados de libertad relevantes, también el valor p de la prueba. summary(stop_dist_model) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Además, puede utilizar la función anova() para mostrar la información en una tabla ANOVA. anova(stop_dist_model) ## Analysis of Variance Table ## ## Response: dist ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## speed 1 21186 21185.5 89.567 1.49e-12 *** ## Residuals 48 11354 236.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Esto también da un valor p para la prueba. Debería notar que el valor p de la prueba \\(t\\) era el mismo. También puede notar que el valor del estadístico de prueba para la prueba \\(t\\), \\(9.46399\\), se puede elevar al cuadrado para obtener el valor de la estadística \\(F\\), \\(89.5671065\\). Tenga en cuenta que hay otra forma equivalente de hacer esto en R, a la que volveremos a menudo para comparar dos modelos. anova(lm(dist ~ 1, data = cars), lm(dist ~ speed, data = cars)) ## Analysis of Variance Table ## ## Model 1: dist ~ 1 ## Model 2: dist ~ speed ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 32539 ## 2 48 11354 1 21186 89.567 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La declaración del modelo lm(dist ~ 1, data = cars) aplica el modelo \\(Y_i = \\beta_0 + \\epsilon_i\\) a los datos cars. Tenga en cuenta que \\(\\hat{y} = \\bar{y}\\) cuando \\(Y_i = \\beta_0 + \\epsilon_i\\). La declaración del modelo lm(dist ~ speed, data = cars) aplica el modelo \\(Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\). Entonces podemos pensar en este uso de anova() como una comparación directa de los dos modelos. (Observe que obtenemos el mismo valor p nuevamente). "],["regresión-lineal-múltiple.html", "Capítulo 9 Regresión lineal múltiple 9.1 Enfoque matricial para la regresión 9.2 Distribución muestral 9.3 Significancia de la regresión 9.4 Modelos anidados 9.5 Simulación", " Capítulo 9 Regresión lineal múltiple La vida es realmente simple, pero insistimos en complicarla.  Confucio Después de leer este capítulo, podrá: Construir e interpretar modelos de regresión lineal con más de un predictor. Comprender cómo se obtienen los modelos de regresión mediante matrices. Crear estimaciones de intervalos y realizar pruebas de hipótesis para múltiples parámetros de regresión. Formular e interpretar estimaciones de intervalos para la respuesta media en diversas condiciones. Comparar modelos anidados usando una prueba F ANOVA. Los dos últimos capítulos vimos cómo ajustar un modelo que asumía una relación lineal entre una variable de respuesta y una única variable predictora. Específicamente, definimos el modelo de regresión lineal simple, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Sin embargo, rara vez se da el caso de que un conjunto de datos tenga una única variable predictora. También es raro el caso de que una variable respuesta solo dependa de una sola variable. Entonces, en este capítulo, ampliaremos nuestro modelo lineal actual para permitir que una respuesta dependa de múltiples predictores. # leer los datos de la web autompg = read.table( &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;, quote = &quot;\\&quot;&quot;, comment.char = &quot;&quot;, stringsAsFactors = FALSE) # dar los encabezados del marco de datos colnames(autompg) = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;, &quot;origin&quot;, &quot;name&quot;) # eliminar los datos que faltan, que se almacenan como &quot;?&quot; autompg = subset(autompg, autompg$hp != &quot;?&quot;) # eliminar el plymouth dependiente, ya que causa algunos problemas autompg = subset(autompg, autompg$name != &quot;plymouth reliant&quot;) # dar los nombres de las filas del conjunto de datos, según el motor, el año y el nombre rownames(autompg) = paste(autompg$cyl, &quot;cylinder&quot;, autompg$year, autompg$name) # eliminar la variable para el nombre, así como el origen autompg = subset(autompg, select = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;)) # cambiar caballos de fuerza de carácter a numérico autompg$hp = as.numeric(autompg$hp) # comprobar la estructura final de los datos str(autompg) ## &#39;data.frame&#39;: 390 obs. of 7 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : int 8 8 8 8 8 8 8 8 8 8 ... ## $ disp: num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year: int 70 70 70 70 70 70 70 70 70 70 ... Una vez más, discutiremos un conjunto de datos con información sobre automóviles. Este conjunto de datos, que se puede encontrar en el UCI Machine Learning Repository contiene una variable de respuesta mpg que almacena la eficiencia de combustible de los automóviles en la ciudad, así como varias variables predictoras para los atributos de los vehículos. Cargamos los datos y realizamos algunos arreglos básicos antes de pasar al análisis. Por ahora nos centraremos en el uso de dos variables, wt yyear, como variables predictoras. Es decir, nos gustaría modelar la eficiencia de combustible (mpg) de un automóvil en función de su peso (wt) y el año del modelo (year). Para ello, definiremos el siguiente modelo lineal, \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i, \\qquad i = 1, 2, \\ldots, n \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). En esta notación definiremos: \\(x_{i1}\\) como el peso (wt) de el \\(i\\)-esimo carro. \\(x_{i2}\\) como el año del modelo (year) de el \\(i\\)-esimo carro. En la siguiente imagen se visualizará lo que nos gustaría lograr. Los puntos de datos $(x_{i1},x_{i2}, y_i) $ ahora existen en un espacio tridimensional, por lo que en lugar de ajustar una línea a los datos, ajustaremos un plano. (Pronto pasaremos a dimensiones más altas, por lo que este será el último ejemplo que sea fácil de visualizar y pensar de esta manera). ¿Cómo encontramos un plano así?, bueno, nos gustaría un plano que esté lo más cerca posible de los puntos de datos. Es decir, nos gustaría que minimizara los errores que está cometiendo. ¿Cómo definiremos estos errores? ¡Distancia al cuadrado, por supuesto! Entonces, nos gustaría minimizar \\[ f(\\beta_0, \\beta_1, \\beta_2) = \\sum_{i = 1}^{n}(y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}))^2 \\] con respecto a \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\beta_2\\). ¿Cómo lo hacemos? Es otro problema sencillo de cálculo multivariado. Todo lo que hemos hecho es agregar una variable extra desde que hicimos esto la última vez. Entonces, nuevamente, derivamos con respecto a cada uno de los \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\ beta_2\\) y los igualamos a cero, luego resolvemos el sistema de ecuaciones resultante. Es decir, \\[ \\begin{aligned} \\frac{\\partial f}{\\partial \\beta_0} &amp;= 0 \\\\ \\frac{\\partial f}{\\partial \\beta_1} &amp;= 0 \\\\ \\frac{\\partial f}{\\partial \\beta_2} &amp;= 0 \\end{aligned} \\] Una vez hecho esto, volveremos a obtener las ecuaciones normales. \\[ \\begin{aligned} n \\beta_0 + \\beta_1 \\sum_{i = 1}^{n} x_{i1} + \\beta_2 \\sum_{i = 1}^{n} x_{i2} &amp;= \\sum_{i = 1}^{n} y_i \\\\ \\beta_0 \\sum_{i = 1}^{n} x_{i1} + \\beta_1 \\sum_{i = 1}^{n} x_{i1}^2 + \\beta_2 \\sum_{i = 1}^{n} x_{i1}x_{i2} &amp;= \\sum_{i = 1}^{n} x_{i1}y_i \\\\ \\beta_0 \\sum_{i = 1}^{n} x_{i2} + \\beta_1 \\sum_{i = 1}^{n} x_{i1}x_{i2} + \\beta_2 \\sum_{i = 1}^{n} x_{i2}^2 &amp;= \\sum_{i = 1}^{n} x_{i2}y_i \\end{aligned} \\] Ahora tenemos tres ecuaciones y tres variables, que podríamos resolver, o simplemente dejar que R resuelva por nosotros. mpg_model = lm(mpg ~ wt + year, data = autompg) coef(mpg_model) ## (Intercept) wt year ## -14.637641945 -0.006634876 0.761401955 \\[ \\hat{y} = -14.6376419 + -0.0066349 x_1 + 0.761402 x_2 \\] Aquí hemos vuelto a ajustar nuestro modelo usando lm(), sin embargo, hemos introducido un nuevo elemento sintáctico. La fórmula mpg ~ wt + year ahora dice: modela la variable de respuesta mpg como una función lineal de wt y year. Es decir, estimará una intersección, así como los coeficientes de la pendiente para wt yyear. Luego los extraemos como lo hicimos antes de usar coef(). En la configuración de regresión lineal múltiple, algunas de las interpretaciones de los coeficientes cambian ligeramente. Aquí, \\(\\hat{\\beta}_0 = -14.6376419\\) es nuestra estimación de \\(\\beta_0\\), la media de millas por galón de un automóvil que pesa 0 libras y fue construido en 1900. Vea que nuestra estimación aquí es negativa, lo cual es una imposibilidad física. Sin embargo, esto no es inesperado, ya que no deberíamos esperar que nuestro modelo sea preciso para autos de 1900 que pesan 0 libras. (¡Porque nunca existieron!) Este no es un gran cambio con respecto a SLR. Es decir, \\(\\beta_0\\) sigue siendo simplemente la media cuando todos los predictores son 0. La interpretación de los coeficientes frente a nuestros predictores es ligeramente diferente al anterior. Por ejemplo, \\(\\hat{\\beta}_1 = -0.0066349\\) es nuestra estimación de \\(\\beta_1\\), el cambio promedio en millas por galón para un aumento de peso (\\(x_{1}\\) ) de una libra para un automóvil de un determinado año de modelo, es decir, por un valor fijo de \\(x_{2}\\). Tenga en cuenta que este coeficiente es en realidad el mismo para cualquier valor dado de \\(x_{2}\\). Más adelante, veremos modelos que permiten un cambio diferente en la respuesta media para diferentes valores de \\(x_{2}\\). También tenga en cuenta que esta estimación es negativa, lo que esperaríamos ya que, en general, la eficiencia del combustible disminuye para los vehículos más grandes. Recuerde que en la configuración de regresión lineal múltiple, esta interpretación depende de un valor fijo para \\(x_{2}\\), es decir, para un automóvil de un determinado año de modelo. Es posible que la relación indirecta entre la eficiencia del combustible y el peso no se mantenga cuando se incluye un factor adicional, digamos un año, y por lo tanto podríamos tener el signo de nuestro coeficiente invertido. Por último, \\(\\hat{\\beta}_2 = 0.761402\\) es nuestra estimación de \\(\\beta_2\\), el cambio promedio en millas por galón para un aumento de un año en el año modelo (\\(x_{2}\\)) por un automóvil de cierto peso, es decir, por un valor fijo de \\(x_{1}\\). No es de extrañar que la estimación sea positiva. Esperamos que a medida que pase el tiempo y los años, la tecnología mejore para que un automóvil de un peso específico obtenga un mejor kilometraje ahora en comparación con sus predecesores. Y, sin embargo, el coeficiente podría haber sido negativo porque también incluimos el peso como variable, y no estrictamente como un valor fijo. 9.1 Enfoque matricial para la regresión En nuestro ejemplo anterior, usamos dos variables predictoras, pero solo se necesitará un poco más de trabajo para permitir un número arbitrario de variables predictoras y derivar sus estimaciones de coeficientes. Podemos considerar el modelo, \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p-1} x_{i(p-1)} + \\epsilon_i, \\qquad i = 1, 2, \\ldots, n \\] donde \\(\\epsilon_i\\sim N(0,\\sigma^2)\\). En este modelo, hay \\(p - 1\\) variables predictoras, \\(x_1,x_2,\\cdots, x_{p-1}\\). Hay un total de \\(p\\) \\(\\beta\\) -parámetros y un solo parámetro \\(\\sigma^2\\) para la varianza de los errores. (Cabe señalar que casi con la misma frecuencia, los autores usarán \\(p\\) como el número de predictores, lo que hace que el número total de \\(\\beta\\) parámetros \\(p+1\\). Esto siempre es algo que debe tener en cuenta al leer sobre regresión múltiple. No existe un estándar que se utilice con más frecuencia). Si tuviéramos que apilar las ecuaciones lineales \\(n\\) que representan cada \\(Y_i\\) en un vector de columna, obtenemos lo siguiente. \\[ \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots\\\\ Y_n \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1(p-1)} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2(p-1)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{n(p-1)} \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_{p-1} \\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots\\\\ \\epsilon_n \\\\ \\end{bmatrix} \\] \\[ Y = X \\beta + \\epsilon \\] \\[ Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots\\\\ Y_n \\end{bmatrix}, \\quad X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1(p-1)} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2(p-1)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{n(p-1)} \\\\ \\end{bmatrix}, \\quad \\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_{p-1} \\\\ \\end{bmatrix}, \\quad \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots\\\\ \\epsilon_n \\end{bmatrix} \\] Así que ahora con los datos \\[ y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots\\\\ y_n \\end{bmatrix} \\] Al igual que antes, podemos estimar \\(\\beta\\) minimizando, \\[ f(\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_{p-1}) = \\sum_{i = 1}^{n}(y_i - (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p-1} x_{i(p-1)}))^2, \\] que requeriría tomar \\(p\\) derivadas, que dan como resultado las siguientes ecuaciones normales. \\[ \\begin{bmatrix} n &amp; \\sum_{i = 1}^{n} x_{i1} &amp; \\sum_{i = 1}^{n} x_{i2} &amp; \\cdots &amp; \\sum_{i = 1}^{n} x_{i(p-1)} \\\\ \\sum_{i = 1}^{n} x_{i1} &amp; \\sum_{i = 1}^{n} x_{i1}^2 &amp; \\sum_{i = 1}^{n} x_{i1}x_{i2} &amp; \\cdots &amp; \\sum_{i = 1}^{n} x_{i1}x_{i(p-1)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\sum_{i = 1}^{n} x_{i(p-1)} &amp; \\sum_{i = 1}^{n} x_{i(p-1)}x_{i1} &amp; \\sum_{i = 1}^{n} x_{i(p-1)}x_{i2} &amp; \\cdots &amp; \\sum_{i = 1}^{n} x_{i(p-1)}^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p-1} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\sum_{i = 1}^{n} y_i \\\\ \\sum_{i = 1}^{n} x_{i1}y_i \\\\ \\vdots \\\\ \\sum_{i = 1}^{n} x_{i(p-1)}y_i \\\\ \\end{bmatrix} \\] Las ecuaciones normales se pueden escribir mucho más resumidas en notación matricial, \\[ X^\\top X \\beta = X^\\top y. \\] Entonces podemos resolver esta expresión multiplicando ambos lados por el inverso de \\(X^\\top X\\), que existe, siempre que las columnas de \\(X\\) sean linealmente independientes. Entonces, como siempre, denotamos nuestra solución con un sombrero. \\[ \\hat{\\beta} = \\left( X^\\top X \\right)^{-1}X^\\top y \\] Para verificar que esto es lo que R ha hecho por nosotros en el caso de dos predictores, creamos una matriz \\(X\\). Tenga en cuenta que la primera columna son todos 1 y las columnas restantes contienen los datos. n = nrow(autompg) p = length(coef(mpg_model)) X = cbind(rep(1, n), autompg$wt, autompg$year) y = autompg$mpg (beta_hat = solve(t(X) %*% X) %*% t(X) %*% y) ## [,1] ## [1,] -14.637641945 ## [2,] -0.006634876 ## [3,] 0.761401955 coef(mpg_model) ## (Intercept) wt year ## -14.637641945 -0.006634876 0.761401955 \\[ \\hat{\\beta} = \\begin{bmatrix} -14.6376419 \\\\ -0.0066349 \\\\ 0.761402 \\\\ \\end{bmatrix} \\] En nuestra nueva notación, los valores ajustados se pueden escribir \\[ \\hat{y} = X \\hat{\\beta}. \\] \\[ \\hat{y} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots\\\\ \\hat{y}_n \\end{bmatrix} \\] Luego, podemos crear un vector para los valores residuales, \\[ e = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots\\\\ e_n \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots\\\\ y_n \\end{bmatrix} - \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots\\\\ \\hat{y}_n \\end{bmatrix}. \\] Y, por último, podemos actualizar nuestra estimación de $ ^2$. \\[ s_e^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - p} = \\frac{e^\\top e}{n-p} \\] Recuerde, nos gusta esta estimación porque no tiene sesgo, es decir, \\[ \\text{E}[s_e^2] = \\sigma^2 \\] Tenga en cuenta que el cambio desde la estimación SLR hasta ahora está en el denominador. Específicamente ahora dividimos por \\(n-p\\) en lugar de \\(n-2\\). O en realidad, deberíamos tener en cuenta que en el caso de SLR, hay dos parámetros \\(\\beta\\) y, por tanto, \\(p = 2\\). También tenga en cuenta que si nos ajustamos al modelo \\(Y_i = \\beta + \\epsilon_i\\) que \\(\\hat{y} = \\bar{y}\\), \\(p = 1\\) y \\(s_e^2\\) se convertiría \\[ s_e^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n - 1} \\] que es probablemente la primera desviación estándar muestral que vio en una clase de estadística matemática. La misma razón para \\(n - 1\\) en este caso, que estimamos un parámetro, por lo que perdemos un grado de libertad. Ahora, en general, estamos estimando los parámetros \\(p\\), los parámetros \\(\\beta\\), por lo que perdemos \\(p\\) grados de libertad. Además, recuerde que la mayoría de las veces nos interesará \\(s_e\\), el error estándar residual como lo llama R, \\[ s_e = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - p}}. \\] En R, podríamos acceder directamente a \\(s_e\\) para un modelo ajustado, como hemos visto antes. summary(mpg_model)$sigma ## [1] 3.431367 Y ahora podemos verificar que nuestra matemática anterior está calculando las mismas cantidades. y_hat = X %*% solve(t(X) %*% X) %*% t(X) %*% y e = y - y_hat sqrt(t(e) %*% e / (n - p)) ## [,1] ## [1,] 3.431367 sqrt(sum((y - y_hat) ^ 2) / (n - p)) ## [1] 3.431367 9.2 Distribución muestral Como podemos ver a continuación, los resultados de llamar a summary () son similares a SLR, pero hay algunas diferencias, la más obvia es una nueva fila para la variable predictora agregada. summary(mpg_model) ## ## Call: ## lm(formula = mpg ~ wt + year, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.852 -2.292 -0.100 2.039 14.325 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.464e+01 4.023e+00 -3.638 0.000312 *** ## wt -6.635e-03 2.149e-04 -30.881 &lt; 2e-16 *** ## year 7.614e-01 4.973e-02 15.312 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.431 on 387 degrees of freedom ## Multiple R-squared: 0.8082, Adjusted R-squared: 0.8072 ## F-statistic: 815.6 on 2 and 387 DF, p-value: &lt; 2.2e-16 Para comprender estas diferencias en detalle, primero necesitaremos obtener la distribución muestral de \\(\\hat{\\beta}\\). La derivación de la distribución muestral de \\(\\hat{\\beta}\\) implica la distribución normal multivariante. Nuestro objetivo ahora es obtener la distribución del vector \\(\\hat{\\beta}\\), \\[ \\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\\\ \\vdots \\\\ \\hat{\\beta}_{p-1} \\end{bmatrix} \\] Recuerde de la última vez que cuando hablamos de distribuciones muestrales, ahora consideramos que \\(\\hat{\\beta}\\) es un vector aleatorio, por lo que usamos \\(Y\\) en lugar del vector de datos \\(y\\). \\[ \\hat{\\beta} = \\left( X^\\top X \\right)^{-1}X^\\top Y \\] Entonces es una consecuencia de la distribución normal multivariante que, \\[ \\hat{\\beta} \\sim N\\left(\\beta, \\sigma^2 \\left(X^\\top X\\right)^{-1} \\right). \\] Entonces tenemos \\[ \\text{E}[\\hat{\\beta}] = \\beta \\] y para cualquier \\(\\hat{\\beta}_j\\) tenemos \\[ \\text{E}[\\hat{\\beta}_j] = \\beta_j. \\] Tambien tenemos \\[ \\text{Var}[\\hat{\\beta}] = \\sigma^2 \\left( X^\\top X \\right)^{-1} \\] y para cualquier \\(\\hat{\\beta}_j\\) tenemos \\[ \\text{Var}[\\hat{\\beta}_j] = \\sigma^2 C_{jj} \\] donde \\[ C = \\left(X^\\top X\\right)^{-1} \\] y los elementos de \\(C\\) se denotan \\[ C = \\begin{bmatrix} C_{00} &amp; C_{01} &amp; C_{02} &amp; \\cdots &amp; C_{0(p-1)} \\\\ C_{10} &amp; C_{11} &amp; C_{12} &amp; \\cdots &amp; C_{1(p-1)} \\\\ C_{20} &amp; C_{21} &amp; C_{22} &amp; \\cdots &amp; C_{2(p-1)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ C_{(p-1)0} &amp; C_{(p-1)1} &amp; C_{(p-1)2} &amp; \\cdots &amp; C_{(p-1)(p-1)} \\\\ \\end{bmatrix}. \\] Esencialmente, los elementos de la diagonal corresponden al vector \\(\\beta\\). Entonces, el error estándar para el vector \\(\\hat{\\beta}\\) viene dado por \\[ \\text{SE}[\\hat{\\beta}] = s_e \\sqrt{\\left( X^\\top X \\right)^{-1}} \\] y en particular \\(\\hat{\\beta}_j\\) \\[ \\text{SE}[\\hat{\\beta}_j] = s_e \\sqrt{C_{jj}}. \\] Por último, cada uno de los \\(\\hat{\\beta}_j\\) siguen una distribución normal, \\[ \\hat{\\beta}_j \\sim N\\left(\\beta_j, \\sigma^2 C_{jj} \\right). \\] por lo tanto \\[ \\frac{\\hat{\\beta}_j - \\beta_j}{s_e \\sqrt{C_{jj}}} \\sim t_{n-p}. \\] Ahora que tenemos los resultados de distribución necesarios, podemos pasar a realizar pruebas y hacer estimaciones de intervalo. 9.2.1 Pruebas de un solo parámetro La primera prueba que veremos es una prueba para un solo \\(\\beta_j\\). \\[ H_0: \\beta_j = 0 \\quad \\text{vs} \\quad H_1: \\beta_j \\neq 0 \\] Nuevamente, el estadístico de prueba toma la forma \\[ \\text{TS} = \\frac{\\text{EST} - \\text{HYP}}{\\text{SE}}. \\] En particular, \\[ t = \\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}[\\hat{\\beta}_j]} = \\frac{\\hat{\\beta}_j-0}{s_e\\sqrt{C_{jj}}}, \\] que, bajo la hipótesis nula, sigue una distribución \\(t\\) con \\(n-p\\) grados de libertad. Recuerde nuestro modelo para mpg, \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i, \\qquad i = 1, 2, \\ldots, n \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(x_{i1}\\) como el peso (wt) de el \\(i\\)-esimo carro. \\(x_{i2}\\) como el año del modelo (year) de el \\(i\\)-esimo carro. Entonces la prueba \\[ H_0: \\beta_1 = 0 \\quad \\text{vs} \\quad H_1: \\beta_1 \\neq 0 \\] se puede encontrar en la salida de summary(), en particular: summary(mpg_model)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.637641945 4.0233913563 -3.638135 3.118311e-04 ## wt -0.006634876 0.0002148504 -30.881372 1.850466e-106 ## year 0.761401955 0.0497265950 15.311765 1.036597e-41 La estimación (Estimate), el error estándar (Std.Error), el estadístico de prueba (valor t) y el valor p (Pr(&gt;|t|)) para esta prueba se muestran en la segundo fila, etiquetada como wt. Recuerde que el valor p dado aquí es específicamente para una prueba de dos colas, donde el valor hipotético es 0. También tenga en cuenta en este caso, al plantear la hipótesis nula de que \\(\\beta_1=0\\) y la alternativa esencialmente especifican dos modelos diferentes: \\(H_0\\): \\(Y = \\beta_0 + \\beta_2 x_{2} + \\epsilon\\) \\(H_1\\): \\(Y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\epsilon\\) Esto es importante. No estamos simplemente probando si existe o no una relación entre el peso y la eficiencia del combustible. Estamos probando si existe una relación entre el peso y la eficiencia del combustible, dado que en el modelo hay un término para el año. (Tenga en cuenta que hemos eliminado algunas indexaciones aquí para facilitar la lectura). 9.2.2 Intervalos de confianza Dado que \\(\\hat{\\beta}_j\\) es nuestra estimación de \\(\\beta_j\\) y tenemos \\[ \\text{E}[\\hat{\\beta}_j] = \\beta_j \\] así como el error estándar, \\[ \\text{SE}[\\hat{\\beta}_j] = s_e\\sqrt{C_{jj}} \\] y la distribución muestral de \\(\\hat{\\beta}_j\\) es Normal, entonces podemos construir fácilmente intervalos de confianza para cada uno de los \\(\\hat{\\beta}_j\\). \\[ \\hat{\\beta}_j \\pm t_{\\alpha/2, n - p} \\cdot s_e\\sqrt{C_{jj}} \\] Podemos encontrarlos en R usando el mismo método que antes. Ahora simplemente habrá filas adicionales para los \\(\\beta\\) adicionales. confint(mpg_model, level = 0.99) ## 0.5 % 99.5 % ## (Intercept) -25.052563681 -4.222720208 ## wt -0.007191036 -0.006078716 ## year 0.632680051 0.890123859 9.2.3 Intervalos de confianza para la respuesta media Como vimos en SLR, podemos crear intervalos de confianza para la respuesta media, es decir, una estimación de intervalo para \\(\\text{E}[Y \\mid X = x]\\). En SLR, la media de \\(Y\\) solo dependía de un valor único \\(x\\). Ahora, en regresión múltiple, \\(\\text{E}[Y \\mid X = x]\\) depende del valor de cada uno de los predictores, por lo que definimos el vector \\(x_0\\) como, \\[ x_{0} = \\begin{bmatrix} 1 \\\\ x_{01} \\\\ x_{02} \\\\ \\vdots \\\\ x_{0(p-1)} \\\\ \\end{bmatrix}. \\] Entonces nuestra estimación de \\(\\text{E}[Y \\mid X = x_0]\\) para un conjunto de valores \\(x_0\\) viene dada por \\[ \\begin{aligned} \\hat{y}(x_0) &amp;= x_{0}^\\top\\hat{\\beta} \\\\ &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{01} + \\hat{\\beta}_2 x_{02} + \\cdots + \\hat{\\beta}_{p-1} x_{0(p-1)}. \\end{aligned} \\] Al igual que con SLR, esta es una estimación no sesgada. \\[ \\begin{aligned} \\text{E}[\\hat{y}(x_0)] &amp;= x_{0}^\\top\\beta \\\\ &amp;= \\beta_0 + \\beta_1 x_{01} + \\beta_2 x_{02} + \\cdots + \\beta_{p-1} x_{0(p-1)} \\end{aligned} \\] Para hacer una estimación de intervalo, también necesitaremos su error estándar. \\[ \\text{SE}[\\hat{y}(x_0)] = s_e \\sqrt{x_{0}^\\top\\left(X^\\top X\\right)^{-1}x_{0}} \\] Poniéndolo todo junto, obtenemos un intervalo de confianza para la respuesta media. \\[ \\hat{y}(x_0) \\pm t_{\\alpha/2, n - p} \\cdot s_e \\sqrt{x_{0}^\\top\\left(X^\\top X\\right)^{-1}x_{0}} \\] Las matemáticas han cambiado un poco, pero el proceso en R sigue siendo casi idéntico. Aquí, creamos un marco de datos para dos autos adicionales. Un automóvil que pesa 3500 libras producido en 1976, así como un segundo automóvil que pesa 5000 libras que se fabricó en 1981. new_cars = data.frame(wt = c(3500, 5000), year = c(76, 81)) new_cars ## wt year ## 1 3500 76 ## 2 5000 81 Entonces podemos usar la función predict() con interval = \"confidence\" para obtener intervalos de la eficiencia media del combustible para ambos autos nuevos. Nuevamente, es importante hacer que los datos pasados a newdata sea un marco de datos, de modo que R sepa qué valores son para qué variables. predict(mpg_model, newdata = new_cars, interval = &quot;confidence&quot;, level = 0.99) ## fit lwr upr ## 1 20.00684 19.4712 20.54248 ## 2 13.86154 12.3341 15.38898 R luego muestra la estimación \\(\\hat{y}(x_0)\\) (fit) para cada uno, así como los límites inferior (lwr) y superior (upr) para el intervalo en un nivel deseado (99%). Una advertencia aquí: una de estas estimaciones es buena, mientras que otra es sospechosa. new_cars$wt ## [1] 3500 5000 range(autompg$wt) ## [1] 1613 5140 Tenga en cuenta que ambos pesos de los automóviles nuevos están dentro del rango de valores observados. new_cars$year ## [1] 76 81 range(autompg$year) ## [1] 70 82 Como son los años de cada uno de los coches nuevos. plot(year ~ wt, data = autompg, pch = 20, col = &quot;dodgerblue&quot;, cex = 1.5) points(new_cars, col = &quot;darkorange&quot;, cex = 3, pch = &quot;X&quot;) Sin embargo, ahora tenemos que considerar el peso y el año juntos. Y según la gráfica anterior, uno de los autos nuevos está dentro de la mancha de valores observados, mientras que el otro, el automóvil de 1981 que pesa 5000 libras, está notablemente fuera de los valores observados. Esta es una extrapolación oculta que debe tener en cuenta cuando utilice la regresión múltiple. Cambiando de velocidad al nuevo par de datos que se puede estimar razonablemente, hacemos una verificación rápida de algunas de las matemáticas en R. x0 = c(1, 3500, 76) x0 %*% beta_hat ## [,1] ## [1,] 20.00684 \\[ x_{0} = \\begin{bmatrix} 1 \\\\ 3500 \\\\ 76 \\\\ \\end{bmatrix} \\] \\[ \\hat{\\beta} = \\begin{bmatrix} -14.6376419 \\\\ -0.0066349 \\\\ 0.761402 \\\\ \\end{bmatrix} \\] \\[ \\hat{y}(x_0) = x_{0}^\\top\\hat{\\beta} = \\begin{bmatrix} 1 &amp; 3500 &amp; 76 \\\\ \\end{bmatrix} \\begin{bmatrix} -14.6376419 \\\\ -0.0066349 \\\\ 0.761402 \\\\ \\end{bmatrix}= 20.0068411 \\] También tenga en cuenta que, usando un valor particular para \\(x_0\\), básicamente podemos extraer ciertos valores de \\(\\hat{\\beta}_j\\). beta_hat ## [,1] ## [1,] -14.637641945 ## [2,] -0.006634876 ## [3,] 0.761401955 x0 = c(0, 0, 1) x0 %*% beta_hat ## [,1] ## [1,] 0.761402 Teniendo esto en cuenta, los intervalos de confianza para el individuo \\(\\hat{\\beta}_j\\) son en realidad un caso especial de un intervalo de confianza para la respuesta media. 9.2.4 Intervalos de predicción Al igual que con SLR, la creación de intervalos de predicción implica un ligero cambio en el error estándar para tener en cuenta el hecho de que ahora estamos considerando una observación, en lugar de una media. Aquí usamos \\(\\hat{y}(x_0)\\) para estimar \\(Y_0\\), una nueva observación de \\(Y\\) en el vector predictor \\(x_0\\). \\[ \\begin{aligned} \\hat{y}(x_0) &amp;= x_{0}^\\top\\hat{\\beta} \\\\ &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{01} + \\hat{\\beta}_2 x_{02} + \\cdots + \\hat{\\beta}_{p-1} x_{0(p-1)} \\end{aligned} \\] \\[ \\begin{aligned} \\text{E}[\\hat{y}(x_0)] &amp;= x_{0}^\\top\\beta \\\\ &amp;= \\beta_0 + \\beta_1 x_{01} + \\beta_2 x_{02} + \\cdots + \\beta_{p-1} x_{0(p-1)} \\end{aligned} \\] Como hicimos con SLR, debemos tener en cuenta la variabilidad adicional de una observación sobre su media. \\[ \\text{SE}[\\hat{y}(x_0) + \\epsilon] = s_e \\sqrt{1 + x_{0}^\\top\\left(X^\\top X\\right)^{-1}x_{0}} \\] Luego llegamos a nuestro intervalo de predicción actualizado para MLR. \\[ \\hat{y}(x_0) \\pm t_{\\alpha/2, n - p} \\cdot s_e \\sqrt{1 + x_{0}^\\top\\left(X^\\top X\\right)^{-1}x_{0}} \\] new_cars ## wt year ## 1 3500 76 ## 2 5000 81 predict(mpg_model, newdata = new_cars, interval = &quot;prediction&quot;, level = 0.99) ## fit lwr upr ## 1 20.00684 11.108294 28.90539 ## 2 13.86154 4.848751 22.87432 9.3 Significancia de la regresión La descomposición de la variación que habíamos visto en SLR todavía se mantiene para MLR. \\[ \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2 \\] Es decir, \\[ \\text{SST} = \\text{SSE} + \\text{SSReg}. \\] Esto significa que, todavía podemos calcular \\(R^2\\) de la misma manera que antes, lo que R continúa haciendo automáticamente. summary(mpg_model)$r.squared ## [1] 0.8082355 La interpretación cambia ligeramente en comparación con SLR. En este caso de MLR, decimos que \\(80.82\\%\\) para la variación observada en millas por galón se explica por la relación lineal con las dos variables predictoras, peso y año. En regresión múltiple, la significancia de la prueba de regresión es \\[ H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p - 1} = 0. \\] Aquí, vemos que la hipótesis nula establece todos los \\(\\beta_j\\) iguales a 0, excepto la intersección, \\(\\beta_0\\). Entonces podríamos decir que el modelo nulo, o modelo bajo la hipótesis nula es \\[ Y_i = \\beta_0 + \\epsilon_i. \\] Este es un modelo donde la regresión es insignificante. Ninguno de los predictores tiene una relación lineal significativa con la respuesta. Notacionalmente, denotaremos los valores ajustados de este modelo como \\(\\hat{y}_{0i}\\), que en este caso resulta ser: \\[ \\hat{y}_{0i} = \\bar{y}. \\] La hipótesis alternativa aquí es que al menos uno de los \\(\\beta_j\\) de la hipótesis nula no es 0. \\[ H_1: \\text{Al menos uno de los } \\beta_j \\neq 0, j = 1, 2, \\cdots, (p-1) \\] Entonces podríamos decir que el modelo completo, o modelo bajo la hipótesis alternativa es \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(p-1)} x_{i(p-1)} + \\epsilon_i \\] Este es un modelo donde la regresión es significativa. Al menos uno de los predictores tiene una relación lineal significativa con la respuesta. Existe una relación lineal entre \\(y\\) y los predictores, \\(x_1, x_2,\\ldots,x_{p-1}\\). Denotaremos los valores ajustados de este modelo como \\(\\hat{y}_{1i}\\). Para desarrollar la prueba \\(F\\) para la significancia de la regresión, organizaremos la descomposición de la varianza en una tabla ANOVA. Fuente Suma de cuadrados Grados de libertad Cuadrado medio \\(F\\) Regression \\(\\sum_{i=1}^{n}(\\hat{y}_{1i} - \\bar{y})^2\\) \\(p - 1\\) \\(\\text{SSReg} / (p - 1)\\) \\(\\text{MSReg} / \\text{MSE}\\) Error \\(\\sum_{i=1}^{n}(y_i - \\hat{y}_{1i})^2\\) \\(n - p\\) \\(\\text{SSE} / (n - p)\\) Total \\(\\sum_{i=1}^{n}(y_i - \\bar{y})^2\\) \\(n - 1\\) En resumen, la estadística \\(F\\) es \\[ F = \\frac{\\sum_{i=1}^{n}(\\hat{y}_{1i} - \\bar{y})^2 / (p - 1)}{\\sum_{i=1}^{n}(y_i - \\hat{y}_{1i})^2 / (n - p)}, \\] y el valor p se calcula como \\[ P(F_{p-1, n-p} &gt; F) \\] ya que rechazamos para valores grandes de \\(F\\). Un gran valor de la estadística corresponde a una gran parte de la varianza explicada por la regresión. Aquí \\(F_{p-1,n-p}\\) representa una variable aleatoria que sigue una distribución \\(F\\) con \\(p-1\\) y \\(n-p\\) grados de libertad. Para realizar esta prueba en R, primero especificamos explícitamente los dos modelos y guardamos los resultados en diferentes variables. Luego usamos anova() para comparar los dos modelos, dando a `anova() el modelo nulo primero y el modelo alternativo (completo) en segundo lugar. (Especificar el modelo completo primero dará como resultado el mismo valor p, pero algunos valores intermedios sin sentido). En este caso, \\(H_0\\): \\(Y_i = \\beta_0 + \\epsilon_i\\) \\(H_1\\): \\(Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\\) Es decir, en el modelo nulo, no usamos ninguno de los predictores, mientras que en el modelo completo (alternativo), al menos uno de los predictores es útil. null_mpg_model = lm(mpg ~ 1, data = autompg) full_mpg_model = lm(mpg ~ wt + year, data = autompg) anova(null_mpg_model, full_mpg_model) ## Analysis of Variance Table ## ## Model 1: mpg ~ 1 ## Model 2: mpg ~ wt + year ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 389 23761.7 ## 2 387 4556.6 2 19205 815.55 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Primero, observe que R no muestra los resultados de la misma manera que la tabla anterior. Más importante que el diseño de la tabla es su contenido. Vemos que el valor de la estadística \\(F\\) es 815.55, y el valor p es extremadamente bajo, por lo que rechazamos la hipótesis nula en cualquier \\(\\alpha\\) y decimos que la regresión es significativa. Al menos uno de wt o year tiene una relación lineal útil con mpg. summary(mpg_model) ## ## Call: ## lm(formula = mpg ~ wt + year, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.852 -2.292 -0.100 2.039 14.325 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.464e+01 4.023e+00 -3.638 0.000312 *** ## wt -6.635e-03 2.149e-04 -30.881 &lt; 2e-16 *** ## year 7.614e-01 4.973e-02 15.312 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.431 on 387 degrees of freedom ## Multiple R-squared: 0.8082, Adjusted R-squared: 0.8072 ## F-statistic: 815.6 on 2 and 387 DF, p-value: &lt; 2.2e-16 Observe que el valor reportado en la fila para el estadístico F es de hecho el estadístico de prueba \\(F\\) para la significancia de la prueba de regresión y, además, informa los dos grados de libertad relevantes. Además, tenga en cuenta que ninguna de las pruebas-\\(t\\) individuales son equivalentes a la prueba \\(F\\) como lo eran en SLR. Esta equivalencia solo es válida para SLR porque la prueba individual para \\(\\beta_1\\) es la misma que la prueba para todos los parámetros que no son de intercepción, ya que solo hay uno. También podemos verificar las sumas de cuadrados y grados de libertad directamente en R. # SSReg sum((fitted(full_mpg_model) - fitted(null_mpg_model)) ^ 2) ## [1] 19205.03 # SSE sum(resid(full_mpg_model) ^ 2) ## [1] 4556.646 # SST sum(resid(null_mpg_model) ^ 2) ## [1] 23761.67 # Degrees of Freedom: Regression length(coef(full_mpg_model)) - length(coef(null_mpg_model)) ## [1] 2 # Degrees of Freedom: Error length(resid(full_mpg_model)) - length(coef(full_mpg_model)) ## [1] 387 # Degrees of Freedom: Total length(resid(null_mpg_model)) - length(coef(null_mpg_model)) ## [1] 389 9.4 Modelos anidados La importancia de la prueba de regresión es en realidad un caso especial de prueba de lo que llamaremos modelos anidados. De manera más general, podemos comparar dos modelos, donde un modelo está anidado dentro del otro, lo que significa que un modelo contiene un subconjunto de predictores solo del modelo más grande. Considere el siguiente modelo completo, \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(p-1)} x_{i(p-1)} + \\epsilon_i \\] Este modelo tiene \\(p - 1\\) predictores, para un total de \\(p\\) \\(\\beta\\)-parámetros. Denotaremos los valores ajustados de este modelo como \\(\\hat{y}_{1i}\\). Sea el modelo nulo \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(q-1)} x_{i(q-1)} + \\epsilon_i \\] donde \\(q&lt;p\\). Este modelo tiene \\(q-1\\) predictores , para un total de \\(q\\) \\(\\beta\\)-parámetros. Denotaremos los valores ajustados de este modelo como \\(\\hat{y}_{0i}\\). La diferencia entre estos dos modelos se puede codificar mediante la hipótesis nula de una prueba. \\[ H_0: \\beta_q = \\beta_{q+1} = \\cdots = \\beta_{p - 1} = 0. \\] Específicamente, los parámetros \\(\\beta\\) del modelo completo que no están en el modelo nulo son cero. El modelo resultante, que está anidado, es el modelo nulo. Luego podemos realizar esta prueba usando una prueba \\(F\\), que es el resultado de la siguiente tabla ANOVA. Fuente Suma de cuadrados Grados de libertad Cuadrado medio \\(F\\) Diff \\(\\sum_{i=1}^{n}(\\hat{y}_{1i} - \\hat{y}_{0i})^2\\) \\(p - q\\) \\(\\text{SSD} / (p - q)\\) \\(\\text{MSD} / \\text{MSE}\\) Full \\(\\sum_{i=1}^{n}(y_i - \\hat{y}_{1i})^2\\) \\(n - p\\) \\(\\text{SSE} / (n - p)\\) Null \\(\\sum_{i=1}^{n}(y_i - \\hat{y}_{0i})^2\\) \\(n - q\\) \\[ F = \\frac{\\sum_{i=1}^{n}(\\hat{y}_{1i} - \\hat{y}_{0i})^2 / (p - q)}{\\sum_{i=1}^{n}(y_i - \\hat{y}_{1i})^2 / (n - p)}. \\] Observe que la fila de Diff compara la suma de las diferencias al cuadrado de los valores ajustados. Los grados de libertad son entonces la diferencia del número de parámetros \\(\\beta\\) estimados entre los dos modelos. Por ejemplo, el conjunto de datos autompg tiene una serie de variables adicionales que todavía tenemos que usar. names(autompg) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;wt&quot; &quot;acc&quot; &quot;year&quot; Continuaremos usando mpg como respuesta, pero ahora consideraremos dos modelos diferentes. Full: mpg ~ wt + year + cyl + disp + hp + acc Null: mpg ~ wt + year Tenga en cuenta que estos son modelos anidados, ya que el modelo nulo contiene un subconjunto de los predictores del modelo completo y no hay predictores adicionales. Ambos modelos tienen un intercepto \\(\\beta_0\\) así como un coeficiente delante de cada uno de los predictores. Entonces podríamos escribir la hipótesis nula para comparar estos dos modelos como, \\[ H_0: \\beta_{\\texttt{cyl}} = \\beta_{\\texttt{disp}} = \\beta_{\\texttt{hp}} = \\beta_{\\texttt{acc}} = 0 \\] La alternativa es simplemente que al menos uno de los \\(\\beta_ {j}\\) de la hipotesis nula no sea 0. Para realizar esta prueba en R primero definimos ambos modelos, luego usamos el comando anova(). null_mpg_model = lm(mpg ~ wt + year, data = autompg) #full_mpg_model = lm(mpg ~ wt + year + cyl + disp + hp + acc, data = autompg) full_mpg_model = lm(mpg ~ ., data = autompg) anova(null_mpg_model, full_mpg_model) ## Analysis of Variance Table ## ## Model 1: mpg ~ wt + year ## Model 2: mpg ~ cyl + disp + hp + wt + acc + year ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 387 4556.6 ## 2 383 4530.5 4 26.18 0.5533 0.6967 Aquí hemos utilizado la fórmula mpg ~ . para definir el modelo completo. Esto es lo mismo que la línea comentada. Específicamente, este es un atajo común en R que dice modelar mpg como respuesta con cada una de las variables restantes en el marco de datos como predictores. Aquí vemos que el valor de la estadística \\(F\\) es 0.553, y el valor p es muy grande, por lo que no rechazamos la hipótesis nula en cualquier \\(\\alpha\\) razonable y podemos decir que ninguno de cyl, disp, hp y acc son significativos con wt y year ya en el modelo. Nuevamente, verificamos las sumas de cuadrados y grados de libertad directamente en R. # SSDiff sum((fitted(full_mpg_model) - fitted(null_mpg_model)) ^ 2) ## [1] 26.17981 # SSE (For Full) sum(resid(full_mpg_model) ^ 2) ## [1] 4530.466 # SST (For Null) sum(resid(null_mpg_model) ^ 2) ## [1] 4556.646 # Degrees of Freedom: Diff length(coef(full_mpg_model)) - length(coef(null_mpg_model)) ## [1] 4 # Degrees of Freedom: Full length(resid(full_mpg_model)) - length(coef(full_mpg_model)) ## [1] 383 # Degrees of Freedom: Null length(resid(null_mpg_model)) - length(coef(null_mpg_model)) ## [1] 387 9.5 Simulación Dado que ignoramos la derivación de ciertos resultados, usaremos nuevamente la simulación para convencernos de algunos de los resultados anteriores. En particular, simularemos muestras de tamaño n = 100 del modelo \\[ Y_i = 5 + -2 x_{i1} + 6 x_{i2} + \\epsilon_i, \\qquad i = 1, 2, \\ldots, n \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2 = 16)\\). Aquí tenemos dos predictores, entonces \\(p = 3\\). set.seed(1337) n = 100 # tamaño de la muestra p = 3 beta_0 = 5 beta_1 = -2 beta_2 = 6 sigma = 4 Como es la norma con la regresión, los valores de \\(x\\) se consideran cantidades fijas y conocidas, por lo que las simularemos primero y permanecerán iguales durante el resto del estudio de simulación. También tenga en cuenta que creamos un x0 que contiene solo 1, que necesitamos para crear nuestra matriz X. Si observa la formulación matricial de regresión, este vector unitario de todos los 1 es un predictor que coloca el intercepto en el modelo. También calculamos la matriz C para su uso posterior. x0 = rep(1, n) x1 = sample(seq(1, 10, length = n)) x2 = sample(seq(1, 10, length = n)) X = cbind(x0, x1, x2) C = solve(t(X) %*% X) Luego simulamos la respuesta de acuerdo con el modelo anterior. Por último, colocamos los dos predictores y la respuesta en un marco de datos. Tenga en cuenta que no colocamos x0 en el marco de datos. Este es el resultado de que R agregue una intersección por defecto. eps = rnorm(n, mean = 0, sd = sigma) y = beta_0 + beta_1 * x1 + beta_2 * x2 + eps sim_data = data.frame(x1, x2, y) Trazar estos datos y ajustar la regresión produce el siguiente gráfico. Luego calculamos \\[ \\hat{\\beta} = \\left( X^\\top X \\right)^{-1}X^\\top y. \\] (beta_hat = C %*% t(X) %*% y) ## [,1] ## x0 7.290735 ## x1 -2.282176 ## x2 5.843424 Observe que estos valores son los mismos que los coeficientes encontrados usando lm() en R. coef(lm(y ~ x1 + x2, data = sim_data)) ## (Intercept) x1 x2 ## 7.290735 -2.282176 5.843424 Además, estos valores están cerca de lo que esperaríamos. c(beta_0, beta_1, beta_2) ## [1] 5 -2 6 Luego calculamos los valores ajustados para calcular \\(s_e\\), vemos que es el mismo sigma que devuelve summary(). y_hat = X %*% beta_hat (s_e = sqrt(sum((y - y_hat) ^ 2) / (n - p))) ## [1] 4.294307 summary(lm(y ~ x1 + x2, data = sim_data))$sigma ## [1] 4.294307 Hasta aquí todo está bien. Ahora finalmente simularemos repetidamente a partir de este modelo para obtener una distribución empírica de \\(\\hat{\\beta}_2\\). Esperamos que \\(\\hat{\\beta}_2\\) siga una distribución normal, \\[ \\hat{\\beta}_2 \\sim N\\left(\\beta_2, \\sigma^2 C_{22} \\right). \\] En este caso, \\[ \\hat{\\beta}_2 \\sim N\\left(\\mu = 6, \\sigma^2 = 16 \\times 0.0014534 = 0.0232549 \\right). \\] \\[ \\hat{\\beta}_2 \\sim N\\left(\\mu = 6, \\sigma^2 = 0.0232549 \\right). \\] Tenga en cuenta que \\(C_{22}\\) corresponde al elemento en la tercera fila y tercera columna ya que \\(\\beta_2\\) es el tercer parámetro en el modelo y porque R está indexado comenzando en 1. Sin embargo, indexamos la matriz \\(C\\) comenzando en 0 para hacer coincidir los elementos diagonales con el \\(\\beta_j\\) correspondiente. C[3, 3] ## [1] 0.00145343 C[2 + 1, 2 + 1] ## [1] 0.00145343 sigma ^ 2 * C[2 + 1, 2 + 1] ## [1] 0.02325487 Ahora realizamos la simulación un gran número de veces. Cada vez, actualizamos la variable y en el marco de datos, dejando las variables x iguales. Luego ajustamos un modelo y almacenamos \\(\\hat{\\beta}_2\\). num_sims = 10000 beta_hat_2 = rep(0, num_sims) for(i in 1:num_sims) { eps = rnorm(n, mean = 0 , sd = sigma) sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + eps fit = lm(y ~ x1 + x2, data = sim_data) beta_hat_2[i] = coef(fit)[3] } Luego vemos que la media de los valores simulados está cerca del valor real de \\(\\beta_2\\). mean(beta_hat_2) ## [1] 5.999723 beta_2 ## [1] 6 También vemos que la varianza de los valores simulados está cerca de la verdadera varianza de \\(\\hat{\\beta}_2\\). \\[ \\text{Var}[\\hat{\\beta}_2] = \\sigma^2 \\cdot C_{22} = 16 \\times 0.0014534 = 0.0232549 \\] var(beta_hat_2) ## [1] 0.02343408 sigma ^ 2 * C[2 + 1, 2 + 1] ## [1] 0.02325487 Las desviaciones estándar encontradas a partir de los datos simulados y la población de origen también son muy cercanas. sd(beta_hat_2) ## [1] 0.1530819 sqrt(sigma ^ 2 * C[2 + 1, 2 + 1]) ## [1] 0.1524955 Por último, graficamos un histograma de los valores simulados y superponemos la distribución verdadera. hist(beta_hat_2, prob = TRUE, breaks = 20, xlab = expression(hat(beta)[2]), main = &quot;&quot;, border = &quot;dodgerblue&quot;) curve(dnorm(x, mean = beta_2, sd = sqrt(sigma ^ 2 * C[2 + 1, 2 + 1])), col = &quot;darkorange&quot;, add = TRUE, lwd = 3) ¡Esto luce bien! El histograma basado en simulación parece ser Normal con una media de 6 y una extensión de aproximadamente 0,15 a medida que mide desde el centro hasta el punto de inflexión. Eso coincide muy bien con la distribución muestral de \\(\\hat{\\beta}_2 \\sim N\\left(\\mu = 6, \\sigma^2 = 0.0232549 \\right)\\). Una última comprobación, verificamos la regla de \\(68 - 95 - 99.7\\). sd_bh2 = sqrt(sigma ^ 2 * C[2 + 1, 2 + 1]) # We expect these to be: 0.68, 0.95, 0.997 mean(beta_2 - 1 * sd_bh2 &lt; beta_hat_2 &amp; beta_hat_2 &lt; beta_2 + 1 * sd_bh2) ## [1] 0.6807 mean(beta_2 - 2 * sd_bh2 &lt; beta_hat_2 &amp; beta_hat_2 &lt; beta_2 + 2 * sd_bh2) ## [1] 0.9529 mean(beta_2 - 3 * sd_bh2 &lt; beta_hat_2 &amp; beta_hat_2 &lt; beta_2 + 3 * sd_bh2) ## [1] 0.9967 "],["construcción-del-modelo.html", "Capítulo 10 Construcción del modelo 10.1 Familia, forma, y ajuste. 10.2 Explicación versus predicción 10.3 Resumen", " Capítulo 10 Construcción del modelo Los estadísticos, como los artistas, tienen la mala costumbre de enamorarse de sus modelos.  George Box Demos un paso atrás y consideremos el proceso de encontrar un modelo de datos en un nivel superior. Estamos intentando encontrar un modelo para una variable de respuesta \\(y\\) basado en varios predictores \\(x_1, x_2, x_3, \\ldots, x_{p-1}\\). Esencialmente, estamos tratando de descubrir la relación funcional entre \\(y\\) y los predictores. En el capítulo anterior, estábamos ajustando modelos para la eficiencia de combustible de un automóvil (mpg) en función de sus atributos (wt, year, cyl, disp, hp, acc). También consideramos que \\(y\\) es una función con algo de ruido. Rara vez, si es que alguna vez, esperamos que haya una relación funcional exacta entre los predictores y la respuesta. \\[ y = f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon \\] Podemos pensar en esto como \\[ \\text{respuesta} = \\text{señal} + \\text{ruido}. \\] Podríamos considerar todo tipo de funciones complicadas para \\(f\\). Es probable que encuentre varias formas de hacer esto en futuros cursos de aprendizaje automático. Hasta ahora, en este curso nos hemos centrado en la regresión lineal (múltiple). Es decir \\[ \\begin{aligned} y &amp;= f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon \\\\ &amp;= \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_{p-1} x_{p-1} + \\epsilon \\end{aligned} \\] En el panorama general de los posibles modelos que podríamos ajustar a estos datos, este es un modelo bastante restrictivo. ¿Qué entendemos por modelo restrictivo? 10.1 Familia, forma, y ajuste. Al modelar datos, hay una serie de opciones que deben tomarse. ¿Qué familia de modelos se considerará? ¿Qué forma del modelo se utilizará? ¿Cómo se ajustará el modelo? discutamos cada una de estas opciones. 10.1.1 Ajuste. Considere uno de los modelos más simples que podríamos ajustar a los datos, la regresión lineal simple. \\[ y = f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon = \\beta_0 + \\beta_1 x_{1} + \\epsilon \\] Entonces, aquí, a pesar de tener múltiples predictores, elegimos usar solo uno. ¿Cómo ajusta este modelo? Usaremos casi exclusivamente el método de mínimos cuadrados, pero recuerde, habíamos visto métodos alternativos para ajustar este modelo. \\[ \\underset{\\beta_0, \\beta_1}{\\mathrm{argmin}} \\max|y_i - (\\beta_0 + \\beta_1 x_i)| \\] \\[ \\underset{\\beta_0, \\beta_1}{\\mathrm{argmin}} \\sum_{i = 1}^{n}|y_i - (\\beta_0 + \\beta_1 x_i)| \\] \\[ \\underset{\\beta_0, \\beta_1}{\\mathrm{argmin}} \\sum_{i = 1}^{n}(y_i - (\\beta_0 + \\beta_1 x_i))^2 \\] Cualquiera de estos métodos (siempre usaremos el último, mínimos cuadrados) obtendrá estimaciones de los parámetros desconocidos \\(\\beta_0\\) y \\(\\beta_1\\). Dado que esas son las únicas incógnitas del modelo especificado, hemos ajustado el modelo. El modelo ajustado es entonces \\[ \\hat{y} = \\hat{f}(x_1, x_2, x_3, \\ldots, x_{p-1}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1} \\] Tenga en cuenta que ahora hemos eliminado el término para el ruido. No hacemos ningún esfuerzo por modelar el ruido, solo la señal. 10.1.2 Forma ¿Cuáles son las diferentes formas que puede adoptar un modelo? Actualmente, para los modelos lineales que hemos considerado, el único método para alterar la forma del modelo es controlar los predictores utilizados. Por ejemplo, una forma del modelo de regresión lineal múltiple es la regresión lineal simple. \\[ y = f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon = \\beta_0 + \\beta_1 x_{1} + \\epsilon \\] También podríamos considerar un modelo SLR con un predictor diferente, alterando así la forma del modelo. \\[ y = f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon = \\beta_0 + \\beta_2 x_{2} + \\epsilon \\] A menudo, usaremos múltiples predictores en nuestro modelo. Frecuentemente, probaremos al menos un modelo con todos los predictores posibles. \\[ \\begin{aligned} y &amp;= f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon \\\\ &amp;= \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_{p-1} x_{p-1} + \\epsilon \\end{aligned} \\] También podríamos utilizar algunos, pero no todos, los predictores. \\[ \\begin{aligned} y &amp;= f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon \\\\ &amp;= \\beta_0 + \\beta_1 x_{1} + \\beta_3 x_{3} + \\beta_5 x_{5} + \\epsilon \\end{aligned} \\] Estas formas son restrictivas en dos sentidos. Primero, solo permiten relaciones lineales entre la respuesta y los predictores. Esto parece una restricción obvia de los modelos lineales, pero de hecho, pronto veremos cómo usar modelos lineales para relaciones no lineales. (Implicará la transformación de variables). En segundo lugar, la forma en que una variable afecta la respuesta es la misma para cualquier valor de los otros predictores. Pronto veremos cómo crear modelos donde el efecto de \\(x_{1}\\) puede ser diferente para diferentes valores de \\(x_{2}\\). Discutiremos el concepto de interacción. 10.1.3 Familia Una familia de modelos es una agrupación más amplia de muchas formas posibles de un modelo. Por ejemplo, arriba vimos varias formas de modelos de la familia de modelos lineales. Solo nos ocuparemos de los modelos lineales, que modelan una respuesta como una combinación lineal de predictores. Ciertamente existen otras familias de modelos. Por ejemplo, hay varias familias de regresión no paramétrica. El suavizado es una amplia familia de modelos. Como son los árboles de regresión. En la regresión lineal, especificamos modelos con parámetros \\(\\beta_j\\) y ajustamos el modelo encontrando los mejores valores de estos parámetros. Este es un enfoque paramétrico. Un enfoque no paramétrico omite el paso de especificar un modelo con parámetros y, a menudo, se describe más como un algoritmo. Los modelos no paramétricos se utilizan a menudo en el aprendizaje automático. Aquí, SLR (paramétrico) se usa a la izquierda, mientras que suavizado (no paramétrico) se usa a la derecha. SLR encuentra la mejor pendiente e intersección. El suavizado produce el valor \\(y\\) ajustado a un valor \\(x\\) particular al considerar los valores \\(y\\) de los datos en una vecindad del valor \\(x\\) considerado. (Suavizado local.) ¿Por qué centrarse en modelos lineales? Dos grandes razones: Los modelos lineales son el modelo de referencia. Los modelos lineales han existido durante mucho tiempo y son computacionalmente fáciles. Es posible que un modelo lineal no sea el modelo final que utilice, pero a menudo debería ser el primer modelo que pruebe. Las ideas detrás de los modelos lineales se pueden transferir fácilmente a otras técnicas de modelado. 10.1.4 Modelo asumido, modelo ajustado Cuando buscamos un modelo, a menudo necesitamos hacer suposiciones. Estos supuestos están codificados en la familia y forma del modelo. Por ejemplo \\[ y = \\beta_0 + \\beta_1 x_{1} + \\beta_3 x_{3} + \\beta_5 x_{5} + \\epsilon \\] se asume que \\(y\\) es una combinación lineal de \\(x_ {1}\\), \\(x_ {3}\\) y \\(x_ {5}\\), y también algo de ruido. Esto supone que el efecto de \\(x_{1}\\) en \\(y\\) es \\(\\beta_1\\), que es el mismo para todos los valores de \\(x_{3}\\) y \\(x_{5}\\). Es decir, estamos usando la familia de modelos lineales con una forma particular. Supongamos que luego ajustamos este modelo a algunos datos y obtenemos el modelo ajustado. Por ejemplo, en R usaríamos fit = lm(y ~ x1 + x3 + x5, data = some_data) Esta es la forma en R de decir que la familia es lineal y de especificar la forma de arriba. Un modelo aditivo con los predictores especificados, así como una intersección. Entonces obtenemos \\[ \\hat{y} = 1.5 + 0.9 x_{1} + 1.1 x_{3} + 2.3 x_{5}. \\] Esta es nuestra mejor suposición para la función \\(f\\) en \\[ y = f(x_1, x_2, x_3, \\ldots, x_{p-1}) + \\epsilon \\] para la familia y forma asumidas. Ajustar un modelo solo nos da el mejor ajuste para la familia y la forma que especificamos. Entonces la pregunta natural es; ¿Cómo elegimos la familia y la forma correctas? Nos facalizaremos en forma ya que nos centramos en la familia de modelos lineales. 10.2 Explicación versus predicción ¿Cuál es el propósito de ajustar un modelo a los datos? Por lo general, es para lograr uno de dos objetivos. Podemos usar un modelo para explicar la relación entre la respuesta y los predictores. Los modelos también se pueden utilizar para predecir la respuesta en función de los predictores. A menudo, un buen modelo hará ambas cosas, pero discutiremos ambos objetivos por separado, ya que el proceso de encontrar modelos para explicar y predecir tiene algunas diferencias. Para nuestros propósitos, dado que solo estamos considerando modelos lineales, buscar un buen modelo es esencialmente buscar una buena forma de un modelo. 10.2.1 Explicación Si el objetivo de un modelo es explicar la relación entre la respuesta y los predictores, buscamos un modelo que sea pequeño e interpretable, pero que se ajuste bien a los datos. Cuando se habla de modelos lineales, el tamaño de un modelo es esencialmente el número de parámetros \\(\\beta\\) utilizados. Supongamos que nos gustaría encontrar un modelo que explique la eficiencia del combustible (mpg) en función de los atributos de un automóvil (wt, year,cyl, disp,hp, acc). Quizás somos un fabricante de automóviles que intenta diseñar un vehículo de bajo consumo de combustible. Si este es el caso, estamos interesados en qué variables predictoras son útiles para explicar la eficiencia de combustible del automóvil y en cómo esas variables afectan la eficiencia de combustible. Al comprender esta relación, podemos utilizar este conocimiento en nuestro beneficio al diseñar un automóvil. Para explicar una relación, nos interesa mantener los modelos lo más pequeños posible, ya que los modelos más pequeños son fáciles de interpretar. Cuantos menos predictores, menos consideraciones debemos tener en cuenta en nuestro proceso de diseño. Tenga en cuenta que los modelos lineales de cualquier tamaño son bastante interpretables para empezar. Más adelante en sus carreras de análisis de datos, verá modelos más complicados que pueden ajustarse mejor a los datos, pero son mucho más difíciles, si no imposibles, de interpretar. Estos modelos no son tan útiles para explicar una relación. Esta es otra razón para intentar siempre un modelo lineal. Si ajusta tan bien como con los métodos más complicados, será el más fácil de entender. Para encontrar modelos pequeños e interpretables, eventualmente usaremos procedimientos de selección, que buscan entre muchas formas posibles de un modelo. Por ahora haremos esto de una manera más ad-hoc usando técnicas de inferencia que ya hemos encontrado. Para usar la inferencia como la hemos visto, necesitamos un supuesto adicional además de la familia y la forma del modelo. \\[ y = \\beta_0 + \\beta_1 x_{1} + \\beta_3 x_{3} + \\beta_5 x_{5} + \\epsilon \\] Nuestra suposición adicional se refiere al término de error. \\[ \\epsilon \\sim N(0, \\sigma^2) \\] Esta suposición de que los errores se distribuyen normalmente con alguna variación común es la clave de todas las inferencias que hemos hecho hasta ahora. Discutiremos esto con gran detalle más adelante. Entonces, con nuestras herramientas de inferencia (ANOVA y $ t $ -test) tenemos dos estrategias potenciales. Comience con un modelo muy pequeño (sin predictores) e intente agregar predictores. O bien, comience con un modelo grande (todos los predictores) e intente eliminarlos. 10.2.1.1 Correlación y causalidad Una advertencia al usar un modelo para explicar una relación. Hay dos términos que se utilizan a menudo para describir una relación entre dos variables: causalidad y correlación. Correlación a menudo también se conoce como asociación. El hecho de que dos variables estén correlacionadas no significa necesariamente que una cause la otra. Por ejemplo, considere modelar mpg solo como una función dehp. plot(mpg ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) ¿Un aumento en los caballos de fuerza causa una disminución en la eficiencia del combustible? O quizás la causalidad se invierte y un aumento en la eficiencia del combustible causa una disminución en los caballos de fuerza. ¡O quizás haya una tercera variable que explique ambos! El problema aquí es que tenemos datos de observación. Con datos de observación, solo podemos detectar asociaciones. Para hablar con confianza sobre causalidad, necesitaríamos realizar experimentos. A menudo, esta decisión se toma por nosotros, antes de que veamos los datos, por lo que solo podemos modificar nuestra interpretación. Discutiremos esto más a fondo cuando analicemos el diseño experimental y las técnicas tradicionales de ANOVA. (Todo lo cual ha sido rebautizado recientemente como prueba A/B). 10.2.2 Predicción Si el objetivo de un modelo es predecir la respuesta, entonces la única consideración es qué tan bien se ajusta el modelo a los datos. Para ello, necesitaremos una métrica. En los problemas de regresión, suele ser RMSE. \\[ \\text{RMSE}(\\text{model, data}) = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2} \\] donde \\(y_i\\) son los valores reales de la respuesta para los datos dados \\(\\hat{y}_i\\) son los valores predichos usando el modelo ajustado y los predictores de los datos Aquí la correlación y la causalidad no son un problema. Si un predictor está correlacionado con la respuesta, es útil para la predicción. Por ejemplo, en los niños en edad de escuela primaria, la talla de su zapato ciertamente no hace que lean a un nivel superior, sin embargo, podríamos usar la talla de un zapato muy fácilmente para hacer una predicción sobre la capacidad de lectura de un niño. Cuanto mayor sea el tamaño de sus zapatos, mejor leen. Sin embargo, hay una variable al acecho aquí, ¡su edad! (No envíe a sus hijos a la escuela con zapatos de talla 40, ¡no les hará leer mejor!) Además, dado que no estamos realizando inferencias, no se necesita la suposición adicional sobre los errores. Lo único que nos importa es qué tan cerca está el modelo ajustado de los datos. Los mínimos cuadrados son mínimos cuadrados. Para un modelo específico, encontrará los valores de los parámetros que minimizarán la pérdida por error al cuadrado. Sus resultados pueden ser en gran parte ininterpretables e inútiles para la inferencia, pero para la predicción nada de eso importa. Supongamos que en lugar del fabricante al que le gustaría fabricar un automóvil, somos un consumidor que desea comprar un automóvil nuevo. Sin embargo, este automóvil en particular es tan nuevo que no ha sido probado rigurosamente, por lo que no estamos seguros de qué eficiencia de combustible esperar. (Y, como escépticos, no confiamos en lo que nos dice el fabricante). En este caso, nos gustaría usar el modelo para ayudar a predecir la eficiencia de combustible de este automóvil en función de sus atributos, que son los predictores del modelo. Cuanto menores son los errores que comete el modelo, más confianza tenemos en su predicción. 10.2.2.1 División de Entrenamiento-Prueba El problema de usar el RMSE para identificar qué tan bien se ajusta un modelo a los datos es que RMSE es siempre (igual o) más bajo para un modelo grande. Esto sugeriría que siempre deberíamos usar el modelo más grande posible cuando busquemos un modelo que prediga bien. El problema con esto es la posibilidad de sobreajustar a los datos. Por lo tanto, queremos un modelo que se adapte bien, pero que no se adapte demasiado. Para comprender el sobreajuste, debemos pensar en aplicar un modelo a los datos visibles y no visibles. Suponga que ajustamos un modelo usando todos los datos disponibles y evaluamos el RMSE en este modelo ajustado y todos los datos vistos. Llamaremos a estos, los datos y RMSE de entrenamiento. Ahora, supongamos que encontramos mágicamente algunos datos adicionales. Para evaluar realmente qué tan bien predice el modelo, debemos evaluar qué tan bien nuestros modelos predicen la respuesta de estos datos. Llamaremos a estos, los datos y RMSE de prueba. RMSE de entrenamiento: ajuste del modelo en datos vistos, evaluado en datos vistos RMSE de prueba: ajuste del modelo en datos vistos, evaluado en datos no vistos A continuación, simulamos algunos datos y ajustamos dos modelos. Llamaremos a la línea azul sólida el modelo simple. La línea naranja discontinua se denominará modelo complejo, que se ajustó con métodos que aún no conocemos. El panel de la izquierda muestra los datos que se utilizaron para ajustar los dos modelos. Claramente, el modelo complejo se ajusta mucho mejor a los datos. El panel de la derecha muestra datos adicionales que se simularon de la misma manera que los datos originales. Aquí vemos que el modelo simple ajusta mucho mejor. La línea discontinua naranja casi parece aleatoria. Modelo RMSE de Entrenamiento RMSE de Prueba Simple 1.71 1.45 Complejo 1.41 2.07 El modelo más complejo y ondulado se ajusta mucho mejor a los datos de entrenamiento, ya que tiene un RMSE de entrenamiento mucho más bajo. Sin embargo, vemos que el modelo simple se ajusta mucho mejor a los datos de prueba, con un RMSE de prueba mucho más bajo. Esto significa que el modelo complejo ha sobreajustado los datos y nosotros preferimos el modelo simple. Al elegir un modelo para la predicción, preferimos un modelo que predice datos invisibles. En la práctica, no puede simplemente generar más datos para evaluar sus modelos. En su lugar, dividimos los datos existentes en datos utilizados para ajustar el modelo (entrenamiento) y datos utilizados para evaluar el modelo (prueba). Nunca ajuste un modelo con datos de prueba. 10.3 Resumen Los modelos se pueden utilizar para explicar relaciones y predecir observaciones. Al usar el modelo para, explicar; preferimos modelos pequeños e interpretables. predecir; preferimos modelos que cometan los errores más pequeños posibles, sin sobreajuste. Los modelos lineales pueden lograr ambos objetivos. Más adelante, veremos que a menudo un modelo lineal que logra uno de estos objetivos, generalmente logra el otro. "],["interacciones-y-predictores-categóricos.html", "Capítulo 11 Interacciones y predictores categóricos 11.1 Variables ficticias (Dummy) 11.2 Interacciones 11.3 Variables factor 11.4 Parametrización 11.5 Construcción de modelos más grandes", " Capítulo 11 Interacciones y predictores categóricos El mayor valor de una imagen es cuando nos obliga a notar lo que nunca esperábamos ver.  John Tukey Después de leer este capítulo, podrá: Incluir e interpretar variables categóricas en un modelo de regresión lineal mediante variables ficticias. Comprender las implicaciones de usar un modelo con una variable categórica de dos maneras: niveles que sirven como predictores únicos versus niveles que sirven como comparación con una línea de base. Construir e interpretar modelos de regresión lineal con términos de interacción. Identificar variables categóricas en un conjunto de datos y convertirlas en variables factor, si es necesario, utilizando R. Hasta ahora, en cada uno de nuestros análisis, solo hemos utilizado variables numéricas como predictores. También hemos utilizado solo modelos aditivos, lo que significa que el efecto que cualquier predictor tuvo en la respuesta no dependió de los otros predictores. En este capítulo, eliminaremos ambas restricciones. Ajustaremos modelos con predictores categóricos y usaremos modelos que permitan a los predictores interactuar. Las matemáticas de la regresión múltiple permanecerán en gran parte sin cambios, sin embargo, prestaremos mucha atención a la interpretación, así como a algunas diferencias en el uso de R. 11.1 Variables ficticias (Dummy) Para este capítulo, usaremos brevemente el conjunto de datos integrado mtcars antes de regresar a nuestro conjunto de datos autompg que creamos en el último capítulo. El conjunto de datos mtcars es algo más pequeño, por lo que rápidamente veremos todo el conjunto de datos. mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Nos interesarán tres de las variables: mpg, hp yam. mpg: eficiencia de combustible, en millas por galón. hp: caballos de fuerza, en libras-pie por segundo. am: transmisión. Automática o manual. Como hacemos a menudo, comenzaremos graficando los datos. Estamos interesados en mpg como variable respuesta y hp como predictor. plot(mpg ~ hp, data = mtcars, cex = 2) Dado que también estamos interesados en el tipo de transmisión, también podríamos etiquetar los puntos, plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2) legend(&quot;topright&quot;, c(&quot;Automática&quot;, &quot;Manual&quot;), col = c(1, 2), pch = c(1, 2)) Usamos un truco común de R al graficar estos datos. La variable am toma dos valores posibles; 0 para transmisión automática y 1 para transmisiones manuales. R puede usar números para representar colores, sin embargo, el color de 0 es blanco. Así que tomamos el vector am y le agregamos 1. Entonces, las observaciones con transmisiones automáticas ahora se representan con 1, que es negro en R, y la transmisión manual se representa con 2, que es rojo en R. (Tenga en cuenta que solo estamos agregando 1 dentro de la llamada a plot(), en realidad no estamos modificando los valores almacenados en am). Ahora ajustamos el modelo SLR \\[ Y = \\beta_0 + \\beta_1 x_1 + \\epsilon, \\] donde \\(Y\\) es mpg y \\(x_1\\) es hp. Para abreviar la notación, descartamos el índice \\(i\\) para las observaciones. mpg_hp_slr = lm(mpg ~ hp, data = mtcars) Luego volvemos a graficar los datos y agregamos la línea ajustada plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2) abline(mpg_hp_slr, lwd = 3, col = &quot;grey&quot;) legend(&quot;topright&quot;, c(&quot;Automática&quot;, &quot;Manual&quot;), col = c(1, 2), pch = c(1, 2)) Deberíamos notar un patrón. Las observaciones manuales rojas están en gran parte por encima de la línea, mientras que las observaciones automáticas negras están en su mayoría por debajo de la línea. Esto significa que nuestro modelo subestima la eficiencia de combustible de las transmisiones manuales y sobreestima la eficiencia de combustible de las transmisiones automáticas. Para corregir esto, agregaremos un predictor a nuestro modelo, a saber, am como \\(x_2\\). Nuestro nuevo modelo es \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon, \\] donde \\(x_1\\) y \\(Y\\) siguen siendo los mismos, pero ahora \\[ x_2 = \\begin{cases} 1 &amp; \\text{transmisión manual} \\\\ 0 &amp; \\text{transmisión automática} \\end{cases}. \\] En este caso, llamamos \\(x_2\\) una variable ficticia. Una variable ficticia tiene un nombre algo desafortunado, ya que de ninguna manera es tonta. De hecho, en realidad es algo inteligente. Una variable ficticia es una variable numérica que se utiliza en un análisis de regresión para codificar una variable categórica binaria. Veamos cómo funciona esto. Primero, tenga en cuenta que am ya es una variable ficticia, ya que usa los valores 0 y 1 para representar transmisiones automáticas y manuales. A menudo, una variable como am almacenaría los valores de los caracteres auto y man y tendríamos que convertirlos en 0 y 1 o, como veremos más adelante,R prestará cuidado de crear variables ficticias por nosotros. Entonces, para ajustar el modelo anterior, lo hacemos como cualquier otro modelo de regresión múltiple que hayamos visto antes. mpg_hp_add = lm(mpg ~ hp + am, data = mtcars) Comprobando brevemente la salida, vemos que R ha estimado los tres parámetros \\(\\beta\\). mpg_hp_add ## ## Call: ## lm(formula = mpg ~ hp + am, data = mtcars) ## ## Coefficients: ## (Intercept) hp am ## 26.58491 -0.05889 5.27709 Dado que \\(x_2\\) solo puede tomar valores 0 y 1, podemos escribir efectivamente dos modelos diferentes, uno para transmisiones manuales y otro para transmisiones automáticas. Para transmisiones automáticas, \\(x_2 = 0\\), tenemos, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\epsilon. \\] Luego, para las transmisiones manuales, \\(x_2=1\\), tenemos, \\[ Y = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon. \\] Observe que estos modelos comparten la misma pendiente, \\(\\beta_1\\), pero tienen intersecciones diferentes, que difieren en \\(\\beta_2\\). Entonces, el cambio en mpg es el mismo para ambos modelos, pero en promedio,mpg difiere en \\(\\beta_2\\) entre los dos tipos de transmisión. Ahora calcularemos la pendiente estimada y el intercepto de estos dos modelos para que podamos agregarlos a una gráfica. Tenga en cuenta que: \\(\\hat{\\beta}_0\\) = coef(mpg_hp_add)[1] = 26.5849137 \\(\\hat{\\beta}_1\\) = coef(mpg_hp_add)[2] = -0.0588878 \\(\\hat{\\beta}_2\\) = coef(mpg_hp_add)[3] = 5.2770853 Luego, podemos combinarlos para calcular la pendiente y las intersecciones estimadas. int_auto = coef(mpg_hp_add)[1] int_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3] slope_auto = coef(mpg_hp_add)[2] slope_manu = coef(mpg_hp_add)[2] Al volver a graficar los datos, usamos estas pendientes e intersecciones para agregar los dos modelos ajustados a la gráfica. plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2) abline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # agregar línea para auto abline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # agregar línea para manual legend(&quot;topright&quot;, c(&quot;Automática&quot;, &quot;Manual&quot;), col = c(1, 2), pch = c(1, 2)) Notamos de inmediato que los puntos ya no son sistemáticamente incorrectos. Las observaciones manuales rojas varían alrededor de la línea roja sin un patrón en particular sin subestimar las observaciones como antes. Los puntos negros automáticos varían alrededor de la línea negra, también sin un patrón obvio. Dicen que una imagen vale más que mil palabras, pero como estadístico, vale la pena realizar un análisis completo. La imagen de arriba hace claramente obvio que \\(\\beta_2\\) es significativo, pero verifiquémoslo matemáticamente. Básicamente nos gustaría probar: \\[ H_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_1: \\beta_2 \\neq 0. \\] Esto no es nada nuevo. Nuevamente, las matemáticas son las mismas que las de los análisis de regresión múltiple que hemos visto antes. Podríamos realizar una prueba \\(t\\) o \\(F\\). La única diferencia es un ligero cambio de interpretación. Podríamos pensar en esto como probar un modelo con una sola línea (\\(H_0\\)) contra un modelo que permite dos líneas (\\(H_1\\)). Para obtener el estadístico de prueba y el valor p para la prueba \\(t\\), usaríamos summary(mpg_hp_add)$coefficients[&quot;am&quot;,] ## Estimate Std. Error t value Pr(&gt;|t|) ## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05 Para hacer lo mismo con la prueba \\(F\\), usaríamos anova(mpg_hp_slr, mpg_hp_add) ## Analysis of Variance Table ## ## Model 1: mpg ~ hp ## Model 2: mpg ~ hp + am ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 30 447.67 ## 2 29 245.44 1 202.24 23.895 3.46e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Tenga en cuenta que estos de hecho están probando lo mismo, ya que los valores p son exactamente iguales. (Y el estadístico de prueba \\(F\\) es el estadístico de prueba \\(t\\) al cuadrado). Recapitulando algunas interpretaciones: \\(\\hat{\\beta}_0 = 26.5849137\\) es el promedio estimado de mpg para un automóvil con transmisión automática y 0 hp. \\(\\hat{\\beta}_0 + \\hat{\\beta}_2 = 31.8619991\\) es el promedio estimado de mpg para un automóvil con transmisión manual y 0 hp. \\(\\hat{\\beta}_2 = 5.2770853\\) es la diferencia estimada en el promedio de mpg para automóviles con transmisión manual en comparación con aquellos con transmisión automática, para cualquier hp. \\(\\hat{\\beta}_1 = -0.0588878\\) es el cambio estimado en el promedio de mpg para un aumento en un hp, para cualquiera de los tipos de transmisión. Deberíamos prestar especial atención a esos dos últimos. En el modelo, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon, \\] vemos que \\(\\beta_1\\) es el cambio promedio en \\(Y\\) para un aumento en \\(x_1\\), sin importar el valor de \\(x_2\\). Además, \\(\\beta_2\\) es siempre la diferencia en el promedio de \\(Y\\) para cualquier valor de \\(x_1\\). Estas son dos restricciones que no siempre querremos, por lo que necesitamos una forma de especificar un modelo más flexible. Aquí nos limitamos a un solo predictor numérico \\(x_1\\) y una variable ficticia \\(x_2\\). Sin embargo, el concepto de variable ficticia se puede utilizar con modelos de regresión múltiple más grandes. Aquí solo usamos un único predictor numérico para facilitar la visualización, ya que podemos pensar en la interpretación de dos líneas. Pero, en general, podemos pensar en una variable ficticia como la creación de dos modelos, uno para cada categoría de una variable categórica binaria. 11.2 Interacciones Para eliminar la restricción de la misma pendiente, ahora discutiremos la interacción. Para ilustrar este concepto, regresaremos al conjunto de datos autompg que creamos en el último capítulo, con algunas modificaciones más. # leer el marco de datos de la web autompg = read.table( &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;, quote = &quot;\\&quot;&quot;, comment.char = &quot;&quot;, stringsAsFactors = FALSE) # dar los encabezados del marco de datos colnames(autompg) = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;, &quot;origin&quot;, &quot;name&quot;) # eliminar los datos que faltan, que se almacenan como &quot;?&quot; autompg = subset(autompg, autompg$hp != &quot;?&quot;) # eliminar plymouth reliant, ya que causa algunos problemas autompg = subset(autompg, autompg$name != &quot;plymouth reliant&quot;) # dar los nombres de las filas del conjunto de datos, según el motor, el año y el nombre rownames(autompg) = paste(autompg$cyl, &quot;cylinder&quot;, autompg$year, autompg$name) # eliminar la variable de nombre autompg = subset(autompg, select = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;, &quot;origin&quot;)) # cambiar caballos de fuerza de carácter a numérico autompg$hp = as.numeric(autompg$hp) # Cree una variable ficticia para automóviles nacionales y extranjeros. nacionales = 1. autompg$domestic = as.numeric(autompg$origin == 1) # quitar los carros de 3 y 5 cilindros (que son muy raros). autompg = autompg[autompg$cyl != 5,] autompg = autompg[autompg$cyl != 3,] # la siguiente línea verificaría que las posibilidades restantes del cilindro son 4, 6, 8 #unique(autompg$cyl) # cambiar cyl a una variable de factor autompg$cyl = as.factor(autompg$cyl) str(autompg) ## &#39;data.frame&#39;: 383 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ... Hemos eliminado los automóviles con cilindros 3 y5, y hemos creado una nueva variable domestic que indica si un automóvil se fabricó o no en los Estados Unidos. Quitar los cilindros 3 y5 es simplemente para facilitar la demostración más adelante en el capítulo y no se haría en la práctica. La nueva variable domestic toma el valor1 si el automóvil se fabricó en los Estados Unidos, y 0 en caso contrario, al que nos referiremos como extranjero. (Estamos usando arbitrariamente los Estados Unidos como punto de referencia.) También hemos convertido cyl y origin en variables de factor, que discutiremos más adelante. Ahora nos ocuparemos de tres variables: mpg, disp y domestic. Usaremos mpg como respuesta. Podemos ajustar un modelo, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon, \\] donde \\(Y\\) es mpg, la eficiencia del combustible en millas por galón, \\(x_1\\) es disp, el desplazamiento en pulgadas cúbicas, \\(x_2\\) es domestic como se describió anteriormente, es una variable ficticia. \\[ x_2 = \\begin{cases} 1 &amp; \\text{Nacional} \\\\ 0 &amp; \\text{Extranjero} \\end{cases} \\] Ajustaremos este modelo, extraeremos la pendiente y el intercepto de las dos líneas, graficaremos los datos y sumaremos las líneas. mpg_disp_add = lm(mpg ~ disp + domestic, data = autompg) int_for = coef(mpg_disp_add)[1] int_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3] slope_for = coef(mpg_disp_add)[2] slope_dom = coef(mpg_disp_add)[2] plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1) abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # agregar línea para autos extranjeros abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # agregar línea para autos nacionales legend(&quot;topright&quot;, c(&quot;Extranjero&quot;, &quot;Nacional&quot;), pch = c(1, 2), col = c(1, 2)) Este es un modelo que muestra dos líneas paralelas, lo que significa que el mpg puede ser diferente en promedio entre los automóviles nacionales y extranjeros del mismo desplazamiento del motor, pero el cambio en elmpg promedio para un aumento en el desplazamiento es el mismo para ambos. Podemos ver que este modelo no está funcionando muy bien. La línea roja se ajusta bastante bien a los puntos rojos, pero a la línea negra no le va muy bien con los puntos negros, claramente debería tener una pendiente más negativa. Básicamente, nos gustaría un modelo que muestre dos pendientes diferentes. Considere el siguiente modelo, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\] donde \\(x_1\\), \\(x_2\\) y \\(Y\\) son los mismos que antes, pero hemos agregado un nuevo término de interacción \\(x_1x_2\\) que multiplica \\(x_1\\) y \\(x_2\\), por lo que también tenemos un parámetro adicional \\(\\beta\\) \\(\\beta_3\\). Este modelo esencialmente crea dos pendientes y dos intersecciones, siendo \\(\\beta_2\\) la diferencia en las intersecciones y \\(\\beta_3\\) la diferencia en las pendientes. Para ver esto, dividiremos el modelo en dos submodelos para automóviles nacionales y extranjeros. Para autos extranjeros, \\(x_2=0\\), tenemos \\[ Y = \\beta_0 + \\beta_1 x_1 + \\epsilon. \\] Para autos nacionales, \\(x_2=1\\), tenemos \\[ Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 + \\epsilon. \\] Estos modelos tienen pendientes e intersecciones diferentes. \\(\\beta_0\\) es el mpg promedio para un automóvil extranjero con 0 disp. \\(\\beta_1\\) es el cambio en el mpg promedio para un aumento de un disp, para vehículos extranjeros. \\(\\beta_0+\\beta_2\\) es el mpg promedio para un automóvil nacional con 0disp. \\(\\beta_1+\\beta_3\\) es el cambio en el promedio de mpg para un aumento de un disp, para autos nacionales. ¿Cómo ajustamos este modelo en R? Hay diferentes maneras. Un método sería simplemente crear una nueva variable y luego ajustar un modelo como cualquier otro. autompg$x3 = autompg$disp * autompg$domestic # ¡ESTE CÓDIGO NO FUNCIONA! do_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # ¡ESTE CÓDIGO NO FUNCIONA! Solo debe hacer esto como último recurso. Preferimos no tener que modificar nuestros datos simplemente para ajustarlos a un modelo. En su lugar, podemos decirle a R que nos gustaría usar los datos existentes con un término de interacción, que creará automáticamente cuando usemos el operador :. mpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg) Un método alternativo, que se ajustará exactamente al mismo modelo anterior, sería utilizar el operador *. Este método crea automáticamente el término de interacción, así como cualquier término de orden inferior, que en este caso son los términos de primer orden para disp y domestic mpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg) Podemos verificar rápidamente que estos están haciendo lo mismo. coef(mpg_disp_int) ## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184 coef(mpg_disp_int2) ## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184 Vemos que tanto las variables como las estimaciones de sus coeficientes son, de hecho, las mismas para ambos modelos. summary(mpg_disp_int) ## ## Call: ## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8332 -2.8956 -0.8332 2.2828 18.7749 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.05484 1.80582 25.504 &lt; 2e-16 *** ## disp -0.15692 0.01668 -9.407 &lt; 2e-16 *** ## domestic -12.57547 1.95644 -6.428 3.90e-10 *** ## disp:domestic 0.10252 0.01692 6.060 3.29e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.308 on 379 degrees of freedom ## Multiple R-squared: 0.7011, Adjusted R-squared: 0.6987 ## F-statistic: 296.3 on 3 and 379 DF, p-value: &lt; 2.2e-16 Vemos que el uso de summary() da el resultado habitual para un modelo de regresión múltiple. Prestamos mucha atención a la fila de disp:domestic que prueba, \\[ H_0: \\beta_3 = 0. \\] En este caso, la prueba de \\(\\beta_3=0\\) está probando dos líneas con pendientes paralelas frente a dos líneas con pendientes posiblemente diferentes. La línea disp:domestic en la salida de summary() usa una prueba \\(t\\) para realizar la prueba. También podríamos usar una prueba ANOVA \\(F\\). El modelo aditivo, sin interacción es nuestro modelo nulo, y el modelo de interacción es el alternativo. anova(mpg_disp_add, mpg_disp_int) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + domestic ## Model 2: mpg ~ disp + domestic + disp:domestic ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 380 7714.0 ## 2 379 7032.6 1 681.36 36.719 3.294e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nuevamente vemos que esta prueba tiene el mismo valor p que la prueba \\(t\\). Además, el valor p es extremadamente bajo, por lo que entre los dos, elegimos el modelo de interacción. int_for = coef(mpg_disp_int)[1] int_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3] slope_for = coef(mpg_disp_int)[2] slope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4] Aquí calculamos nuevamente la pendiente y las intersecciones de las dos líneas para su uso en la gráfica. plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1) abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # line for foreign cars abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # line for domestic cars legend(&quot;topright&quot;, c(&quot;Extranjero&quot;, &quot;Nacionl&quot;), pch = c(1, 2), col = c(1, 2)) Vemos que estas líneas se ajustan mucho mejor a los datos, lo que coincide con el resultado de nuestras pruebas. Hasta ahora solo hemos visto interacción entre una variable categórica (domestic) y una variable numérica (disp). Si bien esto es fácil de visualizar, dado que permite diferentes pendientes para dos líneas, no es el único tipo de interacción que podemos usar en un modelo. También podemos considerar interacciones entre dos variables numéricas. Considere el modelo, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\] donde \\(Y\\) es mpg, la eficiencia de combustible en millas por galón, \\(x_1\\) es disp, el desplazamiento en pulgadas cúbicas, \\(x_2\\) es hp, los caballos de fuerza, en pies-libras por segundo. ¿Cómo cambia mpg basado en disp en este modelo? Podemos reorganizar algunos términos para ver cómo. \\[ Y = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon \\] Entonces, para un aumento de una unidad en \\(x_1\\) (disp), la media de \\(Y\\) (mpg) aumenta \\(\\beta_1 + \\beta_3 x_2\\), que es un valor diferente dependiendo del valor de \\(x_2\\) (hp)! Dado que ahora estamos trabajando en tres dimensiones, este modelo no se puede justificar fácilmente mediante visualizaciones como el ejemplo anterior. En cambio, tendremos que confiar en una prueba. mpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg) mpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg) summary(mpg_disp_int_hp) ## ## Call: ## lm(formula = mpg ~ disp * hp, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.7849 -2.3104 -0.5699 2.1453 17.9211 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.241e+01 1.523e+00 34.42 &lt;2e-16 *** ## disp -1.002e-01 6.638e-03 -15.09 &lt;2e-16 *** ## hp -2.198e-01 1.987e-02 -11.06 &lt;2e-16 *** ## disp:hp 5.658e-04 5.165e-05 10.96 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.896 on 379 degrees of freedom ## Multiple R-squared: 0.7554, Adjusted R-squared: 0.7535 ## F-statistic: 390.2 on 3 and 379 DF, p-value: &lt; 2.2e-16 Usando summary() nos enfocamos en la fila de disp:hp que prueba, \\[ H_0: \\beta_3 = 0. \\] Nuevamente, vemos un valor p muy bajo, por lo que rechazamos el nulo (modelo aditivo) a favor del modelo de interacción. Nuevamente, hay una prueba \\(F\\) equivalente. anova(mpg_disp_add_hp, mpg_disp_int_hp) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + hp ## Model 2: mpg ~ disp * hp ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 380 7576.6 ## 2 379 5754.2 1 1822.3 120.03 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Podemos echar un vistazo más de cerca a los coeficientes de nuestro modelo de interacción ajustado. coef(mpg_disp_int_hp) ## (Intercept) disp hp disp:hp ## 52.4081997848 -0.1001737655 -0.2198199720 0.0005658269 \\(\\hat{\\beta}_0 = 52.4081998\\) es el promedio estimado de mpg para un automóvil con 0 disp y 0 hp. \\(\\hat{\\beta}_1 = -0.1001738\\) es el cambio estimado en el promedio de mpg para un aumento de 1 disp, para un automóvil con 0 hp. \\(\\hat{\\beta}_2 = -0.21982\\) es el cambio estimado en el promedio de mpg para un aumento de 1 hp, para un automóvil con 0 disp. \\(\\hat{\\beta}_3 = 5.658269\\times 10^{-4}\\) es una estimación de la modificación del cambio en el promedio de mpg para un aumento en disp, para un automóvil de cierto hp (o viceversa). Ese último coeficiente necesita más explicación. Recuerda el reordenamiento que hicimos antes. \\[ Y = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon. \\] Entonces, nuestra estimación de \\(\\beta_1+\\beta_3x_2\\), es $_1+_3x_2 $, que en este caso es \\[ -0.1001738 + 5.658269\\times 10^{-4} x_2. \\] Esto dice que, para un aumento de un disp, vemos un cambio estimado en el mpg promedio de \\(-0.1001738 + 5.658269\\times 10^{-4} x_2\\). Entonces, la relación entre disp y mpg depende de los hp del automóvil. Entonces, para un automóvil con 50 hp, el cambio estimado en el promedio de mpg para un aumento de un disp es \\[ -0.1001738 + 5.658269\\times 10^{-4} \\cdot 50 = -0.0718824 \\] Y para un automóvil con 350 hp, el cambio estimado en el promedio dempg para un aumento de un disp es \\[ -0.1001738 + 5.658269\\times 10^{-4} \\cdot 350 = 0.0978657 \\] ¡Observe que el signo cambió! 11.3 Variables factor Hasta ahora en este capítulo, hemos limitado nuestro uso de variables categóricas a variables categóricas binarias. Específicamente, nos hemos limitado a variables ficticias que toman un valor de 0 o 1 y representan una variable categórica numéricamente. Ahora discutiremos las variables factor, que es una forma especial en la que R trata las variables categóricas. Con las variables factor, un usuario puede simplemente pensar en las categorías de una variable, y R se encargará de las variables ficticias necesarias sin que el usuario realice ninguna asignación 0/1. is.factor(autompg$domestic) ## [1] FALSE Anteriormente, cuando usamos la variable domestic, no era una variable factor. Era simplemente una variable numérica que solo tomaba dos valores posibles, 1 para nacionales y 0 para extranjeros. Creemos una nueva variable origin que almacene la misma información, pero de una manera diferente. autompg$origin[autompg$domestic == 1] = &quot;domestic&quot; autompg$origin[autompg$domestic == 0] = &quot;foreign&quot; head(autompg$origin) ## [1] &quot;domestic&quot; &quot;domestic&quot; &quot;domestic&quot; &quot;domestic&quot; &quot;domestic&quot; &quot;domestic&quot; Ahora, la variable origin almacena\"domestic\"para automóviles nacionales y \"foreign\" para automóviles extranjeros. is.factor(autompg$origin) ## [1] FALSE Sin embargo, esto es simplemente un vector de valores de caracteres. Un vector de modelos de automóviles es una variable de carácter en R. Un vector de números de identificación de vehículos (VIN) también es una variable de carácter. Pero esos no representan una lista corta de niveles que podrían influir en una variable respuesta. Querremos coaccionar esta variable origin para que sea algo más: una variable factor. autompg$origin = as.factor(autompg$origin) Ahora, cuando comprobamos la estructura del conjunto de datos autompg, vemos que el origin es una variable de factor. str(autompg) ## &#39;data.frame&#39;: 383 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : Factor w/ 2 levels &quot;domestic&quot;,&quot;foreign&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ... Las variables factor tienen niveles que son los posibles valores (categorías) que puede tomar la variable, en este caso extranjera o nacional. levels(autompg$origin) ## [1] &quot;domestic&quot; &quot;foreign&quot; Recordemos que previamente hemos ajustado el modelo \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\] donde \\(Y\\) es mpg, la eficiencia de combustible en millas por galón, \\(x_1\\) es disp, el desplazamiento en pulgadas cúbicas, \\(x_2\\) es domestic una variable ficticia donde 1 indica un automóvil nacional (mod_dummy = lm(mpg ~ disp * domestic, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp * domestic, data = autompg) ## ## Coefficients: ## (Intercept) disp domestic disp:domestic ## 46.0548 -0.1569 -12.5755 0.1025 Entonces aquí vemos \\[ \\hat{\\beta}_0 + \\hat{\\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709 \\] es el mpg promedio estimado para un automóvil nacional con 0 disp. Ahora intentemos hacer lo mismo, pero usando nuestra nueva variable factor. (mod_factor = lm(mpg ~ disp * origin, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp * origin, data = autompg) ## ## Coefficients: ## (Intercept) disp originforeign disp:originforeign ## 33.47937 -0.05441 12.57547 -0.10252 Parece que no produce los mismos resultados. Inmediatamente notamos que el intercepto es diferente, al igual que el coeficiente delante de disp. También notamos que los dos coeficientes restantes son de la misma magnitud que sus respectivas contrapartes usando la variable domestic, pero con un signo diferente. ¿Por qué está pasando esto? Resulta que al usar una variable de factor, R está creando automáticamente una variable ficticia para nosotros. Sin embargo, no es la variable ficticia que usamos originalmente. R esta ajustando el modelo \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\] donde \\(Y\\) es mpg, la eficiencia de combustible en millas por galón, \\(x_1\\) es disp, el desplazamiento en pulgadas cúbicas, \\(x_2\\) es una variable ficticia creada por R. Utiliza 1 para representar un auto extranjero. Y ahora, \\[ \\hat{\\beta}_0 = 33.4793709 \\] es el mpg promedio estimado para un automóvil doméstico con 0 disp, que de hecho es el mismo que antes. Cuando R creó \\(x_2\\), la variable ficticia, utilizó automóviles nacionales como el nivel de referencia, que es el valor predeterminado de la variable factor. Entonces, cuando la variable ficticia es 0, el modelo representa este nivel de referencia, que es nacional. (R hace esta elección porque domestic viene antes que foreing en orden alfabético). Entonces, los dos modelos tienen coeficientes estimados diferentes, pero debido a las diferentes representaciones, en realidad son el mismo modelo. 11.3.1 Factores con más de dos niveles Consideremos ahora una variable factor con más de dos niveles. En este conjunto de datos, cyl es un ejemplo. is.factor(autompg$cyl) ## [1] TRUE levels(autompg$cyl) ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; Aquí la variable cyl tiene tres niveles posibles: 4, 6 y 8. Quizás se pregunte, ¿por qué no usar simplemente cyl como variable numérica? Ciertamente podría. Sin embargo, eso obligaría a que la diferencia de mpg promedio entre los cilindros 4 y 6 sea la misma que la diferencia de mpg promedio entre los cilindros 6 y 8. Eso suele tener sentido para una variable continua, pero no para una variable discreta con tan pocos valores posibles. En el caso de esta variable, no existe un motor de 7 cilindros o un motor de 6.23 cilindros en los vehículos personales. Por estas razones, simplemente consideraremos que cyl es categórica. Esta es una decisión que normalmente deberá tomarse con variables ordinales. A menudo, con un gran número de categorías, la decisión de tratarlas como variables numéricas es apropiada porque, de lo contrario, se necesita una gran cantidad de variables ficticias para representarlas. Definamos tres variables ficticias relacionadas con la variable del factor cyl. \\[ v_1 = \\begin{cases} 1 &amp; \\text{4 cilindros} \\\\ 0 &amp; \\text{no 4 cilindros} \\end{cases} \\] \\[ v_2 = \\begin{cases} 1 &amp; \\text{6 cilindros} \\\\ 0 &amp; \\text{no 6 cilindros} \\end{cases} \\] \\[ v_3 = \\begin{cases} 1 &amp; \\text{8 cilindros} \\\\ 0 &amp; \\text{no 8 cilindros} \\end{cases} \\] Ahora, ajustemos un modelo aditivo en R, usando mpg como respuesta y disp y cyl como predictores. Este debe ser un modelo que use tres líneas de regresión para modelar mpg, una para cada uno de los posibles niveles de cyl. Todos tendrán la misma pendiente (ya que es un modelo aditivo), pero cada uno tendrá su propio intercepto (mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp + cyl, data = autompg) ## ## Coefficients: ## (Intercept) disp cyl6 cyl8 ## 34.99929 -0.05217 -3.63325 -2.03603 La pregunta es, ¿cuál es el modelo que R ha ajustado? Ha optado por utilizar el modelo \\[ Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon, \\] donde \\(Y\\) es mpg, la eficiencia de combustible en millas por galón, \\(x\\) es disp, el desplazamiento en pulgadas cúbicas, \\(v_2\\) y \\(v_3\\) son las variables ficticias definidas anteriormente. ¿Por qué R no usa \\(v_1\\)? Básicamente porque no es necesario. Para crear tres líneas, solo necesita dos variables ficticias ya que está usando un nivel de referencia, que en este caso es un automóvil de 4 cilindros. Los tres submodelos son entonces: 4 Cilindros: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\) 6 Cilindros: \\(Y = (\\beta_0 + \\beta_2) + \\beta_1 x + \\epsilon\\) 8 Cilindros: \\(Y = (\\beta_0 + \\beta_3) + \\beta_1 x + \\epsilon\\) Observe que todos tienen la misma pendiente. Sin embargo, usando las dos variables ficticias, logramos los tres interceptos. \\(\\beta_0\\) es el mpg promedio para un automóvil de 4 cilindros con 0 disp. \\(\\beta_0+\\beta_2\\) es el mpg promedio para un automóvil de 6 cilindros con 0 disp. \\(\\beta_0+\\beta_3\\) es el mpg promedio para un automóvil de 8 cilindros con 0 disp. Entonces, debido a que 4 cilindros es el nivel de referencia, \\(\\beta_0\\) es específico de 4 cilindros, pero \\(\\beta_2\\) y \\(\\beta_3\\) se utilizan para representar cantidades relativas a 4 cilindros. Como hemos hecho antes, podemos extraer estos interceptos y pendientes para las tres líneas y graficarlas. int_4cyl = coef(mpg_disp_add_cyl)[1] int_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3] int_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4] slope_all_cyl = coef(mpg_disp_add_cyl)[2] plot_colors = c(&quot;Darkorange&quot;, &quot;Darkgrey&quot;, &quot;Dodgerblue&quot;) plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl)) abline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2) abline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2) abline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2) legend(&quot;topright&quot;, c(&quot;4 Cilindros&quot;, &quot;6 Cilindros&quot;, &quot;8 Cilindros&quot;), col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3)) En esta gráfica, tenemos 4 cilindros: puntos naranjas, línea naranja continua. 6 cilindros: puntos grises, línea gris discontinua. 8 cilindros: puntos azules, línea azul punteada. Un resultado extraño es que estamos estimando que los autos de 8 cilindros tienen una mejor eficiencia de combustible que los autos de 6 cilindros en cualquier desplazamiento. La línea azul punteada siempre está por encima de la línea gris punteada. Eso no parece correcto. Tal vez para motores de cilindrada muy grande eso podría ser cierto, pero parece incorrecto para motores de cilindrada media a baja. Para intentar arreglar esto, intentaremos usar un modelo de interacción, es decir, en lugar de simplemente tres interceptos y una pendiente, permitiremos tres pendientes. Nuevamente, dejaremos que R tome el volante, (sin juego de palabras) y luego averiguaremos qué modelo ha aplicado. (mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp * cyl, data = autompg) ## ## Coefficients: ## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817 # también podría usar mpg ~ disp + cyl + disp:cyl R ha vuelto a optar por utilizar coches de 4 cilindros como nivel de referencia, pero ahora esto también tiene un efecto en los términos de interacción. R se ajusta al modelo. \\[ Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon \\] Estamos usando \\(\\gamma\\) como un parámetro \\(\\beta\\) para simplificar, de modo que, por ejemplo, \\(\\beta_2\\) y \\(\\gamma_2\\) están asociados con \\(v_2\\). Ahora, los tres submodelos son: 4 cilindros: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\). 6 cilindros: \\(Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\gamma_2) x + \\epsilon\\). 8 cilindros: \\(Y = (\\beta_0 + \\beta_3) + (\\beta_1 + \\gamma_3) x + \\epsilon\\). Interpretando algunos parámetros y coeficientes: \\((\\beta_0 + \\beta_2)\\) es el mpg promedio de un automóvil de 6 cilindros con 0 disp \\((\\hat{\\beta}_1 + \\hat{\\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\\) es el cambio estimado en el promedio de mpg para un aumento de un disp, en un automóvil de 8 cilindros. Entonces, como hemos visto antes, \\(\\beta_2\\) y \\(\\beta_3\\) cambian los interceptos para autos de 6 y 8 cilindros en relación con el nivel de referencia de \\(\\beta_0\\) para autos de 4 cilindros. Ahora, de manera similar \\(\\gamma_2\\) y \\(\\gamma_3\\) cambian las pendientes para autos de 6 y 8 cilindros en relación con el nivel de referencia de \\(\\beta_1\\) para autos de 4 cilindros. Una vez más, extraemos los coeficientes y graficamos los resultados. int_4cyl = coef(mpg_disp_int_cyl)[1] int_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3] int_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4] slope_4cyl = coef(mpg_disp_int_cyl)[2] slope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5] slope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6] plot_colors = c(&quot;Darkorange&quot;, &quot;Darkgrey&quot;, &quot;Dodgerblue&quot;) plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl)) abline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2) abline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2) abline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2) legend(&quot;topright&quot;, c(&quot;4 cilindros&quot;, &quot;6 cilindros&quot;, &quot;8 cilindros&quot;), col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3)) ¡Esto se ve mucho mejor! Podemos ver que para los coches de cilindrada media, los coches de 6 cilindros ahora funcionan mejor que los de 8 cilindros, lo que parece mucho más razonable que antes. Para justificar completamente el modelo de interacción (es decir, una pendiente única para cada nivel cyl) en comparación con el modelo aditivo (pendiente única), podemos realizar una prueba \\(F\\). Observe primero que no hay una prueba \\(t\\) que pueda hacer esto, ya que la diferencia entre los dos modelos no es un solo parámetro. Nosotros probaremos, \\[ H_0: \\gamma_2 = \\gamma_3 = 0 \\] que representa las líneas de regresión paralelas que vimos antes, \\[ Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon. \\] Nuevamente, esta es una diferencia de dos parámetros, por lo que ninguna prueba \\(t\\) será útil. anova(mpg_disp_add_cyl, mpg_disp_int_cyl) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + cyl ## Model 2: mpg ~ disp * cyl ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 379 7299.5 ## 2 377 6551.7 2 747.79 21.515 1.419e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Como era de esperar, vemos un valor p muy bajo y, por lo tanto, rechazamos la Hipótesis nula. Preferimos el modelo de interacción sobre el modelo aditivo. Recapitulando un poco: Modelo nulo: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon\\) Número de parámetros: \\(q=4\\) Modelo completo: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\\) Número de parámetros: \\(p=6\\) length(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl)) ## [1] 2 Vemos que hay una diferencia de dos parámetros, que también se muestra en la tabla ANOVA resultante de R. Observe que los siguientes dos valores también aparecen en la tabla ANOVA. nrow(autompg) - length(coef(mpg_disp_int_cyl)) ## [1] 377 nrow(autompg) - length(coef(mpg_disp_add_cyl)) ## [1] 379 11.4 Parametrización Hasta ahora, simplemente hemos dejado que R decida cómo crear las variables ficticias y, por lo tanto, R ha estado decidiendo la parametrización de los modelos. Para ilustrar la capacidad de usar parametrizaciones alternativas, recrearemos los datos, pero creando directamente las variables ficticias nosotros mismos. new_param_data = data.frame( y = autompg$mpg, x = autompg$disp, v1 = 1 * as.numeric(autompg$cyl == 4), v2 = 1 * as.numeric(autompg$cyl == 6), v3 = 1 * as.numeric(autompg$cyl == 8)) head(new_param_data, 20) ## y x v1 v2 v3 ## 1 18 307 0 0 1 ## 2 15 350 0 0 1 ## 3 18 318 0 0 1 ## 4 16 304 0 0 1 ## 5 17 302 0 0 1 ## 6 15 429 0 0 1 ## 7 14 454 0 0 1 ## 8 14 440 0 0 1 ## 9 14 455 0 0 1 ## 10 15 390 0 0 1 ## 11 15 383 0 0 1 ## 12 14 340 0 0 1 ## 13 15 400 0 0 1 ## 14 14 455 0 0 1 ## 15 24 113 1 0 0 ## 16 22 198 0 1 0 ## 17 18 199 0 1 0 ## 18 21 200 0 1 0 ## 19 27 97 1 0 0 ## 20 26 97 1 0 0 Ahora, y es mpg x es disp, el desplazamiento en pulgadas cúbicas, v1,v2 y v3 son variables ficticias como se definieron anteriormente. Primero intentemos ajustar un modelo aditivo usando x así como las tres variables ficticias. lm(y ~ x + v1 + v2 + v3, data = new_param_data) ## ## Call: ## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data) ## ## Coefficients: ## (Intercept) x v1 v2 v3 ## 32.96326 -0.05217 2.03603 -1.59722 NA ¿Que está sucediendo aquí? Observe que R esencialmente ignora v3, pero ¿por qué? Bueno, debido a que R usa un intercepto, no puede usar también v3. Esto es porque \\[ \\boldsymbol{1} = v_1 + v_2 + v_3 \\] lo que significa que \\(\\boldsymbol{1}\\), \\(v_1\\), \\(v_2\\) y \\(v_3\\) son linealmente dependientes. Esto haría que la matriz \\(X^\\top X\\) sea singular, pero necesitamos poder invertirla para resolver las ecuaciones normales y obtener \\(\\hat{\\beta}.\\) Con el intercepto, v1 y v2, R puede hacer los tres interceptos necesarips. Entonces, en este caso, v3 es el nivel de referencia. Si eliminamos el intercepto, podemos obtener directamente los tres interceptos sin un nivel de referencia. lm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data) ## ## Call: ## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data) ## ## Coefficients: ## x v1 v2 v3 ## -0.05217 34.99929 31.36604 32.96326 Aquí, estamos ajustando el modelo. \\[ Y = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta x +\\epsilon. \\] Así tenemos: 4 Cilindros: \\(Y = \\mu_1 + \\beta x + \\epsilon\\) 6 Cilindros: \\(Y = \\mu_2 + \\beta x + \\epsilon\\) 8 Cilindros: \\(Y = \\mu_3 + \\beta x + \\epsilon\\) También podríamos hacer algo similar con el modelo de interacción y darle a cada línea un intercepto y una pendiente, sin la necesidad de un nivel de referencia. lm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data) ## ## Call: ## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data) ## ## Coefficients: ## v1 v2 v3 v1:x v2:x v3:x ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252 \\[ Y = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta_1 x v_1 + \\beta_2 x v_2 + \\beta_3 x v_3 +\\epsilon \\] 4 Cilindros: \\(Y = \\mu_1 + \\beta_1 x + \\epsilon\\) 6 Cilindros: \\(Y = \\mu_2 + \\beta_2 x + \\epsilon\\) 8 Cilindros: \\(Y = \\mu_3 + \\beta_3 x + \\epsilon\\) Usando los datos originales, tenemos (al menos) tres formas equivalentes de especificar el modelo de interacción con R. lm(mpg ~ disp * cyl, data = autompg) ## ## Call: ## lm(formula = mpg ~ disp * cyl, data = autompg) ## ## Coefficients: ## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817 lm(mpg ~ 0 + cyl + disp : cyl, data = autompg) ## ## Call: ## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg) ## ## Coefficients: ## cyl4 cyl6 cyl8 cyl4:disp cyl6:disp cyl8:disp ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252 lm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg) ## ## Call: ## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg) ## ## Coefficients: ## disp cyl4 cyl6 cyl8 disp:cyl6 disp:cyl8 ## -0.13069 43.59052 30.39026 22.73346 0.08299 0.10817 Todos se ajustan al mismo modelo, y lo que es más importante, cada uno utiliza seis parámetros, pero los coeficientes significan cosas ligeramente diferentes en cada uno. Sin embargo, una vez que se interpreten como pendientes e interceptos para las tres líneas, tendrán el mismo resultado. Utilice ?All.equal para aprender sobre la función all.equal() y piense en cómo el siguiente código verifica que los residuos de los dos modelos son iguales. all.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)), fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg))) ## [1] TRUE 11.5 Construcción de modelos más grandes Ahora que hemos visto cómo incorporar predictores categóricos, así como términos de interacción, podemos comenzar a construir modelos mucho más grandes y flexibles que potencialmente pueden ajustarse mejor a los datos. Definamos un modelo grande, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon. \\] Aquí, \\(Y\\) es mpg. \\(x_1\\) es disp. \\(x_2\\) es hp. \\(x_3\\) es domestic, que es una variable ficticia que definimos, donde 1 es un vehículo nacional. Lo primero a tener en cuenta, hemos incluido un nuevo término \\(x_1x_2x_3\\) que es una interacción de tres vías. Los términos de interacción pueden ser cada vez mayores, hasta el número de predictores del modelo. Dado que usamos el término de interacción de tres vías, también usamos todas las interacciones de dos vías posibles, así como cada uno de los términos de primer orden (efecto principal). Este es el concepto de una jerarquía. Siempre que haya un término de orden superior en un modelo, también deben incluirse los términos relacionados de orden inferior. Matemáticamente, su inclusión o exclusión a veces es irrelevante, pero desde el punto de vista de la interpretación, es mejor seguir las reglas de la jerarquía. Hagamos un reordenamiento para obtener un coeficiente delante de \\(x_1\\). \\[ Y = \\beta_0 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_6 x_2 x_3 + (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)x_1 + \\epsilon. \\] Específicamente, el coeficiente delante de \\(x_1\\) es \\[ (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3). \\] Analicemos este coeficiente para ayudarnos a comprender la idea de la flexibilidad de un modelo. Recordemos que, \\(\\beta_1\\) es el coeficiente para un término de primer orden, \\(\\beta_4\\) y \\(\\beta_5\\) son coeficientes para interacciones bidireccionales, \\(\\beta_7\\) es el coeficiente de la interacción de tres vías. Si las interacciones de dos y tres vías no estuvieran en el modelo, todo el coeficiente sería simplemente \\[ \\beta_1. \\] Por lo tanto, sin importar los valores de \\(x_2\\) y \\(x_3\\), \\(\\beta_1\\) determinaría la relación entre \\(x_1\\) (disp) y \\(Y\\) (mpg). Con la adición de las interacciones bidireccionales, ahora el coeficiente sería \\[ (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3). \\] Ahora, cambiar \\(x_1\\) (disp) tiene un efecto diferente en \\(Y\\) (mpg), dependiendo de los valores de \\(x_2\\) y \\(x_3\\). Por último, agregar la interacción de tres vías da el coeficiente completo \\[ (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3) \\] que es aún más flexible. Ahora cambiar \\(x_1\\) (disp) tiene un efecto diferente en \\(Y\\) (mpg), dependiendo de los valores de \\(x_2\\) y \\(x_3\\), pero de una manera más flexible que podemos ver con algunos reordenamientos más. Ahora, el coeficiente delante de \\(x_3\\) depende de \\(x_2\\). \\[ (\\beta_1 + \\beta_4 x_2 + (\\beta_5 + \\beta_7 x_2) x_3) \\] ¡Es tan flexible que se está volviendo difícil de interpretar! Ajustemos este modelo de interacción de tres vías en R. big_model = lm(mpg ~ disp * hp * domestic, data = autompg) summary(big_model) ## ## Call: ## lm(formula = mpg ~ disp * hp * domestic, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.9410 -2.2147 -0.4008 1.9430 18.4094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.065e+01 6.600e+00 9.189 &lt; 2e-16 *** ## disp -1.416e-01 6.344e-02 -2.232 0.0262 * ## hp -3.545e-01 8.123e-02 -4.364 1.65e-05 *** ## domestic -1.257e+01 7.064e+00 -1.780 0.0759 . ## disp:hp 1.369e-03 6.727e-04 2.035 0.0426 * ## disp:domestic 4.933e-02 6.400e-02 0.771 0.4414 ## hp:domestic 1.852e-01 8.709e-02 2.126 0.0342 * ## disp:hp:domestic -9.163e-04 6.768e-04 -1.354 0.1766 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.88 on 375 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7556 ## F-statistic: 169.7 on 7 and 375 DF, p-value: &lt; 2.2e-16 ¿Realmente necesitamos un modelo tan grande? Primero probemos la necesidad del término de interacción de tres vías. Es decir, \\[ H_0: \\beta_7 = 0. \\] Entonces, Modelo completo: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon\\) Modelo nulo: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\) Ajustamos el modelo nulo en R como two_way_int_mod, luego usamos anova() para realizar una prueba \\(F\\) como de costumbre. two_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg) #two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg) anova(two_way_int_mod, big_model) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic ## Model 2: mpg ~ disp * hp * domestic ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 376 5673.2 ## 2 375 5645.6 1 27.599 1.8332 0.1766 Vemos que el valor p es algo grande, por lo que no lo rechazaríamos. Preferimos el modelo nulo más pequeño, menos flexible, sin la interacción de tres vías. Una nota rápida: el modelo completo todavía ajusta mejor. Tenga en cuenta que tiene un RMSE más pequeño que el modelo nulo, lo que significa que el modelo completo comete errores (cuadrados) más pequeños en promedio. mean(resid(big_model) ^ 2) ## [1] 14.74053 mean(resid(two_way_int_mod) ^ 2) ## [1] 14.81259 Sin embargo, no es mucho más pequeño. Incluso podríamos decir que la diferencia es insignificante. Esta es una idea a la que volveremos más adelante con mayor detalle. Ahora que hemos elegido el modelo sin la interacción de tres vías, ¿podemos ir más lejos? ¿Necesitamos las interacciones bidireccionales? Vamos a probar \\[ H_0: \\beta_4 = \\beta_5 = \\beta_6 = 0. \\] Recuerde que ya elegimos \\(\\beta_7=0\\), entonces, Modelo completo: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\) Modelo nulo: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\) Ajustamos el modelo nulo en R como additive_mod, luego usamos anova() para realizar una prueba \\(F\\) como de costumbre. additive_mod = lm(mpg ~ disp + hp + domestic, data = autompg) anova(additive_mod, two_way_int_mod) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + hp + domestic ## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 379 7369.7 ## 2 376 5673.2 3 1696.5 37.478 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Aquí el valor p es pequeño, por lo que rechazamos el nulo y preferimos el modelo completo (alternativo). De los modelos que hemos considerado, nuestra preferencia final es por \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon. \\] "],["análisis-de-varianza.html", "Capítulo 12 Análisis de varianza 12.1 Experimentos 12.2 Prueba t de dos muestras 12.3 ANOVA de una vía 12.4 Pruebas Post Hoc 12.5 ANOVA de dos vías", " Capítulo 12 Análisis de varianza Estado del capítulo: Este capítulo debe considerarse opcional para una primera lectura de este texto. Su inclusión es principalmente en beneficio de algunos cursos que utilizan el texto. .table { width: 70%; margin-left:10%; margin-right:10%; } Para saber qué sucede cuando cambia algo, es necesario cambiarlo.  Box, Hunter, and Hunter, Statistics for Experimenters (1978) Hasta ahora, hemos construido modelos para respuestas numéricas, cuando los predictores son todos numéricos. Tomaremos un pequeño desvío para volver atrás y considerar modelos que solo tienen predictores categóricos. Un predictor categórico es una variable que toma solo un número finito de valores, que no están ordenados. Por ejemplo, una variable que toma como valores posibles rojo,azul, verde es categórica. En el contexto de usar una variable categórica como predictor, colocaría las observaciones en diferentes grupos (categorías). También hemos estado tratando principalmente con datos de observación. Los métodos de esta sección son más útiles en entornos experimentales, pero aún funcionan con datos de observación. (Sin embargo, para determinar la causalidad, necesitamos experimentos). 12.1 Experimentos La mayor diferencia entre un estudio observacional y un experimento es cómo se obtienen los datos del predictor. ¿Tiene el experimentador el control? En un estudio observacional, tanto los datos de respuesta como los predictores se obtienen mediante observación. En un experimento, los datos del predictor son valores determinados por el experimentador. Se ejecuta el experimento y se observa la respuesta. En un experimento, los predictores, que son controlados por el experimentador, se denominan factores. Los posibles valores de estos factores se denominan niveles. Los sujetos se asignan al azar a un nivel de cada uno de los factores. El diseño de experimentos podría ser un curso en sí mismo. El artículo de Wikipedia sobre diseño de experimentos ofrece una buena descripción general. Originalmente, la mayor parte de la metodología fue desarrollada para aplicaciones agrícolas por R. A. Fisher, pero hoy todavía están en uso, ahora en una amplia variedad de áreas de aplicación. En particular, estos métodos han experimentado un resurgimiento como parte de las pruebas A/B. 12.2 Prueba t de dos muestras El ejemplo más simple de un diseño experimental es la configuración de una prueba \\(t\\) de dos muestras. Hay una variable de factor único con dos niveles que divide a los sujetos en dos grupos. A menudo, un nivel se considera el control, mientras que el otro es el tratamiento. Los sujetos se asignan aleatoriamente a uno de los dos grupos. Después de ser asignado a un grupo, cada sujeto tiene una cantidad medida, que es la variable de respuesta. Matemáticamente, consideramos el modelo \\[ y_{ij} \\sim N(\\mu_i, \\sigma^2) \\] donde \\(i= 1,2\\) para los dos grupos y \\(j=1,2,\\ldots n_i\\). Aquí \\(n_i\\) es el número de sujetos en el grupo \\(i\\). Entonces \\(y_{13}\\) sería la medida para el tercer miembro del primer grupo. Por tanto, las mediciones de los sujetos del grupo \\(1\\) siguen una distribución normal con una media de \\(\\mu_1\\). \\[ y_{1j} \\sim N(\\mu_1, \\sigma^2) \\] Luego, las mediciones de los sujetos en el grupo \\(2\\) siguen una distribución normal con media \\(\\mu_2\\). \\[ y_{2j} \\sim N(\\mu_2, \\sigma^2) \\] Este modelo hace una serie de suposiciones. Específicamente, Las observaciones siguen una distribución normal. La media de cada grupo es diferente. Varianza igual para cada grupo. Independencia. Lo cual es creíble si los grupos fueran asignados al azar. Más adelante, investigaremos los supuestos de varianza normal e igual. Por ahora, continuaremos asumiendo que son razonables. La pregunta natural que se debe hacer: ¿hay alguna diferencia entre los dos grupos? La pregunta específica que responderemos: ¿Son diferentes las medias de los dos grupos? Matemáticamente, es \\[ H_0: \\mu_1 = \\mu_2 \\quad \\text{vs} \\quad H_1: \\mu_1 \\neq \\mu_2 \\] Para el modelo establecido y asumiendo que la hipótesis nula es verdadera, el estadístico de prueba \\(t\\) seguiría una distribución \\(t\\) con \\(n_1+n_2-2\\) grados de libertad. Como ejemplo, suponga que estamos interesados en el efecto de la melatonina sobre la duración del sueño. Un investigador obtiene una muestra aleatoria de 20 varones adultos. De estos sujetos, 10 se eligen al azar para el grupo de control, que recibirá un placebo. Los 10 restantes recibirán 5 mg de melatonina antes de acostarse. Luego se mide la duración del sueño en horas de cada sujeto. El investigador elige un nivel de significancia de \\(\\alpha=0.10\\). ¿La melatonina afectó la duración del sueño? melatonin ## sleep group ## 1 8.145150 control ## 2 7.522362 tratamiento ## 3 6.935754 control ## 4 8.959435 tratamiento ## 5 6.985122 control ## 6 8.072651 tratamiento ## 7 8.313826 control ## 8 8.086409 tratamiento ## 9 8.922108 control ## 10 8.124743 tratamiento ## 11 8.065844 control ## 12 10.943974 tratamiento ## 13 4.833367 control ## 14 7.865453 tratamiento ## 15 6.340014 control ## 16 8.963140 tratamiento ## 17 6.158896 control ## 18 5.012253 tratamiento ## 19 3.571440 control ## 20 9.784136 tratamiento Aquí, nos gustaría probar, \\[ H_0: \\mu_C = \\mu_T \\quad \\text{vs} \\quad H_1: \\mu_C \\neq \\mu_T \\] Para hacerlo en R, usamos la función t.test(), con el argumento var.equal establecido en TRUE. t.test(sleep ~ group, data = melatonin, var.equal = TRUE) ## ## Two Sample t-test ## ## data: sleep by group ## t = -2.0854, df = 18, p-value = 0.05154 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.02378261 0.01117547 ## sample estimates: ## mean in group control mean in group tratamiento ## 6.827152 8.333456 A un nivel de significancia de \\(\\alpha=0.10\\), rechazamos la hipótesis nula. Parece que la melatonina tuvo un efecto estadísticamente significativo. Tenga en cuenta que la significación estadística no siempre es la misma que la significación científica o práctica. Para determinar la importancia práctica, necesitamos investigar el tamaño del efecto en el contexto de la situación. Aquí, el tamaño del efecto es la diferencia de las medias muestrales. t.test(sleep ~ group, data = melatonin, var.equal = TRUE)$estimate ## mean in group control mean in group tratamiento ## 6.827152 8.333456 Aquí vemos que los sujetos del grupo de melatonina duermen un promedio de aproximadamente 1,5 horas más que el grupo de control. ¡Una hora y media de sueño es ciertamente importante! Con un tamaño de muestra lo suficientemente grande, podríamos hacer un tamaño del efecto de, digamos, cuatro minutos estadísticamente significativo. ¿Vale la pena tomar una pastilla todas las noches para dormir cuatro minutos más? (Probablemente no.) boxplot(sleep ~ group, data = melatonin, col = 5:6) 12.3 ANOVA de una vía ¿Y si hay más de dos grupos? Considere el modelo \\[ y_{ij} = \\mu + \\alpha_i + e_{ij}. \\] donde \\[ \\sum \\alpha_i = 0 \\] y \\[ e_{ij} \\sim N(0,\\sigma^{2}). \\] Aquí, \\(i = 1, 2, \\ldots g\\) donde \\(g\\) es el número de grupos. \\(j = 1, 2, \\ldots n_i\\) donde \\(n_i\\) es el número de observaciones en el grupo \\(i\\). Entonces el tamaño total de la muestra es \\[ N = \\sum_{i = 1}^{g} n_i \\] Las observaciones del grupo \\(i\\) siguen una distribución normal \\[ y_{ij} \\sim N(\\mu_i,\\sigma^{2}) \\] donde la media de cada grupo viene dada por \\[ \\mu_i = \\mu + \\alpha_i. \\] Aquí \\(\\alpha_i\\) mide el efecto del grupo \\(i\\). Es la diferencia entre la media general y la media del grupo \\(i\\). Esencialmente, las suposiciones son las mismas que las del caso de dos muestras, sin embargo ahora, simplemente tenemos más grupos. Al igual que en el caso de dos muestras, nuevamente nos gustaría probar si las medias de los grupos son iguales. \\[ H_0: \\mu_1 = \\mu_2 = \\ldots \\mu_g \\quad \\text{vs} \\quad H_1: \\text{ No todos los } \\mu_i \\text{ son iguales.} \\] Observe que la alternativa simplemente indica que algunas de las medias no son iguales, no específicamente cuáles de las medias no son iguales. Más sobre eso después. Alternativamente, podríamos escribir \\[ H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_g = 0 \\quad \\text{vs} \\quad H_1: \\text{ No todos los } \\alpha_i \\text{ son } 0. \\] Esta prueba se llama Análisis de varianza. El análisis de varianza (ANOVA) compara la variación debida a fuentes específicas (entre grupos) con la variación entre individuos que deberían ser similares (dentro de los grupos). En particular, ANOVA prueba si varias poblaciones tienen la misma media comparando qué tan alejadas están las medias de la muestra con cuánta variación hay dentro de las muestras. Usamos la variabilidad de medias para probar la igualdad de medias, de ahí el uso de varianza en el nombre para una prueba sobre medias. Dejaremos fuera la mayoría de los detalles sobre cómo se realiza la estimación, pero veremos más adelante, que se realiza mediante mínimos cuadrados. Usaremos R para obtener estas estimaciones, pero en realidad son bastante simples. Basta pensar en las medias muestrales de los grupos. \\(\\bar{y}_i\\) es la media muestral del grupo \\(i\\). \\(\\bar{y}\\) es la media muestral general. \\(s_{i}^{2}\\) es la varianza muestral del grupo \\(i\\). Luego descompondremos la varianza, como hemos visto antes en la regresión. La variación total mide cuánto varían las observaciones sobre la media general de la muestra, ignorando los grupos. \\[ SST = \\sum_{i = 1}^{g} \\sum_{j = 1}^{n_i} (y_{ij} - \\bar{y})^2 \\] La variación entre grupos analiza qué tan lejos están las medias de la muestra individual de la media general de la muestra. \\[ SSB = \\sum_{i = 1}^{g} \\sum_{j = 1}^{n_i} (\\bar{y}_i - \\bar{y})^2 = \\sum_{i = 1}^{g} n_i (\\bar{y}_i - \\bar{y})^2 \\] Por último, la variación dentro de grupo mide qué tan lejos están las observaciones de la media muestral de su grupo. \\[ SSW = \\sum_{i = 1}^{g} \\sum_{j = 1}^{n_i} (y_{ij} - \\bar{y}_i)^2 = \\sum_{i = 1}^{g} (n_i - 1) s_{i}^{2} \\] Esto también podría pensarse como la suma de los cuadrados del error, donde \\(y_{ij}\\) es una observación y \\(\\bar{y}_i\\) es su valor ajustado (predicho) del modelo. Para desarrollar el estadístico de prueba para ANOVA, colocamos esta información en una tabla ANOVA. Fuente Suma de cuadrados Grados de libertad Cuadrado medio \\(F\\) Entre SSB \\(g - 1\\) SSB / DFB MSB / MSW Dentro SSW \\(N - g\\) SSW / DFW Total SST \\(N - 1\\) Rechazamos al hipótesis nula (medias iguales) cuando la estadística \\(F\\) es grande. Esto ocurre cuando la variación entre grupos es grande en comparación con la variación dentro de los grupos. Bajo la hipótesis nula, la distribución del estadístico de prueba es \\(F\\) con grados de libertad \\(g-1\\) y \\(N-g\\). Observemos cómo se ve esto en algunas situaciones. En cada uno de los siguientes ejemplos, consideraremos muestrear 20 observaciones (\\(n_i=20\\)) de tres poblaciones (grupos). Primero, considere \\(\\mu_A = -5, \\mu_B = 0, \\mu_C = 5\\) con \\(\\sigma = 1\\). El panel de la izquierda muestra las tres distribuciones normales de las que estamos muestreando. Las marcas a lo largo del eje \\(x\\) muestran las observaciones muestreadas al azar. El panel derecho vuelve a mostrar solo los valores muestreados en un gráico de caja. Tenga en cuenta que la línea media de los cuadros suele ser la mediana de la muestra. Estos diagramas de caja se han modificado para utilizar la media muestral. Aquí, las medias de la muestra varían mucho alrededor de la media de la muestra general, que es la línea gris sólida en el panel derecho. Dentro de los grupos existe variabilidad, pero aún es obvio que las medias muestrales son muy diferentes. Como resultado, obtenemos un estadístico de prueba grande, por lo tanto, un valor p pequeño. \\(F = 374.4469511\\) \\(\\text{p-value} = 1.6349862\\times 10^{-33}\\) Ahora considere \\(\\mu_A = 0, \\mu_B = 0, \\mu_C = 0\\) con \\(\\sigma = 1\\). Es decir, medias iguales para los grupos. Aquí, las medias de la muestra varían solo un poco alrededor de la media de la muestra general. Dentro de los grupos hay variabilidad, esta vez mucho mayor que la variabilidad de las medias muestrales. Como resultado, obtenemos un estadístico de prueba pequeño, por lo tanto, un valor p grande. \\(F = 2.667892\\) \\(\\text{p-value} = 0.0780579\\) Los siguientes dos ejemplos muestran diferentes medias, con diferentes niveles de ruido. Observe cómo estos afectan el estadístico de prueba y el valor p. \\(\\mu_A = -1, \\mu_B = 0, \\mu_C = 1, \\sigma = 1\\) \\(F = 16.4879492\\) \\(\\text{p-value} = 2.2378806\\times 10^{-6}\\) Arriba, no hay una separación obvia entre los grupos como en el primer ejemplo, pero aún es obvio que las medias son diferentes. Abajo, hay más ruido. Visualmente es algo difícil de decir, pero la prueba aún sugiere una diferencia de medias. (A un \\(\\alpha\\) de 0.05). \\(\\mu_A = -1, \\mu_B = 0, \\mu_C = 1, \\sigma = 2\\) \\(n_i = 20\\) para cada grupo. \\(F = 4.6256472\\) \\(\\text{p-value} = 0.0137529\\) Consideremos un ejemplo con datos reales. Usaremos el conjunto de datos coagulation del paquete faraway. Aquí se administraron cuatro dietas diferentes (A, B, C, D) a una muestra aleatoria de 24 animales. Los sujetos fueron asignados aleatoriamente a una de las cuatro dietas. Para cada uno, se midió el tiempo de coagulación de la sangre en segundos. Aquí nos gustaría probar \\[ H_0: \\mu_A = \\mu_B = \\mu_C = \\mu_D \\] donde, por ejemplo, \\(\\mu_A\\) es el tiempo medio de coagulación de la sangre para un animal que consumió la dieta A. library(faraway) names(coagulation) ## [1] &quot;coag&quot; &quot;diet&quot; plot(coag ~ diet, data = coagulation, col = 2:5) Primero cargamos los datos y creamos el gráfico de caja. La geáfica por sí sola sugiere una diferencia de medios. La función aov() se utiliza para obtener las sumas de cuadrados relevantes. Usando la función summary() en la salida de aov() se crea la tabla ANOVA deseada. (Sin la fila innecesaria para el total). coag_aov = aov(coag ~ diet, data = coagulation) coag_aov ## Call: ## aov(formula = coag ~ diet, data = coagulation) ## ## Terms: ## diet Residuals ## Sum of Squares 228 112 ## Deg. of Freedom 3 20 ## ## Residual standard error: 2.366432 ## Estimated effects may be unbalanced summary(coag_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 3 228 76.0 13.57 4.66e-05 *** ## Residuals 20 112 5.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Si lleváramos a cabo este experimento, habríamos especificado previamente un nivel de significancia. Sin embargo, observe que el valor p de esta prueba es increíblemente bajo, por lo que al usar cualquier nivel de significancia razonable rechazaríamos la hipótesis nula. Por lo tanto, creemos que las dietas tuvieron un efecto sobre el tiempo de coagulación de la sangre. diets = data.frame(diet = unique(coagulation$diet)) data.frame(diets, coag = predict(coag_aov, diets)) ## diet coag ## 1 A 61 ## 2 B 66 ## 3 C 68 ## 4 D 61 Aquí, hemos creado un marco de datos con una fila para cada dieta. Al predecir en este marco de datos, obtenemos las medias muestrales de cada dieta (grupo). 12.3.1 Variables factor Al realizar ANOVA en R, asegúrese de que la variable de agrupación sea una variable factor. Si no es así, es posible que el resultado no sea ANOVA, sino una regresión lineal con una variable predictora numérica. set.seed(42) response = rnorm(15) group = c(rep(1, 5), rep(2, 5), rep(3, 5)) bad = data.frame(response, group) summary(aov(response ~ group, data = bad)) # DF equivocado! ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 1 0.017 0.0173 0.015 0.903 ## Residuals 13 14.698 1.1306 good = data.frame(response, group = as.factor(group)) summary(aov(response ~ group, data = good)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 0.232 0.1158 0.096 0.909 ## Residuals 12 14.484 1.2070 is.factor(bad$group) # 1, 2 y 3 son números. ## [1] FALSE is.factor(good$group) # 1, 2 y 3 son etiquetas. ## [1] TRUE 12.3.2 Algo de simulación Verificamos la distribución del estadístico de prueba bajo la hipótesis nula. Simulamos a partir de un modelo nulo (varianza igual) para obtener una distribución empírica del estadístico \\(F\\). Agregamos la curva para la distribución esperada. library(broom) sim_anova = function(n = 10, mu_a = 0, mu_b = 0, mu_c = 0, mu_d = 0, sigma = 1, stat = TRUE) { # crear datos a partir del modelo ANOVA de una vía con cuatro grupos de igual tamaño # respuesta simulada de una normal con media de grupo, varianza compartida # la variable de grupo indica el grupo A, B, C o D sim_data = data.frame( response = c(rnorm(n = n, mean = mu_a, sd = sigma), rnorm(n = n, mean = mu_b, sd = sigma), rnorm(n = n, mean = mu_c, sd = sigma), rnorm(n = n, mean = mu_d, sd = sigma)), group = c(rep(&quot;A&quot;, times = n), rep(&quot;B&quot;, times = n), rep(&quot;C&quot;, times = n), rep(&quot;D&quot;, times = n)) ) # obtener el estadístico F y el valor p para probar la diferencia de medias # use lm en lugar de aov para un mejor formato de resultados aov_results = lm(response ~ group, data = sim_data) f_stat = glance(aov_results)$statistic p_val = glance(aov_results)$p.value # devuelve f_stat si stat=TRUE, de lo contrario, valor p ifelse(stat, f_stat, p_val) } f_stats = replicate(n = 5000, sim_anova(stat = TRUE)) hist(f_stats, breaks = 100, prob = TRUE, border = &quot;dodgerblue&quot;, main = &quot;Distribución empírica de F&quot;) curve(df(x, df1 = 4 - 1, df2 = 40 - 4), col = &quot;darkorange&quot;, add = TRUE, lwd = 2) 12.3.3 Potencia Ahora que estamos realizando experimentos, obtener más datos significa encontrar más sujetos de prueba, realizar más pruebas de laboratorio, etc. En otras palabras, costará más tiempo y dinero. Nos gustaría diseñar nuestro experimento para que tengamos una buena posibilidad de detectar un tamaño de efecto interesante, sin gastar demasiado dinero. No tiene sentido ejecutar un experimento si hay muy pocas posibilidades de que tenga un resultado significativo que le interese. (Recuerde, no todos los resultados estadísticamente significativos tienen valor práctico). Nos gustaría que la prueba ANOVA tuviera una potencia elevada para una hipótesis alternativa con un tamaño de efecto mínimo deseado. \\[ \\text{Potencia } = P(\\text{Rechazar } H_0 \\mid H_0 \\text{ Falso}) \\] Es decir, para una verdadera diferencia de medias que consideremos interesante, queremos que la prueba se rechace con alta probabilidad. Varias cosas pueden afectar la potencia de una prueba: Tamaño del efecto. Es más fácil detectar efectos mayores. Nivel de ruido \\(\\sigma\\). Cuanto menos ruido, más fácil es detectar la señal (efecto). No tenemos mucha capacidad para controlar esto, excepto quizás para medir con mayor precisión. Nivel de significancia \\(\\alpha\\). Un nivel de significancia más bajo hace que el rechazo sea más difícil. (Pero también permite menos falsos positivos). Tamaño de la muestra. Las muestras grandes significan efectos más fáciles de detectar. Diseño balanceado. Un número igual de observaciones por grupo conduce a una mayor potencia. Las siguientes simulaciones analizan el efecto del nivel de significancia, el tamaño del efecto y el nivel de ruido en la potencia de una prueba ANOVA \\(F\\). El ejercicio examinará el tamaño y el equilibrio de la muestra. p_vals = replicate(n = 1000, sim_anova(mu_a = -1, mu_b = 0, mu_c = 0, mu_d = 1, sigma = 1.5, stat = FALSE)) mean(p_vals &lt; 0.05) ## [1] 0.663 mean(p_vals &lt; 0.01) ## [1] 0.39 p_vals = replicate(n = 1000, sim_anova(mu_a = -1, mu_b = 0, mu_c = 0, mu_d = 1, sigma = 2.0, stat = FALSE)) mean(p_vals &lt; 0.05) ## [1] 0.408 mean(p_vals &lt; 0.01) ## [1] 0.179 p_vals = replicate(n = 1000, sim_anova(mu_a = -2, mu_b = 0, mu_c = 0, mu_d = 2, sigma = 2.0, stat = FALSE)) mean(p_vals &lt; 0.05) ## [1] 0.964 mean(p_vals &lt; 0.01) ## [1] 0.855 12.4 Pruebas Post Hoc Suponga que rechazamos la hipótesis nula de la prueba ANOVA para medias iguales. Eso nos dice que las medias son diferentes. ¿Pero qué significa? ¿Todas ellas? ¿Algunas? La estrategia obvia es probar todas las posibles comparaciones de dos medias. Podemos hacer esto fácilmente en R. with(coagulation, pairwise.t.test(coag, diet, p.adj = &quot;none&quot;)) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: coag and diet ## ## A B C ## B 0.00380 - - ## C 0.00018 0.15878 - ## D 1.00000 0.00086 2.3e-05 ## ## P value adjustment method: none # pairwise.t.test(coagulation$coag, coagulation$diet, p.adj = &quot;none&quot;) Observe que la función pairwise.t.test() no tiene un argumento data. Para evitar usar attach() o el operador $, introducimos la función with(). La línea comentada realizaría la misma operación. También tenga en cuenta que estamos usando el argumento p.adj = \"none\". ¿Qué es ésto? Un ajuste (en este caso no un ajuste) al valor p de cada prueba. ¿Por qué tendríamos que hacer esto? El ajuste es un intento de corregir el problema de pruebas múltiples. (Consulte también: XKCD relevante.) Imagine que sabe de antemano que va a realizar 100 pruebas \\(t\\). Suponga que desea hacer esto con una tasa de falsos positivos de \\(\\alpha=0.05\\). Si usamos este nivel de significancia para cada una de las 100 pruebas, entonces esperamos 5 falsos positivos. Eso significa que, con 100 pruebas, es casi seguro que tendremos al menos un error. Lo que realmente nos gustaría es que la tasa de error familiar sea 0.05. Si consideramos que las 100 pruebas son un solo experimento, el FWER es la tasa de uno o más falsos positivos en el experimento completo (100 pruebas). Considérelo una tasa de error para un procedimiento completo, en lugar de una sola prueba. Con esto en mente, uno de los ajustes más simples que podemos hacer es aumentar los valores p para cada prueba, dependiendo del número de pruebas. En particular, la corrección de Bonferroni simplemente se multiplica por el número de pruebas. \\[ \\text{valor p-bonf} = \\min(1, n_{tests} \\cdot \\text{valor p}) \\] with(coagulation, pairwise.t.test(coag, diet, p.adj = &quot;bonferroni&quot;)) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: coag and diet ## ## A B C ## B 0.02282 - - ## C 0.00108 0.95266 - ## D 1.00000 0.00518 0.00014 ## ## P value adjustment method: bonferroni Vemos que estos valores p son mucho más altos que los valores p no ajustados, por lo tanto, es menos probable que rechacemos cada prueba. Como resultado, el FWER es 0.05, en lugar de una tasa de error de 0.05 para cada prueba. Podemos simular el escenario de 100 pruebas para ilustrar este punto. get_p_val = function() { # crear datos para dos grupos, media igual y = rnorm(20, mean = 0, sd = 1) g = c(rep(&quot;A&quot;, 10), rep(&quot;B&quot;, 10)) # Valor p de la prueba t cuando la nula es verdadera glance(t.test(y ~ g, var.equal = TRUE))$p.value } set.seed(1337) # FWER con 100 pruebas # tasa deseada = 0.05 # sin ajuste mean(replicate(1000, any(replicate(100, get_p_val()) &lt; 0.05))) ## [1] 0.994 # FWER con 100 pruebas # tasa deseada = 0.05 # ajuste bonferroni mean(replicate(1000, any(p.adjust(replicate(100, get_p_val()), &quot;bonferroni&quot;) &lt; 0.05))) ## [1] 0.058 Para el caso específico de probar todas las diferencias medias de dos vías después de una prueba ANOVA, existen varios métodos potenciales para hacer un ajuste de este tipo. Los pros y los contras de los métodos potenciales están más allá del alcance de este curso. Elegimos un método por su facilidad de uso. La diferencia de significancia de Tukey se puede aplicar directamente a un objeto que fue creado usando aov(). Ajustará los valores p de las comparaciones por pares de las medias para controlar el FWER, en este caso, para 0.05. Observe que también proporciona intervalos de confianza para la diferencia de las medias. TukeyHSD(coag_aov, conf.level = 0.95) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = coag ~ diet, data = coagulation) ## ## $diet ## diff lwr upr p adj ## B-A 5 0.7245544 9.275446 0.0183283 ## C-A 7 2.7245544 11.275446 0.0009577 ## D-A 0 -4.0560438 4.056044 1.0000000 ## C-B 2 -1.8240748 5.824075 0.4766005 ## D-B -5 -8.5770944 -1.422906 0.0044114 ## D-C -7 -10.5770944 -3.422906 0.0001268 Según estos resultados, no vemos ninguna diferencia entre A y D, así como entre B y C. Todas las demás comparaciones por pares son significativas. Si vuelve a la gráfica de caja original, estos resultados no deberían sorprender. Además, podemos producir fácilmente un gráfico de estos intervalos de confianza. plot(TukeyHSD(coag_aov, conf.level = 0.95)) El creador de este método, John Tukey, es una figura importante en la historia de la ciencia de datos. Básicamente predijo el auge de la ciencia de datos hace más de 50 años. Para ver algunas reflexiones retrospectivas sobre esos 50 años, consulte este artículo de David Donoho. 12.5 ANOVA de dos vías ¿Qué pasa si hay más de una variable factor? ¿Por qué tenemos que limitarnos a experimentar con un solo factor? ¡Nosotros no! Considere el modelo \\[ y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}. \\] donde \\(\\epsilon_{ijk}\\) son variables aleatorias \\(N(0, \\sigma^2)\\). Agregamos restricciones \\[ \\sum \\alpha_i = 0 \\quad \\quad \\sum \\beta_j = 0. \\] y \\[ (\\alpha \\beta)_{1j} + (\\alpha \\beta)_{2j} + (\\alpha \\beta)_{3j} = 0 \\\\ (\\alpha \\beta)_{i1} + (\\alpha \\beta)_{i2} + (\\alpha \\beta)_{i3} + (\\alpha \\beta)_{i4} = 0 \\] para cualquier \\(i\\) o \\(j\\). Aquí, \\(i=1,2, \\ldots I\\) donde \\(I\\) es el número de niveles del factor \\(A\\). \\(j=1,2, \\ldots J\\) donde \\(J\\) es el número de niveles del factor \\(B\\). \\(k=1,2, \\ldots K\\) donde \\(K\\) es el número de réplicas por grupo. Aquí, podemos pensar en un grupo como una combinación de un nivel de cada uno de los factores. Entonces, por ejemplo, un grupo recibirá el nivel \\(2\\) del factor \\(A\\) y el nivel \\(3\\) del factor \\(B\\). El número de réplicas es el número de sujetos en cada grupo. Por ejemplo, \\(y_{135}\\) sería la medida para el quinto miembro (réplica) del grupo para el nivel \\(1\\) del factor \\(A\\) y el nivel \\(3\\) del factor \\(B\\). A esta configuración la llamamos \\(I \\times J\\) diseño factorial con réplicas de \\(K\\). (Nuestra notación actual solo permite réplicas iguales en cada grupo. No es difícil permitir réplicas diferentes para grupos diferentes, pero procederemos a usar réplicas iguales por grupo, lo cual, es deseable). \\(\\alpha_i\\) mide el efecto del nivel \\(i\\) del factor \\(A\\). A estos los llamamos los efectos principales del factor \\(A\\). \\(\\beta_j\\) mide el efecto del nivel \\(j\\) del factor \\(B\\). A estos los llamamos los efectos principales del factor \\(B\\). \\((\\alpha\\beta)_{ij}\\) es un solo parámetro. Usamos \\(\\alpha\\beta\\) para notar que este parámetro mide la interacción entre los dos efectos principales. Bajo esta configuración, hay varios modelos que podemos comparar. Considere un diseño factorial de \\(2 \\times 2\\). Las siguientes tablas muestran las medias para cada uno de los posibles grupos de cada modelo. Modelo de Interacción: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\\) Factor B, Nivel 1 Factor B, Nivel 2 Factor A, Nivel 1 \\(\\mu + \\alpha_1 + \\beta_1 + (\\alpha\\beta)_{11}\\) \\(\\mu + \\alpha_1 + \\beta_2 + (\\alpha\\beta)_{12}\\) Factor A, Nivel 2 \\(\\mu + \\alpha_2 + \\beta_1 + (\\alpha\\beta)_{21}\\) \\(\\mu + \\alpha_2 + \\beta_2 + (\\alpha\\beta)_{22}\\) Modelo Aditivo: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk}\\) Factor B, Nivel 1 Factor B, Nivel 2 Factor A, Nivel 1 \\(\\mu + \\alpha_1 + \\beta_1\\) \\(\\mu + \\alpha_1 + \\beta_2\\) Factor A, Nivel 2 \\(\\mu + \\alpha_2 + \\beta_1\\) \\(\\mu + \\alpha_2 + \\beta_2\\) Factor B Modelo (una vía): \\(y_{ijk} = \\mu + \\beta_j + \\epsilon_{ijk}\\) Factor B, Nivel 1 Factor B, Nivel 2 Factor A, Nivel 1 \\(\\mu + \\beta_1\\) \\(\\mu + \\beta_2\\) Factor A, Nivel 2 \\(\\mu + \\beta_1\\) \\(\\mu + \\beta_2\\) Factor A Modelo (una vía): \\(y_{ijk} = \\mu + \\alpha_i + \\epsilon_{ijk}\\) Factor B, Nivel 1 Factor B, Nivel 2 Factor A, Nivel 1 \\(\\mu + \\alpha_1\\) \\(\\mu + \\alpha_1\\) Factor A, Nivel 2 \\(\\mu + \\alpha_2\\) \\(\\mu + \\alpha_2\\) Modelo Nulo : \\(y_{ijk} = \\mu + \\epsilon_{ijk}\\) Factor B, Nivel 1 Factor B, Nivel 2 Factor A, Nivel 1 \\(\\mu\\) \\(\\mu\\) Factor A, Nivel 2 \\(\\mu\\) \\(\\mu\\) La pregunta entonces es ¿cuál de estos modelos deberíamos usar si tenemos dos factores? La pregunta más importante a considerar es si debemos modelar la interacción. ¿Es el efecto del factor A el mismo para todos los niveles del factor B? en el modelo aditivo, sí. En el modelo de interacción, no. Ambos modelos utilizarían una media diferente para cada grupo, pero de forma muy específica en ambos casos. Analicemos estas comparaciones observando algunos ejemplos. Primero veremos los datos de rats del paquete faraway. Aquí hay dos factores: poison y treat. Usamos la función levels() para extraer los niveles de una variable factor. levels(rats$poison) ## [1] &quot;I&quot; &quot;II&quot; &quot;III&quot; levels(rats$treat) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; Aquí, se asignó aleatoriamente a 48 ratas tanto a uno de los tres venenos como a uno de los cuatro tratamientos posibles. Luego, los experimentadores miden su tiempo de supervivencia en decenas de horas. Un total de 12 grupos, cada uno con 4 repeticiones. Antes de ejecutar cualquier prueba, primero debemos mirar los datos. Crearemos ** gráficos de interacción **, que nos ayudarán a visualizar el efecto de un factor a medida que avanzamos por los niveles de otro factor. par(mfrow = c(1, 2)) with(rats, interaction.plot(poison, treat, time, lwd = 2, col = 1:4)) with(rats, interaction.plot(treat, poison, time, lwd = 2, col = 1:3)) No hay interacción, por lo tanto, en un modelo aditivo, esperaríamos ver líneas paralelas. Eso significaría que cuando cambiamos el nivel de un factor, puede haber un efecto en la respuesta. Sin embargo, la diferencia entre los niveles del otro factor debería seguir siendo la misma. La indicación obvia de interacción serían las líneas que se cruzan mientras se dirigen en diferentes direcciones. Aquí no vemos eso, pero las líneas no son estrictamente paralelas y hay algo de superposición en el panel derecho. Sin embargo, ¿este efecto de interacción es significativo? Ajustemos cada uno de los posibles modelos, luego observemos sus estimaciones para cada una de las medias del grupo. rats_int = aov(time ~ poison * treat, data = rats) # modelo de interacción rats_add = aov(time ~ poison + treat, data = rats) # modelo aditivo rats_pois = aov(time ~ poison , data = rats) # modelo de factor único rats_treat = aov(time ~ treat, data = rats) # modelo de factor único rats_null = aov(time ~ 1, data = rats) # modelo nulo Para obtener las estimaciones, crearemos una tabla en la que predeciremos. rats_table = expand.grid(poison = unique(rats$poison), treat = unique(rats$treat)) rats_table ## poison treat ## 1 I A ## 2 II A ## 3 III A ## 4 I B ## 5 II B ## 6 III B ## 7 I C ## 8 II C ## 9 III C ## 10 I D ## 11 II D ## 12 III D matrix(paste0(rats_table$poison, &quot;-&quot;, rats_table$treat) , 4, 3, byrow = TRUE) ## [,1] [,2] [,3] ## [1,] &quot;I-A&quot; &quot;II-A&quot; &quot;III-A&quot; ## [2,] &quot;I-B&quot; &quot;II-B&quot; &quot;III-B&quot; ## [3,] &quot;I-C&quot; &quot;II-C&quot; &quot;III-C&quot; ## [4,] &quot;I-D&quot; &quot;II-D&quot; &quot;III-D&quot; Como repetiremos varias veces, escribimos una función para realizar la predicción. Se realizan algunas tareas de limpieza para mantener las estimaciones en orden y proporcionar nombres de filas y columnas. Arriba, mostramos dónde se colocarán cada una de las estimaciones en la matriz resultante. get_est_means = function(model, table) { mat = matrix(predict(model, table), nrow = 4, ncol = 3, byrow = TRUE) colnames(mat) = c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;) rownames(mat) = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) mat } Primero, obtenemos las estimaciones del modelo de interacción. Tenga en cuenta que cada celda tiene un valor diferente. knitr::kable(get_est_means(model = rats_int, table = rats_table)) I II III A 0.4125 0.3200 0.210 B 0.8800 0.8150 0.335 C 0.5675 0.3750 0.235 D 0.6100 0.6675 0.325 A continuación, obtenemos las estimaciones del modelo aditivo. Nuevamente, cada celda tiene un valor diferente. También vemos que estas estimaciones se acercan un poco a las del modelo de interacción. knitr::kable(get_est_means(model = rats_add, table = rats_table)) I II III A 0.4522917 0.3791667 0.1110417 B 0.8147917 0.7416667 0.4735417 C 0.5306250 0.4575000 0.1893750 D 0.6722917 0.5991667 0.3310417 Para comprender la diferencia, consideremos el efecto de los tratamientos. additive_means = get_est_means(model = rats_add, table = rats_table) additive_means[&quot;A&quot;,] - additive_means[&quot;B&quot;,] ## I II III ## -0.3625 -0.3625 -0.3625 interaction_means = get_est_means(model = rats_int, table = rats_table) interaction_means[&quot;A&quot;,] - interaction_means[&quot;B&quot;,] ## I II III ## -0.4675 -0.4950 -0.1250 Esta es la diferencia clave entre los modelos de interacción y aditivos. La diferencia entre el efecto de los tratamientos A y B es igual para cada veneno en el modelo aditivo. Ahora, son diferentes en el modelo de interacción. Los tres modelos restantes son mucho más simples, ya sea que solo tienen efectos de fila o de columna. O sin efectos en el caso del modelo nulo. knitr::kable(get_est_means(model = rats_pois, table = rats_table)) I II III A 0.6175 0.544375 0.27625 B 0.6175 0.544375 0.27625 C 0.6175 0.544375 0.27625 D 0.6175 0.544375 0.27625 knitr::kable(get_est_means(model = rats_treat, table = rats_table)) I II III A 0.3141667 0.3141667 0.3141667 B 0.6766667 0.6766667 0.6766667 C 0.3925000 0.3925000 0.3925000 D 0.5341667 0.5341667 0.5341667 knitr::kable(get_est_means(model = rats_null, table = rats_table)) I II III A 0.479375 0.479375 0.479375 B 0.479375 0.479375 0.479375 C 0.479375 0.479375 0.479375 D 0.479375 0.479375 0.479375 Para realizar las pruebas necesarias, necesitaremos crear otra tabla ANOVA. (Omitiremos los detalles de los cálculos de sumas de cuadrados y simplemente dejaremos que R se encargue de ellos). Fuente Suma de cuadrados Grados de libertad Cuadrado medio \\(F\\) Factor A SSA \\(I -1\\) SSA / DFA MSA / MSE Factor B SSB \\(J -1\\) SSB / DFB MSB / MSE Interacción AB SSAB \\((I -1)(J -1)\\) SSAB / DFAB MSAB / MSE Error SSE \\(IJ(K - 1)\\) SSE / DFE Total SST \\(IJK - 1\\) La fila para las pruebas de Interacción AB: \\[ H_0: \\text{ Todos }(\\alpha\\beta)_{ij} = 0. \\quad \\text{vs} \\quad H_1: \\text{ No todos } (\\alpha\\beta)_{ij} \\text{ son } 0. \\] Modelo nulo: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk}.\\) (Modelo aditivo.) Modelo alternativo: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}.\\) (Modelo de interacción). Rechazamos la hipótesis nula cuando la estadística \\(F\\) es grande. Bajo la hipótesis nula, la distribución del estadístico de prueba es \\(F\\) con grados de libertad \\((I-1)(J-1)\\) y \\(IJ(K-1)\\). La fila para las pruebas de Factor B: \\[ H_0: \\text{ Todos }\\beta_{j} = 0. \\quad \\text{vs} \\quad H_1: \\text{ No todos } \\beta_{j} \\text{ son } 0. \\] Modelo nulo: \\(y_{ijk} = \\mu + \\alpha_i + \\epsilon_{ijk}.\\) (Modelo de factor A.) Modelo alternativo: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk}.\\) (Modelo aditivo.) Rechazamos el valor nulo cuando la estadística \\(F\\) es grande. Bajo la hipótesis nula, la distribución del estadístico de prueba es \\(F\\) con grados de libertad \\(J-1\\) y \\(IJ(K-1)\\). La fila para las pruebas de Factor A: \\[ H_0: \\text{ Todos }\\alpha_{i} = 0. \\quad \\text{vs} \\quad H_1: \\text{ No todos } \\alpha_{i} \\text{ son } 0. \\] Modelo nulo: \\(y_{ijk} = \\mu + \\beta_j + \\epsilon_{ijk}.\\) (Modelo de factor B) Modelo alternativo: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk}.\\) (Modelo aditivo.) Rechazamos la hipótesis nula cuando la estadística \\(F\\) es grande. Bajo la hipótesis nula, la distribución del estadístico de prueba es \\(F\\) con grados de libertad \\(I-1\\) y \\(IJ(K-1)\\). Estas pruebas deben realizarse de acuerdo con el modelo de jerarquía. Primero considere la prueba de interacción. Si es significativo, seleccionamos el modelo de interacción y no realizamos más pruebas. Si la interacción no es significativa, entonces consideramos la necesidad de los factores individuales del modelo aditivo. Jerarquía de modelos summary(aov(time ~ poison * treat, data = rats)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## poison 2 1.0330 0.5165 23.222 3.33e-07 *** ## treat 3 0.9212 0.3071 13.806 3.78e-06 *** ## poison:treat 6 0.2501 0.0417 1.874 0.112 ## Residuals 36 0.8007 0.0222 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Usando un nivel de significancia de \\(\\alpha=0.05\\), vemos que la interacción no es significativa. Dentro del modelo aditivo, ambos factores son significativos, por lo que seleccionamos el modelo aditivo. Dentro del modelo aditivo, podríamos hacer más pruebas sobre los efectos principales. TukeyHSD(aov(time ~ poison + treat, data = rats)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = time ~ poison + treat, data = rats) ## ## $poison ## diff lwr upr p adj ## II-I -0.073125 -0.2089936 0.0627436 0.3989657 ## III-I -0.341250 -0.4771186 -0.2053814 0.0000008 ## III-II -0.268125 -0.4039936 -0.1322564 0.0000606 ## ## $treat ## diff lwr upr p adj ## B-A 0.36250000 0.18976135 0.53523865 0.0000083 ## C-A 0.07833333 -0.09440532 0.25107198 0.6221729 ## D-A 0.22000000 0.04726135 0.39273865 0.0076661 ## C-B -0.28416667 -0.45690532 -0.11142802 0.0004090 ## D-B -0.14250000 -0.31523865 0.03023865 0.1380432 ## D-C 0.14166667 -0.03107198 0.31440532 0.1416151 Para ver un ejemplo con interacción, investigamos el conjunto de datos warpbreaks, un conjunto de datos predeterminado en R. par(mfrow = c(1, 2)) with(warpbreaks, interaction.plot(wool, tension, breaks, lwd = 2, col = 2:4)) with(warpbreaks, interaction.plot(tension, wool, breaks, lwd = 2, col = 2:3)) Cualquiera de las dos gráficas deja bastante claro que los factores de wool y tension interactúan. summary(aov(breaks ~ wool * tension, data = warpbreaks)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Usando un \\(\\alpha\\) de \\(0.05\\), la prueba ANOVA encuentra que la interacción es significativa, por lo que usamos el modelo de interacción. "],["diagnóstico-de-modelos.html", "Capítulo 13 Diagnóstico de modelos 13.1 Supuestos del modelo 13.2 Comprobación de supuestos 13.3 Observaciones inusuales 13.4 Ejemplos de análisis de datos", " Capítulo 13 Diagnóstico de modelos Tus suposiciones son tus ventanas al mundo. Frótelas de vez en cuando o la luz no entrará.  Isaac Asimov Después de leer este capítulo, podrá: Comprender los supuestos de un modelo de regresión. Evaluar los supuestos del modelo de regresión mediante visualizaciones y pruebas. Comprender el apalancamiento, los valores atípicos y los puntos influyentes. Ser capaz de identificar observaciones inusuales en modelos de regresión. 13.1 Supuestos del modelo Recuerde el modelo de regresión lineal múltiple que hemos definido. \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p-1} x_{i(p-1)} + \\epsilon_i, \\qquad i = 1, 2, \\ldots, n. \\] Usando notación matricial, este modelo se puede escribir mucho más resumido como \\[ Y = X \\beta + \\epsilon. \\] Dados los datos, encontramos las estimaciones para los parámetros \\(\\beta\\) usando \\[ \\hat{\\beta} = \\left( X^\\top X \\right)^{-1}X^\\top y. \\] Luego, notamos que estas estimaciones tenían una media \\[ \\text{E}[\\hat{\\beta}] = \\beta, \\] y varianza \\[ \\text{Var}[\\hat{\\beta}] = \\sigma^2 \\left( X^\\top X \\right)^{-1}. \\] En particular, un parámetro individual, digamos \\(\\hat{\\beta}_j\\) tenía una distribución normal \\[ \\hat{\\beta}_j \\sim N\\left(\\beta_j, \\sigma^2 C_{jj} \\right) \\] donde \\(C\\) fue la matriz definida como \\[ C = \\left(X^\\top X\\right)^{-1}. \\] Luego usamos este hecho para definir \\[ \\frac{\\hat{\\beta}_j - \\beta_j}{s_e \\sqrt{C_{jj}}} \\sim t_{n-p}, \\] que usamos para realizar pruebas de hipótesis. Hasta ahora hemos analizado varias métricas como RMSE, RSE y \\(R^2\\) para determinar qué tan bien nuestro modelo se ajusta a nuestros datos. Cada uno de estos de alguna manera considera la expresión \\[ \\sum_{i = 1}^n (y_i - \\hat{y}_i)^2. \\] Entonces, esencialmente cada uno de estos mira qué tan cerca están los puntos de datos del modelo. Sin embargo, ¿eso es todo lo que nos importa? Puede ser que los errores se cometan de forma sistemática, lo que significa que nuestro modelo está mal especificado. Es posible que necesitemos términos de interacción adicionales o términos polinomiales que veremos más adelante. También es posible que en un conjunto particular de valores predictores, los errores sean muy pequeños, pero en un conjunto diferente de valores predictores, los errores sean grandes. Quizás la mayoría de los errores sean muy pequeños, pero algunos son muy grandes. Esto sugeriría que los errores no siguen una distribución normal. ¿Son estos temas los que nos preocupan? Si todo lo que quisiéramos hacer es predecir, posiblemente no, ya que solo nos preocuparíamos por el tamaño de nuestros errores. Sin embargo, si nos gustaría realizar inferencias, por ejemplo, para determinar si un predictor en particular es importante, nos importa mucho. Todos los resultados distributivos, como una prueba \\(t\\) para un solo predictor, se derivan de los supuestos de nuestro modelo. Técnicamente, los supuestos del modelo se codifican directamente en una declaración del modelo como, \\[ Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p-1} x_{i(p-1)} + \\epsilon_i \\] donde \\(\\epsilon_i \\sim N(0, \\sigma^2).\\) A menudo, los supuestos de regresión lineal, se expresan como, Linealidad: la respuesta se puede escribir como una combinación lineal de los predictores. (Con ruido sobre esta verdadera relación lineal). Independencia: los errores son independientes. Normalidad: la distribución de los errores debe seguir una distribución normal. Igualdad de varianza: la varianza del error es la misma en cualquier conjunto de valores predictores. El supuesto de linealidad se codifica como \\[ \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{p-1} x_{i(p-1)}, \\] mientras que los tres restantes, están codificados en \\[ \\epsilon_i \\sim N(0, \\sigma^2), \\] ya que \\(\\epsilon_i\\) son \\(iid\\) variables aleatorias normales con varianza constante. Si se cumplen estas suposiciones, ¡genial! Podemos realizar inferencia, y es válido. Si estas suposiciones no se cumplen, aún podemos realizar una prueba \\(t\\) usando R, pero los resultados no son válidos. Las distribuciones de las estimaciones de los parámetros no serán las esperadas. Las pruebas de hipótesis aceptarán o rechazarán incorrectamente. Básicamente, basura entra, basura sale. 13.2 Comprobación de supuestos Ahora veremos una serie de herramientas para verificar los supuestos de un modelo lineal. Para probar estas herramientas, usaremos datos simulados de tres modelos: \\[ \\text{Modelo 1:} \\quad Y = 3 + 5x + \\epsilon, \\quad \\epsilon \\sim N(0, 1) \\] \\[ \\text{Modelo 2:} \\quad Y = 3 + 5x + \\epsilon, \\quad \\epsilon \\sim N(0, x^2) \\] \\[ \\text{Modelo 3:} \\quad Y = 3 + 5x^2 + \\epsilon, \\quad \\epsilon \\sim N(0, 25) \\] sim_1 = function(sample_size = 500) { x = runif(n = sample_size) * 5 y = 3 + 5 * x + rnorm(n = sample_size, mean = 0, sd = 1) data.frame(x, y) } sim_2 = function(sample_size = 500) { x = runif(n = sample_size) * 5 y = 3 + 5 * x + rnorm(n = sample_size, mean = 0, sd = x) data.frame(x, y) } sim_3 = function(sample_size = 500) { x = runif(n = sample_size) * 5 y = 3 + 5 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 5) data.frame(x, y) } 13.2.1 Gráfica de ajustados versus residuos Probablemente nuestra herramienta más útil sea una Gráfica de ajustados versus residuales. Será útil para verificar los supuestos de linealidad y varianza constante. Los datos generados a partir del Modelo 1 no deberían mostrar ningún signo de infracción de los supuestos, por lo que usaremos esto para ver cómo debería verse una buena gráfica de ajustados versus residuales. Primero, simularemos observaciones de este modelo. set.seed(42) sim_data_1 = sim_1() head(sim_data_1) ## x y ## 1 4.574030 24.773995 ## 2 4.685377 26.475936 ## 3 1.430698 8.954993 ## 4 4.152238 23.951210 ## 5 3.208728 20.341344 ## 6 2.595480 14.943525 Luego ajustamos el modelo y agregamos la línea ajustada a un diagrama de dispersión. plot(y ~ x, data = sim_data_1, col = &quot;grey&quot;, pch = 20, main = &quot;Datos del modelo 1&quot;) fit_1 = lm(y ~ x, data = sim_data_1) abline(fit_1, col = &quot;darkorange&quot;, lwd = 3) Ahora trazamos un gráfico de ajustados versus residuales. Tenga en cuenta que se trata de residuos en el eje \\(y\\) a pesar del orden en el nombre. A veces lo verá llamado gráfico de residuales versus ajustados, o residuales versus predichos. plot(fitted(fit_1), resid(fit_1), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Datos del modelo 1&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Deberíamos buscar dos cosas en este gráfico. En cualquier valor ajustado, la media de los residuos debe ser aproximadamente 0. Si este es el caso, el supuesto de linealidad es válido. Por esta razón, generalmente agregamos una línea horizontal en \\(y=0\\) para enfatizar este punto. En cada valor ajustado, la dispersión de los residuos debe ser aproximadamente la misma. Si este es el caso, el supuesto de varianza constante es válido. Aquí vemos que este es el caso. Para tener una mejor idea de cómo un gráfico de ajustados versus residuales puede ser útil, simularemos a partir de modelos con supuestos violados. El modelo 2 es un ejemplo de varianza no constante. En este caso, la varianza es mayor para valores mayores de la variable predictora \\(x\\). set.seed(42) sim_data_2 = sim_2() fit_2 = lm(y ~ x, data = sim_data_2) plot(y ~ x, data = sim_data_2, col = &quot;grey&quot;, pch = 20, main = &quot;Datos del modelo 2&quot;) abline(fit_2, col = &quot;darkorange&quot;, lwd = 3) En realidad, esto es bastante fácil de ver agregando la línea ajustada a un gráfico de dispersión. Esto se debe a que solo estamos realizando una regresión lineal simple. Con la regresión múltiple, una gráfica de ajustados versus residuales es una necesidad, ya que agregar una regresión ajustada a una gráfica de dispersión no es exactamente posible. plot(fitted(fit_2), resid(fit_2), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Datos del modelo 2&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) En la gráfica de ajustados versus residuales, vemos dos cosas claramente. Para cualquier valor ajustado, los residuos parecen centrados aproximadamente en 0. ¡Esto es bueno! No se viola el supuesto de linealidad. Sin embargo, también vemos claramente que para valores ajustados más grandes, la dispersión de los residuos es mayor. ¡Esto es malo! Aquí se viola el supuesto de varianza constante. Ahora mostraremos un modelo que no cumple con el supuesto de linealidad. El modelo 3 es un ejemplo de un modelo en el que \\(Y\\) no es una combinación lineal de los predictores. En este caso, el predictor es \\(x\\), pero el modelo usa \\(x^2\\). (Veremos más adelante que esto es algo con lo que puede lidiar un modelo lineal. La solución es simple, ¡simplemente haga que \\(x^2\\) sea un predictor!) set.seed(42) sim_data_3 = sim_3() fit_3 = lm(y ~ x, data = sim_data_3) plot(y ~ x, data = sim_data_3, col = &quot;grey&quot;, pch = 20, main = &quot;Datos del modelo 3&quot;) abline(fit_3, col = &quot;darkorange&quot;, lwd = 3) Una vez más, esto es bastante claro en el gráfico de dispersión, pero de nuevo, no podríamos comprobar este gráfico para la regresión múltiple. plot(fitted(fit_3), resid(fit_3), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Datos del modelo 3&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Esta vez en la gráfica de ajustados versus residuales, para cualquier valor ajustado, la dispersión de los residuales es aproximadamente la misma. Sin embargo, ¡ni siquiera están cerca de centrarse en cero! En valores ajustados pequeños y grandes, el modelo se subestima, mientras que en valores ajustados medianos, el modelo se sobreestima. Estos son errores sistemáticos, no ruido aleatorio. Entonces se cumple el supuesto de varianza constante, pero se viola el supuesto de linealidad. La forma de nuestro modelo es simplemente incorrecta. ¡Estamos tratando de ajustar una línea a una curva! 13.2.2 Prueba de Breusch-Pagan La varianza constante a menudo se denomina homocedasticidad. Por el contrario, la varianza no constante se denomina heterocedasticidad. Hemos visto cómo podemos usar una gráfica de ajustados versus residuales para buscar estos atributos. Si bien una gráfica de ajustados versus residuales puede darnos una idea sobre la homocedasticidad, a veces preferiríamos una prueba más formal. Hay muchas pruebas de varianza constante, pero aquí presentaremos una, la ** Prueba Breusch-Pagan **. Los detalles exactos de la prueba se omitirán aquí, pero lo que es más importante, se puede considerar que la nula y la alternativa son, \\(H_0\\): Homoscedasticidad. Los errores tienen una variación constante con respecto al modelo real. \\(H_1\\): Heteroscedasticidad. Los errores tienen una varianza no constante con respecto al modelo real. ¿No es eso conveniente? Una prueba que probará específicamente el supuesto de varianza constante. La prueba Breusch-Pagan no se puede realizar de forma predeterminada en R, sin embargo, la función bptest en el paquete lmtest implementa la prueba. #install.packages(&quot;lmtest&quot;) library(lmtest) Probémoslo en los tres modelos que ajustamos arriba. Recordar, fit_1 no ha violado los supuestos, fit_2 violó el supuesto de varianza constante, pero no linealidad, fit_3 violó linealidad, pero no varianza constante. bptest(fit_1) ## ## studentized Breusch-Pagan test ## ## data: fit_1 ## BP = 1.0234, df = 1, p-value = 0.3117 Para fit_1 vemos un valor p grande, por lo que no rechazamos la hipótesis nula de homocedasticidad, que es lo que esperaríamos. bptest(fit_2) ## ## studentized Breusch-Pagan test ## ## data: fit_2 ## BP = 76.693, df = 1, p-value &lt; 2.2e-16 Para fit_2 vemos un valor p pequeño, por lo que rechazamos la hipótesis nula de homocedasticidad. Se viola el supuesto de varianza constante. Esto coincide con nuestros hallazgos en la gráfica de ajustados versus residuales. bptest(fit_3) ## ## studentized Breusch-Pagan test ## ## data: fit_3 ## BP = 0.33466, df = 1, p-value = 0.5629 Por último, para fit_3 nuevamente vemos un valor p grande, por lo que no rechazamos la hipótesis nula de homocedasticidad, que coincide con nuestros hallazgos en la gráfica de ajustados versus residuales. 13.2.3 Histogramas Disponemos de una serie de herramientas para evaluar el supuesto de normalidad. Lo más obvio sería hacer un histograma de los residuos. Si parece más o menos normal, creeremos que los errores podrían ser realmente normales. par(mfrow = c(1, 3)) hist(resid(fit_1), xlab = &quot;Residuales&quot;, main = &quot;Histograma de residuos, fit_1&quot;, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, breaks = 20) hist(resid(fit_2), xlab = &quot;Residuales&quot;, main = &quot;Histograma de residuos, fit_2&quot;, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, breaks = 20) hist(resid(fit_3), xlab = &quot;Residuales&quot;, main = &quot;Histograma de residuos, fit_3&quot;, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, breaks = 20) Arriba están los histogramas para cada una de las tres regresiones que hemos estado considerando. Observe que el primero, para fit_1, parece muy normal. El tercero, para fit_3, parece ser muy anormal. Sin embargo, fit_2 no es tan claro. Tiene una forma de campana rugosa, sin embargo, también tiene un pico muy afilado. Por esta razón usualmente usaremos herramientas más poderosas como gráficos Q-Q y la prueba de Shapiro-Wilk para evaluar la normalidad de los errores. 13.2.4 Gráficos Q-Q Otro método visual para evaluar la normalidad de los errores, que es más poderoso que un histograma, es un gráfico cuantil-cuantílico normal, o gráfico Q-Q para abreviar. En R estos son muy fáciles de hacer. La función qqnorm() traza los puntos, y la función qqline() agrega la línea necesaria. Creamos un gráfico Q-Q para los residuos de fit_1 para verificar si los errores realmente se pueden distribuir normalmente. qqnorm(resid(fit_1), main = &quot;Normal Q-Q Plot, fit_1&quot;, col = &quot;darkgrey&quot;) qqline(resid(fit_1), col = &quot;dodgerblue&quot;, lwd = 2) En resumen, si los puntos de la gráfica no siguen de cerca una línea recta, esto sugeriría que los datos no provienen de una distribución normal. Los cálculos necesarios para crear el gráfico varían según la implementación, pero esencialmente el eje \\(y\\) son los datos ordenados (cuantiles observados o de muestra), y el eje \\(x\\) son los valores que esperaríamos si los datos procedieran de una distribución normal (cuantiles teóricos). La página de Wikipedia para gráficos de probabilidad normal brinda detalles sobre cómo se implementa esto en R si está interesado. Además, para tener una mejor idea de cómo funcionan los gráficos Q-Q, aquí hay una función rápida que crea un gráfico Q-Q: qq_plot = function(e) { n = length(e) normal_quantiles = qnorm(((1:n - 0.5) / n)) # normal_quantiles = qnorm(((1:n) / (n + 1))) # graficar cuantiles teóricos vs observados plot(normal_quantiles, sort(e), xlab = c(&quot;Cuantiles teóricos&quot;), ylab = c(&quot;Cuantiles de muestra&quot;), col = &quot;darkgrey&quot;) title(&quot;Normal Q-Q Plot&quot;) # calcular la línea a través del primer y tercer cuartil slope = (quantile(e, 0.75) - quantile(e, 0.25)) / (qnorm(0.75) - qnorm(0.25)) intercept = quantile(e, 0.25) - slope * qnorm(0.25) # agregar al gráfico existente abline(intercept, slope, lty = 2, lwd = 2, col = &quot;dodgerblue&quot;) } Entonces podemos verificar que es esencialmente equivalente a usar qqnorm() y qqline() en R. set.seed(420) x = rnorm(100, mean = 0 , sd = 1) par(mfrow = c(1, 2)) qqnorm(x, col = &quot;darkgrey&quot;) qqline(x, lty = 2, lwd = 2, col = &quot;dodgerblue&quot;) qq_plot(x) Para tener una mejor idea de lo que significa cerca de la línea, realizamos una serie de simulaciones y creamos gráficos Q-Q. Primero, simulamos datos de una distribución normal con diferentes tamaños de muestra, y cada vez creamos una gráfica Q-Q. par(mfrow = c(1, 3)) set.seed(420) qq_plot(rnorm(10)) qq_plot(rnorm(25)) qq_plot(rnorm(100)) Dado que estos datos son muestreados a partir de una distribución normal, todos estos son, por definición, buenos gráficos Q-Q. Los puntos están cerca de la línea y concluiríamos que estos datos podrían haber sido muestreados a partir de una distribución normal. Observe que en la primera gráfica, un punto está algo lejos de la línea, pero solo un punto, en combinación con el tamaño pequeño de la muestra, no es suficiente para preocuparnos. Vemos que con el gran tamaño de la muestra, todos los puntos están bastante cerca de la línea. A continuación, simulamos datos de una distribución \\(t\\) con pequeños grados de libertad, para diferentes tamaños de muestra. par(mfrow = c(1, 3)) set.seed(420) qq_plot(rt(10, df = 4)) qq_plot(rt(25, df = 4)) qq_plot(rt(100, df = 4)) Recuerde que a medida que los grados de libertad para una distribución \\(t\\) aumentan, la distribución se vuelve cada vez más similar a una normal. Aquí, usando 4 grados de libertad, tenemos una distribución que es algo normal, es simétrica y tiene forma de campana, sin embargo tiene colas gruesas. Esto se presenta claramente en el tercer panel. Si bien muchos de los puntos están cerca de la línea, en los bordes existen grandes discrepancias. Esto indica que los valores son demasiado pequeños (negativos) o demasiado grandes (positivos) en comparación con lo que esperaríamos para una distribución normal. Entonces, para el tamaño de muestra de 100, concluiríamos que se viola el supuesto de normalidad. (Si estos fueran residuos de un modelo). Para tamaños de muestra de 10 y 25 podemos sospechar, pero no del todo seguros. Leer gráficos Q-Q es un arte, no completamente una ciencia. A continuación, simulamos datos de una distribución exponencial. par(mfrow = c(1, 3)) set.seed(420) qq_plot(rexp(10)) qq_plot(rexp(25)) qq_plot(rexp(100)) Esta es una distribución que no es muy similar a una normal, por lo que en los tres casos vemos puntos que están lejos de las líneas, por lo que pensaríamos que se viola el supuesto de normalidad. Para comprender mejor qué gráficas Q-Q son buenas, repita las simulaciones anteriores varias veces (sin establecer la semilla) y preste atención a las diferencias entre las que se simulan de la normal y las que no. También considere diferentes tamaños de muestras y parámetros de distribución. Volviendo a nuestras tres regresiones, recuerde: fit_1 no ha violado los supuestos, fit_2 violó el supuesto de varianza constante, pero no linealidad, fit_3 violó linealidad, pero no varianza constante. Ahora crearemos un gráfico Q-Q para cada uno con el objetivo de evaluar la normalidad de los errores. qqnorm(resid(fit_1), main = &quot;Normal Q-Q Plot, fit_1&quot;, col = &quot;darkgrey&quot;) qqline(resid(fit_1), col = &quot;dodgerblue&quot;, lwd = 2) Para fit_1, tenemos una gráfica Q-Q casi perfecta. Creemos que los errores siguen una distribución normal. qqnorm(resid(fit_2), main = &quot;Normal Q-Q Plot, fit_2&quot;, col = &quot;darkgrey&quot;) qqline(resid(fit_2), col = &quot;dodgerblue&quot;, lwd = 2) Para fit_2, tenemos una gráfica Q-Q sospechosa. Probablemente no creeríamos que los errores siguen una distribución normal. qqnorm(resid(fit_3), main = &quot;Normal Q-Q Plot, fit_3&quot;, col = &quot;darkgrey&quot;) qqline(resid(fit_3), col = &quot;dodgerblue&quot;, lwd = 2) Por último, para fit_3, nuevamente tenemos una gráfica Q-Q sospechosa. Probablemente no creeríamos que los errores siguen una distribución normal. 13.2.5 Prueba de Shapiro-Wilk Los histogramas y los gráficos Q-Q brindan una buena representación visual de la distribución de los residuos; sin embargo, si estamos interesados en las pruebas formales, hay varias opciones disponibles. Una prueba de uso común es la prueba de Shapiro-Wilk, que se implementa en R. set.seed(42) shapiro.test(rnorm(25)) ## ## Shapiro-Wilk normality test ## ## data: rnorm(25) ## W = 0.9499, p-value = 0.2495 shapiro.test(rexp(25)) ## ## Shapiro-Wilk normality test ## ## data: rexp(25) ## W = 0.71164, p-value = 1.05e-05 Esto nos da el valor del estadístico de prueba y su valor p. La hipótesis nula asume que los datos fueron muestreados a partir de una distribución normal, por lo que un valor p pequeño indica que creemos que hay solo una pequeña probabilidad de que los datos pudieran haber sido muestreados a partir de una distribución normal. Para obtener más detalles, consulte: Wikipedia: ShapiroWilk test. En los ejemplos anteriores, vemos que no rechazamos los datos muestreados de la normal y rechazamos los datos no normales, para cualquier \\(\\alpha\\) razonable. Volviendo de nuevo a fit_1, fit_2 y fit_3, vemos el resultado de ejecutar shapiro.test() en los residuos de cada uno, devuelve un resultado para cada uno que coincide con las decisiones basadas en los gráficos Q-Q. shapiro.test(resid(fit_1)) ## ## Shapiro-Wilk normality test ## ## data: resid(fit_1) ## W = 0.99858, p-value = 0.9622 shapiro.test(resid(fit_2)) ## ## Shapiro-Wilk normality test ## ## data: resid(fit_2) ## W = 0.93697, p-value = 1.056e-13 shapiro.test(resid(fit_3)) ## ## Shapiro-Wilk normality test ## ## data: resid(fit_3) ## W = 0.97643, p-value = 3.231e-07 13.3 Observaciones inusuales Además de verificar los supuestos de regresión, también buscamos cualquier observación inusual en los datos. A menudo, una pequeña cantidad de puntos de datos puede tener una influencia extremadamente grande en una regresión, a veces tanto que los supuestos de la regresión se violan como resultado de estos puntos. Los siguientes tres gráficos están inspirados en un ejemplo de Modelos lineales con R. par(mfrow = c(1, 3)) set.seed(42) ex_data = data.frame(x = 1:10, y = 10:1 + rnorm(n = 10)) ex_model = lm(y ~ x, data = ex_data) # apalancamiento bajo, residual grande, Influencia pequeña point_1 = c(5.4, 11) ex_data_1 = rbind(ex_data, point_1) model_1 = lm(y ~ x, data = ex_data_1) plot(y ~ x, data = ex_data_1, cex = 2, pch = 20, col = &quot;grey&quot;, main = &quot;Apalancamiento bajo, Residual grande, Influencia pequeña&quot;) points(x = point_1[1], y = point_1[2], pch = 1, cex = 4, col = &quot;black&quot;, lwd = 2) abline(ex_model, col = &quot;dodgerblue&quot;, lwd = 2) abline(model_1, lty = 2, col = &quot;darkorange&quot;, lwd = 2) legend(&quot;bottomleft&quot;, c(&quot;Original Data&quot;, &quot;Added Point&quot;), lty = c(1, 2), col = c(&quot;dodgerblue&quot;, &quot;darkorange&quot;)) # Apalancamiento alto, Residual Pequeño, Pequeña influencia point_2 = c(18, -5.7) ex_data_2 = rbind(ex_data, point_2) model_2 = lm(y ~ x, data = ex_data_2) plot(y ~ x, data = ex_data_2, cex = 2, pch = 20, col = &quot;grey&quot;, main = &quot;Apalancamiento alto, Residual Pequeño, Influencia pequeña&quot;) points(x = point_2[1], y = point_2[2], pch = 1, cex = 4, col = &quot;black&quot;, lwd = 2) abline(ex_model, col = &quot;dodgerblue&quot;, lwd = 2) abline(model_2, lty = 2, col = &quot;darkorange&quot;, lwd = 2) legend(&quot;bottomleft&quot;, c(&quot;Original Data&quot;, &quot;Added Point&quot;), lty = c(1, 2), col = c(&quot;dodgerblue&quot;, &quot;darkorange&quot;)) # Apalancamiento alto, residual grande, Gran influencia point_3 = c(14, 5.1) ex_data_3 = rbind(ex_data, point_3) model_3 = lm(y ~ x, data = ex_data_3) plot(y ~ x, data = ex_data_3, cex = 2, pch = 20, col = &quot;grey&quot;, ylim = c(-3, 12), main = &quot;Apalancamiento alto, Residual grande, Gran influencia&quot;) points(x = point_3[1], y = point_3[2], pch = 1, cex = 4, col = &quot;black&quot;, lwd = 2) abline(ex_model, col = &quot;dodgerblue&quot;, lwd = 2) abline(model_3, lty = 2, col = &quot;darkorange&quot;, lwd = 2) legend(&quot;bottomleft&quot;, c(&quot;Original Data&quot;, &quot;Added Point&quot;), lty = c(1, 2), col = c(&quot;dodgerblue&quot;, &quot;darkorange&quot;)) La línea sólida azul en cada gráfico es un ajuste de regresión a los 10 puntos de datos originales almacenados en ex_data. La línea discontinua naranja en cada gráfico es el resultado de agregar un solo punto a los datos originales en ex_data. Este punto adicional está indicado por el punto encerrado en un círculo. La pendiente de la regresión para los diez puntos originales, la línea azul sólida, viene dada por: coef(ex_model)[2] ## x ## -0.9696033 El punto agregado en la primera gráfica tiene un efecto pequeño en la pendiente, que se convierte en: coef(model_1)[2] ## x ## -0.9749534 Diremos que este punto tiene un apalancamiento bajo, es un valor atípico debido a su gran residual, pero tiene poca influencia. El punto agregado en la segunda gráfica también tiene un efecto pequeño en la pendiente, que es: coef(model_2)[2] ## x ## -0.9507397 Diremos que este punto tiene un apalancamiento alto, no es un valor atípico debido a su pequeño residuo y tiene una influencia muy pequeña. Por último, el punto agregado en la tercera gráfica tiene un efecto grande en la pendiente, que ahora es: coef(model_3)[2] ## x ## -0.5892241 Este punto agregado es influyente. tiene un alto apalancamiento y es un valor atípico debido a su gran residual. Ahora hemos mencionado tres nuevos conceptos: apalancamiento, valores atípicos y puntos influyentes, cada uno de los cuales discutiremos en detalle. 13.3.1 Apalancamiento Un punto de datos con apalancamiento alto, es un punto de datos que podría tener una gran influencia al ajustar el modelo. Recordemos que, \\[ \\hat{\\beta} = \\left(X^\\top X \\right)^{-1} X^\\top y. \\] Por lo tanto, \\[ \\hat{y} = X \\hat{\\beta} = X \\left(X^\\top X \\right)^{-1} X^\\top y \\] Ahora definimos, \\[ H = X \\left(X^\\top X\\right)^{-1} X^\\top \\] a la que nos referiremos como hat matrix. La matriz sombrero se utiliza para proyectar en el subespacio que se extiende por las columnas de \\(X\\). También se conoce simplemente como matriz de proyección. La matriz sombrero es una matriz que toma los valores originales de \\(y\\) y agrega un sombrero. \\[ \\hat{y} = H y \\] Los elementos diagonales de esta matriz se denominan apalancamiento \\[ H_{ii} = h_i, \\] donde \\(h_i\\) es el apalancamiento para la observación \\(i\\) ésima. Los valores grandes de \\(h_i\\) indican valores extremos en \\(X\\), que pueden influir en la regresión. Tenga en cuenta que los apalancamientos solo dependen de \\(X\\). Aquí, \\(p\\) el número de \\(\\beta\\) también es el trazo (y rango) de la matriz sombrero. \\[ \\sum_{i = 1}^n h_i = p \\] ¿Cuál es el valor de \\(h_i\\) que se consideraría grande? No hay una respuesta exacta a esta pregunta. Una heurística común sería comparar cada apalancamiento con dos veces el apalancamiento promedio. Un apalancamiento mayor que esto se considera una observación a tener en cuenta. Es decir, si \\[ h_i &gt; 2 \\bar{h} \\] decimos que la observación \\(i\\) tiene un gran apalancamiento. Aquí, \\[ \\bar{h} = \\frac{\\sum_{i = 1}^n h_i}{n} = \\frac{p}{n}. \\] Para la regresión lineal simple, el apalancamiento para cada punto viene dado por \\[ h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}. \\] Esta expresión debería resultarle familiar. (Piense en la inferencia de SLR). Sugiere que los grandes apalancamientos ocurren cuando los valores de \\(x\\) están lejos de su media. Recuerde que la regresión pasa por el punto \\((\\bar{x}, \\bar{y})\\). Hay varias formas de encontrar apalancamientos en R. lev_ex = data.frame( x1 = c(0, 11, 11, 7, 4, 10, 5, 8), x2 = c(1, 5, 4, 3, 1, 4, 4, 2), y = c(11, 15, 13, 14, 0, 19, 16, 8)) plot(x2 ~ x1, data = lev_ex, cex = 2) points(7, 3, pch = 20, col = &quot;red&quot;, cex = 2) Aquí hemos creado algunos datos multivariados. Observe que hemos trazado los valores de \\(x\\), no los valores de \\(y\\). El punto rojo es \\((7,3)\\) que es la media de x1 y la media dex2 respectivamente. Podríamos calcular los apalancamientos usando las expresiones definidas anteriormente. Primero creamos la matriz \\(X\\), luego calculamos \\(H\\) como se define y extraemos los elementos diagonales. X = cbind(rep(1, 8), lev_ex$x1, lev_ex$x2) H = X %*% solve(t(X) %*% X) %*% t(X) diag(H) ## [1] 0.6000 0.3750 0.2875 0.1250 0.4000 0.2125 0.5875 0.4125 Observe que aquí tenemos dos predictores, por lo que la regresión tendría 3 parámetros \\(\\beta\\), por lo que la suma de los elementos diagonales es 3. sum(diag(H)) ## [1] 3 Alternativamente, el método que usaremos con más frecuencia es simplemente ajustar una regresión, luego usar la función hatvalues(), que devuelve los apalancamientos. lev_fit = lm(y ~ ., data = lev_ex) hatvalues(lev_fit) ## 1 2 3 4 5 6 7 8 ## 0.6000 0.3750 0.2875 0.1250 0.4000 0.2125 0.5875 0.4125 Nuevamente, tenga en cuenta que aquí hemos usado los valores \\(y\\) para ajustar la regresión, pero R aún los ignora al calcular los apalancamientos, ya que los apalancamientos solo dependen de los valores \\(x\\). coef(lev_fit) ## (Intercept) x1 x2 ## 3.7 -0.7 4.4 Veamos qué sucede con estos coeficientes cuando modificamos el valor y del punto con el apalancamiento más alto. which.max(hatvalues(lev_fit)) ## 1 ## 1 lev_ex[which.max(hatvalues(lev_fit)),] ## x1 x2 y ## 1 0 1 11 Vemos que el valor original de y es 11. Crearemos una copia de los datos y modificaremos este punto para que tenga un valor y de20. lev_ex_1 = lev_ex lev_ex_1$y[1] = 20 lm(y ~ ., data = lev_ex_1) ## ## Call: ## lm(formula = y ~ ., data = lev_ex_1) ## ## Coefficients: ## (Intercept) x1 x2 ## 8.875 -1.375 4.625 Observe los cambios grandes en los coeficientes. También observe que cada uno de los coeficientes ha cambiado de alguna manera. Tenga en cuenta que el apalancamiento de los puntos no habría cambiado, ya que no hemos modificado ninguno de los valores de \\(x\\). Ahora veamos qué sucede con estos coeficientes cuando modificamos el valor y del punto con el apalancamiento más bajo. which.min(hatvalues(lev_fit)) ## 4 ## 4 lev_ex[which.min(hatvalues(lev_fit)),] ## x1 x2 y ## 4 7 3 14 Vemos que el valor original de y es 14. Nuevamente crearemos una copia de los datos y modificaremos este punto para que tenga un valor y de 30. lev_ex_2 = lev_ex lev_ex_2$y[4] = 30 lm(y ~ ., data = lev_ex_2) ## ## Call: ## lm(formula = y ~ ., data = lev_ex_2) ## ## Coefficients: ## (Intercept) x1 x2 ## 5.7 -0.7 4.4 Esta vez, a pesar de un gran cambio en el valor de y, solo hay un pequeño cambio en los coeficientes. Además, ¡solo ha cambiado el intercepto! mean(lev_ex$x1) ## [1] 7 mean(lev_ex$x2) ## [1] 3 lev_ex[4,] ## x1 x2 y ## 4 7 3 14 Observe que este punto fue la media de ambos predictores. Volviendo a nuestras tres gráficas, cada una con un punto añadido, podemos calcular los apalancamientos para cada una. Tenga en cuenta que el undécimo punto de datos es el punto agregado. hatvalues(model_1) ## 1 2 3 4 5 6 7 ## 0.33534597 0.23860732 0.16610842 0.11784927 0.09382988 0.09405024 0.11851036 ## 8 9 10 11 ## 0.16721022 0.24014985 0.33732922 0.09100926 hatvalues(model_2) ## 1 2 3 4 5 6 7 ## 0.23238866 0.18663968 0.14979757 0.12186235 0.10283401 0.09271255 0.09149798 ## 8 9 10 11 ## 0.09919028 0.11578947 0.14129555 0.66599190 hatvalues(model_3) ## 1 2 3 4 5 6 7 ## 0.27852761 0.21411043 0.16319018 0.12576687 0.10184049 0.09141104 0.09447853 ## 8 9 10 11 ## 0.11104294 0.14110429 0.18466258 0.49386503 ¿Alguno de estos es grande? hatvalues(model_1) &gt; 2 * mean(hatvalues(model_1)) ## 1 2 3 4 5 6 7 8 9 10 11 ## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE hatvalues(model_2) &gt; 2 * mean(hatvalues(model_2)) ## 1 2 3 4 5 6 7 8 9 10 11 ## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE hatvalues(model_3) &gt; 2 * mean(hatvalues(model_3)) ## 1 2 3 4 5 6 7 8 9 10 11 ## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE Vemos que en la segunda y tercer gráfica, el punto agregado es un punto de alto apalancamiento. Recuerde que solo en el tercer gráfico eso influyó en la regresión. Para entender por qué, necesitaremos analizar los valores atípicos. 13.3.2 Valores atípicos Los valores atípicos son puntos que no se ajustan bien al modelo. Pueden tener o no un gran efecto en él. Para identificar valores atípicos, buscaremos observaciones con residuos grandes. Ahora, \\[ e = y - \\hat{y} = Iy - Hy = (I - H) y \\] Entonces, bajo los supuestos de regresión lineal, \\[ \\text{Var}(e_i) = (1 - h_i) \\sigma^2 \\] y por lo tanto la estimación de \\(\\sigma^2\\) con \\(s_e^2\\) es \\[ \\text{SE}[e_i] = s_e \\sqrt{(1 - h_i)}. \\] Luego podemos mirar el residuo estandarizado para cada observación, \\(i = 1, 2, \\ldots n\\), \\[ r_i = \\frac{e_i}{s_e\\sqrt{1 - h_i}} \\overset{approx}{\\sim} N(\\mu = 0, \\sigma^ 2 = 1) \\] cuando \\(n\\) es grande. Podemos utilizar este hecho para identificar residuos grandes. Por ejemplo, los residuos estandarizados de magnitud superior a 2 solo deberían ocurrir aproximadamente el 5 por ciento de las veces. Volviendo nuevamente a nuestras tres gráficas, cada una con un punto agregado, podemos calcular los residuos y los residuos estandarizados para cada una. Los residuos estandarizados se pueden obtener en R usando rstandard() donde normalmente usaríamos resid(). resid(model_1) ## 1 2 3 4 5 6 7 ## 0.4949887 -1.4657145 -0.5629345 -0.3182468 -0.5718877 -1.1073271 0.4852728 ## 8 9 10 11 ## -1.1459548 0.9420814 -1.1641029 4.4138254 rstandard(model_1) ## 1 2 3 4 5 6 7 ## 0.3464701 -0.9585470 -0.3517802 -0.1933575 -0.3428264 -0.6638841 0.2949482 ## 8 9 10 11 ## -0.7165857 0.6167268 -0.8160389 2.6418234 rstandard(model_1)[abs(rstandard(model_1)) &gt; 2] ## 11 ## 2.641823 En la primera gráfica, vemos que el undécimo punto, el punto agregado, es un residuo estandarizado grande. resid(model_2) ## 1 2 3 4 5 6 ## 1.03288292 -0.95203397 -0.07346766 0.14700626 -0.13084829 -0.69050140 ## 7 8 9 10 11 ## 0.87788484 -0.77755647 1.28626601 -0.84413207 0.12449986 rstandard(model_2) ## 1 2 3 4 5 6 ## 1.41447023 -1.26655590 -0.09559792 0.18822094 -0.16574677 -0.86977220 ## 7 8 9 10 11 ## 1.10506546 -0.98294409 1.64121833 -1.09295417 0.25846620 rstandard(model_2)[abs(rstandard(model_2)) &gt; 2] ## named numeric(0) En la segunda gráfica, vemos que no hay puntos con grandes residuos estandarizados. resid(model_3) ## 1 2 3 4 5 6 ## 2.30296166 -0.04347087 0.47357980 0.33253808 -0.30683212 -1.22800087 ## 7 8 9 10 11 ## -0.02113027 -2.03808722 -0.33578039 -2.82769411 3.69191633 rstandard(model_3) ## 1 2 3 4 5 6 ## 1.41302755 -0.02555591 0.26980722 0.18535382 -0.16873216 -0.67141143 ## 7 8 9 10 11 ## -0.01157256 -1.12656475 -0.18882474 -1.63206526 2.70453408 rstandard(model_3)[abs(rstandard(model_3)) &gt; 2] ## 11 ## 2.704534 En la última gráfica, vemos que el undécimo punto, el punto agregado, es un residuo estandarizado grande. Recuerde que el punto agregado en las gráficas dos y tres fueron ambos de alto apalancamiento, pero ahora solo el punto en la gráfica tres tiene un gran residuo. Ahora combinaremos esta información y discutiremos la influencia. 13.3.3 Influencia Como hemos visto en los tres gráficos, algunos valores atípicos solo cambian la regresión en pequeña cantidad (gráfico uno) y algunos valores atípicos tienen un gran efecto en la regresión (gráfico tres). Las observaciones que caen en la última categoría, puntos con (alguna combinación de) alto apalancamiento y gran residual, los llamaremos influyentes. Una medida común de influencia es la Distancia de Cook, que se define como \\[ D_i = \\frac{1}{p}r_i^2\\frac{h_i}{1-{h_i}}. \\] Tenga en cuenta que esta es una función tanto del apalancamiento como de los residuales estandarizados. La distancia Cook a menudo se considera grande si \\[ D_i &gt; \\frac{4}{n} \\] y una observación con una distancia de Cook grande se llama influyente. De nuevo, esto es simplemente una heurística y no una regla exacta. La distancia de Cook para cada punto de una regresión se puede calcular usando cooks.distance() que es una función predeterminada en R. Busquemos puntos influyentes en las tres gráficas que habíamos estado considerando. Recuerde que los puntos encerrados en un círculo en cada gráfico tienen características diferentes: Gráfico Uno: apalancamiento bajo, residuo grande. Gráfico Dos: apalancamiento alto, residuo pequeño. Gráfico Tres: apalancamiento alto, residuo grande. Ahora comprobaremos directamente si cada uno de estos es influyente. cooks.distance(model_1)[11] &gt; 4 / length(cooks.distance(model_1)) ## 11 ## FALSE cooks.distance(model_2)[11] &gt; 4 / length(cooks.distance(model_2)) ## 11 ## FALSE cooks.distance(model_3)[11] &gt; 4 / length(cooks.distance(model_3)) ## 11 ## TRUE Y, como se esperaba, ¡el punto agregado en la tercer gráfica, con un alto apalancamiento y un gran residuo, se considera influyente! 13.4 Ejemplos de análisis de datos 13.4.1 Buenos diagnósticos En el último capítulo ajustamos una regresión aditiva a los datos de mtcars con mpg como respuesta y hp y am como predictores. Realicemos algunos diagnósticos en este modelo. Primero, ajuste el modelo como hicimos en el capítulo anterior. mpg_hp_add = lm(mpg ~ hp + am, data = mtcars) plot(fitted(mpg_hp_add), resid(mpg_hp_add), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;mtcars: Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) La gráfica de ajustados versus residuales se ve bien. No vemos ningún patrón obvio y la varianza parece aproximadamente constante. (Tal vez un poco más grande para valores ajustados grandes, pero no lo suficiente como para preocuparse). bptest(mpg_hp_add) ## ## studentized Breusch-Pagan test ## ## data: mpg_hp_add ## BP = 7.5858, df = 2, p-value = 0.02253 La prueba de Breusch-Pagan verifica esto, al menos para un pequeño valor de \\(\\alpha\\). qqnorm(resid(mpg_hp_add), col = &quot;darkgrey&quot;) qqline(resid(mpg_hp_add), col = &quot;dodgerblue&quot;, lwd = 2) La gráfica Q-Q se ve extremadamente bien y la prueba de Shapiro-Wilk está de acuerdo. shapiro.test(resid(mpg_hp_add)) ## ## Shapiro-Wilk normality test ## ## data: resid(mpg_hp_add) ## W = 0.96485, p-value = 0.3706 sum(hatvalues(mpg_hp_add) &gt; 2 * mean(hatvalues(mpg_hp_add))) ## [1] 2 Vemos que hay dos puntos de gran apalancamiento. sum(abs(rstandard(mpg_hp_add)) &gt; 2) ## [1] 1 También hay un punto con un gran residuo. ¿Estos resultan en puntos que se consideran influyentes? cd_mpg_hp_add = cooks.distance(mpg_hp_add) sum(cd_mpg_hp_add &gt; 4 / length(cd_mpg_hp_add)) ## [1] 2 large_cd_mpg = cd_mpg_hp_add &gt; 4 / length(cd_mpg_hp_add) cd_mpg_hp_add[large_cd_mpg] ## Toyota Corolla Maserati Bora ## 0.1772555 0.3447994 Encontramos dos puntos influyentes. Curiosamente, son coches muy diferentes. coef(mpg_hp_add) ## (Intercept) hp am ## 26.5849137 -0.0588878 5.2770853 Dado que los diagnósticos se veían bien, no hay mucha necesidad de preocuparse por estos dos puntos, pero veamos cuánto cambian los coeficientes si los eliminamos. mpg_hp_add_fix = lm(mpg ~ hp + am, data = mtcars, subset = cd_mpg_hp_add &lt;= 4 / length(cd_mpg_hp_add)) coef(mpg_hp_add_fix) ## (Intercept) hp am ## 27.22190933 -0.06286249 4.29765867 Parece que no hay mucho cambio en los coeficientes como resultado de eliminar los supuestos puntos influyentes. Tenga en cuenta que no creamos un nuevo conjunto de datos para lograr esto. En su lugar, usamos el argumento subset para lm(). Piense en lo que hace el código cd_mpg_hp_add &lt;= 4 / length(cd_mpg_hp_add) aquí. par(mfrow = c(2, 2)) plot(mpg_hp_add) Observe que, al llamar a plot() en una variable que almacena un objeto creado por lm(), se generan cuatro diagramas de diagnóstico por defecto. Utilice ?Plot.lm para obtener más información. Los dos primeros ya deberían resultarle familiares. 13.4.2 Diagnóstico sospechoso Consideremos el modelo big_model del último capítulo que se ajustó al conjunto de datos autompg. Se usó mpg como respuesta y se consideraron muchos términos de interacción entre los predictores disp, hp y domestic. str(autompg) ## &#39;data.frame&#39;: 383 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ... big_model = lm(mpg ~ disp * hp * domestic, data = autompg) qqnorm(resid(big_model), col = &quot;darkgrey&quot;) qqline(resid(big_model), col = &quot;dodgerblue&quot;, lwd = 2) shapiro.test(resid(big_model)) ## ## Shapiro-Wilk normality test ## ## data: resid(big_model) ## W = 0.96161, p-value = 1.824e-08 Aquí, tanto la gráfica Q-Q como la prueba de Shapiro-Wilk sugieren que se viola el supuesto de normalidad. big_mod_cd = cooks.distance(big_model) sum(big_mod_cd &gt; 4 / length(big_mod_cd)) ## [1] 31 Aquí, encontramos 31, ¡así que quizás eliminarlos ayude! big_model_fix = lm(mpg ~ disp * hp * domestic, data = autompg, subset = big_mod_cd &lt; 4 / length(big_mod_cd)) qqnorm(resid(big_model_fix), col = &quot;grey&quot;) qqline(resid(big_model_fix), col = &quot;dodgerblue&quot;, lwd = 2) shapiro.test(resid(big_model_fix)) ## ## Shapiro-Wilk normality test ## ## data: resid(big_model_fix) ## W = 0.99035, p-value = 0.02068 Eliminar estos puntos da como resultado una gráfica Q-Q mucho mejor, y ahora Shapiro-Wilk no puede rechazar por un \\(\\alpha\\) bajo. Ahora hemos visto que, a veces, la modificación de los datos puede solucionar problemas con la regresión. Sin embargo, en el próximo capítulo, en lugar de modificar los datos, modificaremos el modelo mediante transformaciones. "],["transformaciones.html", "Capítulo 14 Transformaciones 14.1 Transformación de respuesta 14.2 Transformación del predictor Transformaciones de respuesta Transformaciones de predictores", " Capítulo 14 Transformaciones Dame una palanca lo suficientemente larga y un punto de apoyo para colocarla, y moveré el mundo.  Archimedes Después de leer este capítulo, podrá: Comprender el concepto de transformación estabilizadora de varianza. Utilizar transformaciones de la respuesta para mejorar los modelos de regresión. Utilice términos polinomiales como predictores para ajustar modelos de regresión más flexibles. En el último capítulo, verificamos los supuestos de los modelos de regresión y buscamos formas de diagnosticar posibles problemas. En este capítulo usaremos transformaciones de variables de respuesta y predictoras para corregir problemas con los diagnósticos del modelo, y también potencialmente, simplemente, hacer que un modelo se ajuste mejor a los datos. 14.1 Transformación de respuesta Veamos algunos datos salariales (ficticios) de la empresa (ficticia) Initech. Intentaremos modelar salary en función de years. Los datos se pueden encontrar en initech.csv. initech = read.csv(&quot;data/initech.csv&quot;) plot(salary ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) Primero ajustamos un modelo lineal simple. initech_fit = lm(salary ~ years, data = initech) summary(initech_fit) ## ## Call: ## lm(formula = salary ~ years, data = initech) ## ## Residuals: ## Min 1Q Median 3Q Max ## -57225 -18104 241 15589 91332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5302 5750 0.922 0.359 ## years 8637 389 22.200 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27360 on 98 degrees of freedom ## Multiple R-squared: 0.8341, Adjusted R-squared: 0.8324 ## F-statistic: 492.8 on 1 and 98 DF, p-value: &lt; 2.2e-16 Este modelo parece significativo, pero ¿cumple los supuestos del modelo? plot(salary ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) abline(initech_fit, col = &quot;darkorange&quot;, lwd = 2) Al agregar la línea ajustada al gráfico, vemos que la relación lineal parece correcta. par(mfrow = c(1, 2)) plot(fitted(initech_fit), resid(initech_fit), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(initech_fit), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(initech_fit), col = &quot;dodgerblue&quot;, lwd = 2) Sin embargo, a partir de la gráfica de ajustados versus residuales, parece que hay una varianza no constante. Específicamente, la varianza aumenta a medida que aumenta el valor ajustado. 14.1.1 Transformaciones estabilizadoras de varianza Recuerde que el valor ajustado es nuestra estimación de la media a un valor particular de \\(x\\). Bajo nuestras suposiciones habituales, \\[ \\epsilon \\sim N(0,\\sigma^2) \\] y por lo tanto \\[ \\text{Var}[Y | X = x] = \\sigma^2 \\] que es un valor constante para cualquier valor de \\(x\\). Sin embargo, aquí vemos que la varianza es una función de la media, \\[ \\text{Var}[Y \\mid X = x] = h(\\text{E}[Y \\mid X = x]). \\] En este caso, \\(h\\) es una función creciente. Para corregir esto, nos gustaría encontrar alguna función de \\(Y\\), \\(g(Y)\\) tal que, \\[ \\text{Var}[g(Y) \\mid X = x] = c \\] donde \\(c\\) es una constante que no depende de la media, \\(\\text{E}[Y \\mid X = x]\\). Una transformación que logra esto se llama transformación estabilizadora de varianza. Una transformación de estabilización de la varianza (VST) común cuando vemos una varianza creciente en una gráfica de ajustados versus residuales es \\(\\log(Y)\\). Además, si los valores de una variable abarcan más de un orden de magnitud y la variable es estrictamente positiva, es probable que sea útil reemplazar la variable por su logaritmo. Un recordatorio, que para nuestros propósitos, \\(\\log\\) y \\(\\ln\\) son ambos logaritmos naturales. R usa log para referirse al logaritmo natural, a menos que se especifique una base diferente. Ahora usaremos un modelo con una respuesta transformada logarítmicamente para los datos Initech, \\[ \\log(Y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i. \\] Tenga en cuenta que si volvemos a escalar el modelo desde una escala logarítmica a la escala original de los datos, ahora tenemos \\[ Y_i = \\exp(\\beta_0 + \\beta_1 x_i) \\cdot \\exp(\\epsilon_i) \\] que tiene los errores que ingresan al modelo de forma multiplicativa. El ajuste de este modelo en R requiere sólo una pequeña modificación en la especificación de nuestra fórmula. initech_fit_log = lm(log(salary) ~ years, data = initech) Tenga en cuenta que aunque log(y) se considera la nueva variable respuesta, en realidad no creamos una nueva variable en R, sino que simplemente transformamos la variable dentro de la fórmula del modelo. plot(log(salary) ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) abline(initech_fit_log, col = &quot;darkorange&quot;, lwd = 2) Al graficar los datos en la escala logarítmica transformada y al agregar la línea ajustada, la relación vuelve a parecer lineal y ya podemos ver que la variación sobre la línea ajustada parece constante. plot(salary ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) curve(exp(initech_fit_log$coef[1] + initech_fit_log$coef[2] * x), from = 0, to = 30, add = TRUE, col = &quot;darkorange&quot;, lwd = 2) Al trazar los datos en la escala original y agregar la regresión ajustada, vemos una relación exponencial. Sin embargo, este sigue siendo un modelo lineal, ya que la nueva respuesta transformada, \\(\\log(y)\\), sigue siendo una combinación lineal de los predictores. par(mfrow = c(1, 2)) plot(fitted(initech_fit_log), resid(initech_fit_log), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(initech_fit_log), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(initech_fit_log), col = &quot;dodgerblue&quot;, lwd = 2) La gráfica de ajustados versus residuales se ve mucho mejor. Parece que el supuesto de varianza constante ya no se viola. Al comparar el RMSE usando la respuesta original y transformada, también vemos que el modelo logarítmico transformado simplemente se ajusta mejor, con un error cuadrático promedio más pequeño. sqrt(mean(resid(initech_fit) ^ 2)) ## [1] 27080.16 sqrt(mean(resid(initech_fit_log) ^ 2)) ## [1] 0.1934907 Pero espere, eso no es justo, esta diferencia se debe simplemente a las diferentes escalas que se utilizan. sqrt(mean((initech$salary - fitted(initech_fit)) ^ 2)) ## [1] 27080.16 sqrt(mean((initech$salary - exp(fitted(initech_fit_log))) ^ 2)) ## [1] 24280.36 Transformando los valores ajustados del modelo logarítmico de nuevo a la escala de los datos, ¡de hecho vemos que encaja mejor! summary(initech_fit_log) ## ## Call: ## lm(formula = log(salary) ~ years, data = initech) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57022 -0.13560 0.03048 0.14157 0.41366 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.48381 0.04108 255.18 &lt;2e-16 *** ## years 0.07888 0.00278 28.38 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1955 on 98 degrees of freedom ## Multiple R-squared: 0.8915, Adjusted R-squared: 0.8904 ## F-statistic: 805.2 on 1 and 98 DF, p-value: &lt; 2.2e-16 Nuevamente, la respuesta transformada es una combinación lineal de los predictores, \\[ \\log(\\hat{y}(x)) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x = 10.484 + 0.079x. \\] Pero ahora, si volvemos a escalar los datos de una escala logarítmica a la escala original, ahora tenemos \\[ \\hat{y}(x) = \\exp(\\hat{\\beta}_0) \\exp(\\hat{\\beta}_1 x) = \\exp(10.484)\\exp(0.079x). \\] Vemos que por cada año adicional de experiencia, el salario promedio aumenta \\(\\exp(0.079) = 1.0822\\) veces. Ahora estamos multiplicando, no sumando. Si bien el uso de una transformación \\(\\log\\) es posiblemente la transformación de variable de respuesta más común, existen muchas otras. Ahora consideraremos una familia de transformaciones y elegiremos la mejor de entre ellas, que incluye la transformación \\(\\log\\). 14.1.2 Transformaciones de Box-Cox El método de Box-Cox considera una familia de transformaciones sobre variables respuesta estrictamente positivas, \\[ g_\\lambda(y) = \\left\\{ \\begin{array}{lr}\\displaystyle\\frac{y^\\lambda - 1}{\\lambda} &amp; \\lambda \\neq 0\\\\ &amp; \\\\ \\log(y) &amp; \\lambda = 0 \\end{array} \\right. \\] El parámetro \\(\\lambda\\) se elige maximizando numéricamente la log-verosimilitud, \\[ L(\\lambda) = -\\frac{n}{2}\\log(RSS_\\lambda / n) + (\\lambda -1)\\sum \\log(y_i). \\] Un intervalo de confianza al \\(100(1 - \\alpha)\\%\\) para \\(\\lambda\\) es, \\[ \\left\\{ \\lambda : L(\\lambda) &gt; L(\\hat{\\lambda}) - \\frac{1}{2}\\chi_{1,\\alpha}^2 \\right\\} \\] que R trazará para que nos ayude a seleccionar rápidamente un valor \\(\\lambda\\) apropiado. A menudo elegimos un valor agradable dentro del intervalo de confianza, en lugar del valor de \\(\\lambda\\) que realmente maximiza la probabilidad. library(MASS) library(faraway) Aquí necesitamos el paquete MASS para la función boxcox(), y consideraremos un par de conjuntos de datos del paquete faraway. Primero usaremos el conjunto de datos savings como un ejemplo del uso del método Box-Cox para justificar el uso de ninguna transformación. Ajustamos un modelo de regresión múltiple aditiva con sr como respuesta y cada una de las otras variables como predictoras. savings_model = lm(sr ~ ., data = savings) Luego usamos la función boxcox() para encontrar la mejor transformación de la forma considerada por el método Box-Cox. boxcox(savings_model, plotit = TRUE) R grafica automáticamente la log-verosimilitud como una función de los posibles valores \\(\\lambda\\). Indica tanto el valor que maximiza la probabilidad logarítmica como un intervalo de confianza para el valor \\(\\lambda\\) que maximiza la probabilidad logarítmica. boxcox(savings_model, plotit = TRUE, lambda = seq(0.5, 1.5, by = 0.1)) Tenga en cuenta que podemos especificar un rango de valores \\(\\lambda\\) para considerar y, por lo tanto, graficarlos. A menudo especificamos un rango que es más interesante visualmente. Aquí vemos que \\(\\lambda = 1\\) está en el intervalo de confianza y está extremadamente cerca del máximo. Esto sugiere una transformación de la forma. \\[ \\frac{y^\\lambda - 1}{\\lambda} = \\frac{y^1 - 1}{1} = y - 1. \\] Básicamente, esto no es una transformación. No cambiaría la varianza ni haría que el modelo se ajustara mejor. Restando 1 de cada valor, solo cambiaríamos la intersección del modelo y los errores resultantes serían los mismos. plot(fitted(savings_model), resid(savings_model), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) Al observar una gráfica de ajustados versus residuales se verifica que probablemente no haya ningún problema con los supuestos de este modelo, que verifican las pruebas de Breusch-Pagan y Shapiro-Wilk. library(lmtest) bptest(savings_model) ## ## studentized Breusch-Pagan test ## ## data: savings_model ## BP = 4.9852, df = 4, p-value = 0.2888 shapiro.test(resid(savings_model)) ## ## Shapiro-Wilk normality test ## ## data: resid(savings_model) ## W = 0.98698, p-value = 0.8524 Ahora usaremos el conjunto de datos gala como ejemplo del uso del método Box-Cox para justificar una transformación distinta de \\(\\log\\). Ajustamos un modelo de regresión múltiple aditivo con Species como respuesta y la mayoría de las otras variables como predictores. gala_model = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) plot(fitted(gala_model), resid(gala_model), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) Aunque no hay muchos datos para valores ajustados grandes, todavía parece muy claro que se viola el supuesto de varianza constante. boxcox(gala_model, lambda = seq(-0.25, 0.75, by = 0.05), plotit = TRUE) Usando el método de Box-Cox, vemos que \\(\\lambda=0.3\\) está en el intervalo de confianza y está extremadamente cerca del máximo, lo que sugiere una transformación de la forma \\[ \\frac{y^\\lambda - 1}{\\lambda} = \\frac{y^{0.3} - 1}{0.3}. \\] Luego ajustamos un modelo con esta transformación aplicada a la respuesta. gala_model_cox = lm((((Species ^ 0.3) - 1) / 0.3) ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) plot(fitted(gala_model_cox), resid(gala_model_cox), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) ¡La gráfica resultante de ajustados versus residuales se ve mucho mejor! Por último, volvemos a los datos de initech y al modelo initech_fit que habíamos usado anteriormente. Recuerde que este era el modelo sin transformar y que usamos una transformación \\(\\log\\) para arreglarlo. boxcox(initech_fit) Usando el método Box-Cox, vemos que \\(\\lambda=0\\) está tanto en el intervalo como muy cerca del máximo, lo que sugiere una transformación de la forma \\[ \\log(y). \\] ¡Entonces el método Box-Cox justifica nuestra elección previa de una transformación \\(\\log\\)! 14.2 Transformación del predictor Además de la transformación de la variable de respuesta, también podemos considerar transformaciones de variables predictoras. A veces, estas transformaciones pueden ayudar con la violación de los supuestos del modelo, y otras veces se pueden usar para simplemente adaptarse a un modelo más flexible. str(autompg) ## &#39;data.frame&#39;: 383 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ... Recuerde el conjunto de datos autompg del capítulo anterior. Aquí intentaremos modelar mpg en función de hp. par(mfrow = c(1, 2)) plot(mpg ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) mpg_hp = lm(mpg ~ hp, data = autompg) abline(mpg_hp, col = &quot;darkorange&quot;, lwd = 2) plot(fitted(mpg_hp), resid(mpg_hp), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) Primero intentamos SLR, pero vemos un patrón bastante obvio en la gráfica de ajustados versus residuales, que incluye una varianza creciente, por lo que intentamos una transformación \\(\\log\\) de la respuesta par(mfrow = c(1, 2)) plot(log(mpg) ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) mpg_hp_log = lm(log(mpg) ~ hp, data = autompg) abline(mpg_hp_log, col = &quot;darkorange&quot;, lwd = 2) plot(fitted(mpg_hp_log), resid(mpg_hp_log), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) Después de realizar la transformación \\(\\log\\) de la respuesta, todavía tenemos algunos de los mismos problemas con la los ajustados versus respuesta. Ahora, intentaremos también \\(\\log\\) para transformar el predictor. par(mfrow = c(1, 2)) plot(log(mpg) ~ log(hp), data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) mpg_hp_loglog = lm(log(mpg) ~ log(hp), data = autompg) abline(mpg_hp_loglog, col = &quot;darkorange&quot;, lwd = 2) plot(fitted(mpg_hp_loglog), resid(mpg_hp_loglog), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) Aquí, nuestra gráfica de ajustados versus residuales se ve bien. 14.2.1 Polinomios Otra transformación muy común de una variable predictora es el uso de transformaciones polinomiales. Son extremadamente útiles ya que permiten modelos más flexibles, pero no cambian las unidades de las variables. No debería sorprender que las ventas de un producto estén relacionadas con el presupuesto publicitario del producto, pero hay rendimientos decrecientes. Una empresa no siempre puede esperar rendimientos lineales basados en un mayor presupuesto publicitario. Considere los datos mensuales de las ventas de widgets Initech , \\(y\\), en función del gasto publicitario de Initech para dicho widget, \\(x\\), ambos por cada diez mil dólares. Los datos se pueden encontrar en marketing.csv. marketing = read.csv(&quot;data/marketing.csv&quot;) plot(sales ~ advert, data = marketing, xlab = &quot;Gasto publicitario (* $100,00)&quot;, ylab = &quot;Ventas (* $100,00)&quot;, pch = 20, cex = 2) Nos gustaría ajustarnos al modelo, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\] donde \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) para \\(i = 1, 2, \\cdots 21.\\) La respuesta \\(y\\) ahora es una función lineal de dos variables que permiten que \\(y\\) sea una función no lineal del predictor único original \\(x\\). Consideramos que esto es una transformación, aunque en cierto sentido hemos agregado otro predictor. Por lo tanto, nuestra matriz \\(X\\) es, \\[ \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 \\\\[3pt] 1 &amp; x_2 &amp; x_2^2 \\\\[3pt] 1 &amp; x_3 &amp; x_3^2 \\\\[3pt] \\vdots &amp; \\vdots &amp; \\vdots \\\\[3pt] 1 &amp; x_{n} &amp; x_{n}^2 \\\\ \\end{bmatrix} \\] Luego podemos proceder a ajustar el modelo como lo hicimos en el pasado para la regresión lineal múltiple. \\[ \\hat{\\beta} = \\left( X^\\top X \\right)^{-1}X^\\top y. \\] Nuestras estimaciones tendrán las propiedades habituales. La media sigue \\[ E[\\hat{\\beta}] = \\beta, \\] y varianza \\[ \\text{Var}[\\hat{\\beta}] = \\sigma^2 \\left( X^\\top X \\right)^{-1}. \\] También mantenemos los mismos resultados distribucionales \\[ \\hat{\\beta}_j \\sim N\\left(\\beta_j, \\sigma^2 C_{jj} \\right). \\] mark_mod = lm(sales ~ advert, data = marketing) summary(mark_mod) ## ## Call: ## lm(formula = sales ~ advert, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7845 -1.4762 -0.5103 1.2361 3.1869 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.4502 0.6806 13.88 2.13e-11 *** ## advert 1.1918 0.0937 12.72 9.65e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.907 on 19 degrees of freedom ## Multiple R-squared: 0.8949, Adjusted R-squared: 0.8894 ## F-statistic: 161.8 on 1 and 19 DF, p-value: 9.646e-11 Si bien el modelo SLR es significativo, la gráfica de ajustados versus residuales tendría un patrón muy claro. mark_mod_poly2 = lm(sales ~ advert + I(advert ^ 2), data = marketing) summary(mark_mod_poly2) ## ## Call: ## lm(formula = sales ~ advert + I(advert^2), data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9175 -0.8333 -0.1948 0.9292 2.1385 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.76161 0.67219 10.059 8.16e-09 *** ## advert 2.46231 0.24830 9.917 1.02e-08 *** ## I(advert^2) -0.08745 0.01658 -5.275 5.14e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 18 degrees of freedom ## Multiple R-squared: 0.9587, Adjusted R-squared: 0.9541 ## F-statistic: 209 on 2 and 18 DF, p-value: 3.486e-13 Para agregar el término de segundo orden, necesitamos usar la función I() en la especificación del modelo alrededor de nuestro predictor recién creado. Vemos que con el término de primer orden en el modelo, el término cuadrático también es significativo. n = length(marketing$advert) X = cbind(rep(1, n), marketing$advert, marketing$advert ^ 2) t(X) %*% X ## [,1] [,2] [,3] ## [1,] 21.00 120.70 1107.95 ## [2,] 120.70 1107.95 12385.86 ## [3,] 1107.95 12385.86 151369.12 solve(t(X) %*% X) %*% t(X) %*% marketing$sales ## [,1] ## [1,] 6.76161045 ## [2,] 2.46230964 ## [3,] -0.08745394 Aquí verificamos que las estimaciones de los parámetros se encontraron como era de esperar. También podríamos agregar términos de orden superior, como un predictor de tercer grado. Esto es fácil de hacer. Nuestra matriz \\(X\\) simplemente se vuelve más grande nuevamente. \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\epsilon_i \\] \\[ \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; x_1^3 \\\\[3pt] 1 &amp; x_2 &amp; x_2^2 &amp; x_2^3 \\\\[3pt] 1 &amp; x_3 &amp; x_3^2 &amp; x_3^3 \\\\[3pt] \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\[3pt] 1 &amp; x_{n} &amp; x_{n}^2 &amp; x_{n}^3 \\\\ \\end{bmatrix} \\] mark_mod_poly3 = lm(sales ~ advert + I(advert ^ 2) + I(advert ^ 3), data = marketing) summary(mark_mod_poly3) ## ## Call: ## lm(formula = sales ~ advert + I(advert^2) + I(advert^3), data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.44322 -0.61310 -0.01527 0.68131 1.22517 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.890070 0.761956 5.105 8.79e-05 *** ## advert 4.681864 0.501032 9.344 4.14e-08 *** ## I(advert^2) -0.455152 0.078977 -5.763 2.30e-05 *** ## I(advert^3) 0.016131 0.003429 4.704 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8329 on 17 degrees of freedom ## Multiple R-squared: 0.9821, Adjusted R-squared: 0.9789 ## F-statistic: 310.2 on 3 and 17 DF, p-value: 4.892e-15 Ahora vemos que con los términos de primer y segundo orden en el modelo, el término de tercer orden también es significativo. ¿Pero esto tiene sentido en la práctica? El siguiente gráfico debería dar pistas sobre por qué no es así. (¡El modelo con el término de tercer orden no tiene rendimientos decrecientes!) plot(sales ~ advert, data = marketing, xlab = &quot;Gasto publicitario (* $100,00)&quot;, ylab = &quot;Ventas (* $100,00)&quot;, pch = 20, cex = 2) abline(mark_mod, lty = 2, col = &quot;green&quot;, lwd = 2) xplot = seq(0, 16, by = 0.01) lines(xplot, predict(mark_mod_poly2, newdata = data.frame(advert = xplot)), col = &quot;blue&quot;, lwd = 2) lines(xplot, predict(mark_mod_poly3, newdata = data.frame(advert = xplot)), col = &quot;red&quot;, lty = 3, lwd = 3) El gráfico anterior se realizó utilizando gráficos base en R. El siguiente gráfico se hizo usando el paquete ggplot2, un método de graficación cada vez más popular en R. library(ggplot2) ggplot(data = marketing, aes(x = advert, y = sales)) + stat_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;green&quot;, formula = y ~ x) + stat_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;blue&quot;, formula = y ~ x + I(x ^ 2)) + stat_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, formula = y ~ x + I(x ^ 2)+ I(x ^ 3)) + geom_point(colour = &quot;black&quot;, size = 3) Tenga en cuenta que podríamos ajustar un polinomio de un orden arbitrario, \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\cdots + \\beta_{p-1}x_i^{p-1} + \\epsilon_i \\] Sin embargo, debemos tener cuidado con el ajuste excesivo, ya que con un polinomio de un grado menor que el número de observaciones, a veces es posible ajustar un modelo perfectamente. set.seed(1234) x = seq(0, 10) y = 3 + x + 4 * x ^ 2 + rnorm(11, 0, 20) plot(x, y, ylim = c(-300, 400), cex = 2, pch = 20) fit = lm(y ~ x + I(x ^ 2)) #summary(fit) fit_perf = lm(y ~ x + I(x ^ 2) + I(x ^ 3) + I(x ^ 4) + I(x ^ 5) + I(x ^ 6) + I(x ^ 7) + I(x ^ 8) + I(x ^ 9) + I(x ^ 10)) summary(fit_perf) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + ## I(x^7) + I(x^8) + I(x^9) + I(x^10)) ## ## Residuals: ## ALL 11 residuals are 0: no residual degrees of freedom! ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.114e+01 NA NA NA ## x -1.918e+03 NA NA NA ## I(x^2) 4.969e+03 NA NA NA ## I(x^3) -4.932e+03 NA NA NA ## I(x^4) 2.581e+03 NA NA NA ## I(x^5) -8.035e+02 NA NA NA ## I(x^6) 1.570e+02 NA NA NA ## I(x^7) -1.947e+01 NA NA NA ## I(x^8) 1.490e+00 NA NA NA ## I(x^9) -6.424e-02 NA NA NA ## I(x^10) 1.195e-03 NA NA NA ## ## Residual standard error: NaN on 0 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: NaN ## F-statistic: NaN on 10 and 0 DF, p-value: NA xplot = seq(0, 10, by = 0.1) lines(xplot, predict(fit, newdata = data.frame(x = xplot)), col = &quot;dodgerblue&quot;, lwd = 2, lty = 1) lines(xplot, predict(fit_perf, newdata = data.frame(x = xplot)), col = &quot;darkorange&quot;, lwd = 2, lty = 2) Observe que en el resumen, R no pudo calcular los errores estándar. Este es el resultado de estar fuera de los grados de libertad. Con 11 parámetros \\(\\beta\\) y 11 puntos de datos, usamos todos los grados de libertad antes de poder estimar \\(\\sigma\\). En este ejemplo, la verdadera relación es cuadrática, pero el ajuste del polinomio de orden 10 es perfecto. En el próximo capítulo nos centraremos en el compromiso entre bondad de ajuste (minimización de errores) y complejidad del modelo. Suponga que trabaja para un fabricante de automóviles que fabrica un gran sedán de lujo. Le gustaría saber cómo funciona el automóvil desde el punto de vista de la eficiencia del combustible cuando se conduce a varias velocidades. En lugar de probar el automóvil a todas las velocidades imaginables (lo que sería imposible), crea un experimento en el que el automóvil se conduce a velocidades de interés en incrementos de 5 millas por hora. Nuestro objetivo, entonces, es ajustar un modelo a estos datos para poder predecir la eficiencia del combustible al conducir a ciertas velocidades. Los datos de este ejemplo se pueden encontrar en fuel_econ.csv. econ = read.csv(&quot;data/fuel_econ.csv&quot;) En este ejemplo, con frecuencia buscaremos una gráfica de ajustados versus residuales, por lo que deberíamos escribir una función para hacer nuestra vida más fácil, pero esto se deja como un ejercicio de tarea. También agregaremos curvas ajustadas a los graficos de dispersión repetidamente, así que, de manera inteligente, escribiremos una función para hacerlo. plot_econ_curve = function(model){ plot(mpg ~ mph, data = econ, xlab = &quot;Velocidad (millas por hora)&quot;, ylab = &quot;Eficiencia de combustible (millas por galón)&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) xplot = seq(10, 75, by = 0.1) lines(xplot, predict(model, newdata = data.frame(mph = xplot)), col = &quot;darkorange&quot;, lwd = 2, lty = 1) } Así que ahora primero ajustamos una regresión lineal simple a estos datos. fit1 = lm(mpg ~ mph, data = econ) par(mfrow = c(1, 2)) plot_econ_curve(fit1) plot(fitted(fit1), resid(fit1), xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Claramente podemos hacerlo mejor. Sí, la eficiencia del combustible aumenta a medida que aumenta la velocidad, pero solo hasta cierto punto. Ahora agregaremos términos polinomiales hasta que encajemos en un ajuste adecuado. fit2 = lm(mpg ~ mph + I(mph ^ 2), data = econ) summary(fit2) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8411 -0.9694 0.0017 1.0181 3.3900 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4444505 1.4241091 1.716 0.0984 . ## mph 1.2716937 0.0757321 16.792 3.99e-15 *** ## I(mph^2) -0.0145014 0.0008719 -16.633 4.97e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.663 on 25 degrees of freedom ## Multiple R-squared: 0.9188, Adjusted R-squared: 0.9123 ## F-statistic: 141.5 on 2 and 25 DF, p-value: 2.338e-14 par(mfrow = c(1, 2)) plot_econ_curve(fit2) plot(fitted(fit2), resid(fit2), xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Si bien este modelo claramente se ajusta mucho mejor, y el término de segundo orden es significativo, todavía vemos un patrón en la gráfica ajustada versus residual que sugiere que los términos de orden superior ayudarán. Además, esperaríamos que la curva se aplana a medida que aumenta o disminuye la velocidad, no que descienda bruscamente como vemos aquí. fit3 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3), data = econ) summary(fit3) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2) + I(mph^3), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8112 -0.9677 0.0264 1.0345 3.3827 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.258e+00 2.768e+00 0.816 0.4227 ## mph 1.291e+00 2.529e-01 5.103 3.2e-05 *** ## I(mph^2) -1.502e-02 6.604e-03 -2.274 0.0322 * ## I(mph^3) 4.066e-06 5.132e-05 0.079 0.9375 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.697 on 24 degrees of freedom ## Multiple R-squared: 0.9188, Adjusted R-squared: 0.9087 ## F-statistic: 90.56 on 3 and 24 DF, p-value: 3.17e-13 par(mfrow = c(1, 2)) plot_econ_curve(fit3) plot(fitted(fit3), resid(fit3), xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Agregar el término de tercer orden no parece ayudar en absoluto. La curva ajustada apenas cambia. Esto tiene sentido, ya que lo que nos gustaría es que la curva se aplanara en los extremos. Para ello necesitaremos un término polinomial de grado par. fit4 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3) + I(mph ^ 4), data = econ) summary(fit4) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.57410 -0.60308 0.04236 0.74481 1.93038 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.146e+01 2.965e+00 7.238 2.28e-07 *** ## mph -1.468e+00 3.913e-01 -3.751 0.00104 ** ## I(mph^2) 1.081e-01 1.673e-02 6.463 1.35e-06 *** ## I(mph^3) -2.130e-03 2.844e-04 -7.488 1.31e-07 *** ## I(mph^4) 1.255e-05 1.665e-06 7.539 1.17e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9307 on 23 degrees of freedom ## Multiple R-squared: 0.9766, Adjusted R-squared: 0.9726 ## F-statistic: 240.2 on 4 and 23 DF, p-value: &lt; 2.2e-16 par(mfrow = c(1, 2)) plot_econ_curve(fit4) plot(fitted(fit4), resid(fit4), xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Ahora estamos progresando. El término de cuarto orden es significativo con los demás términos del modelo. También estamos empezando a ver lo que esperábamos para baja y alta velocidad. Sin embargo, todavía parece haber un patrón en los residuos, por lo que volveremos a intentar términos de orden superior. Sumaremos el quinto y el sexto juntos, ya que sumar el quinto será similar a sumar el tercero. fit6 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3) + I(mph ^ 4) + I(mph ^ 5) + I(mph^6), data = econ) summary(fit6) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + ## I(mph^6), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1129 -0.5717 -0.1707 0.5026 1.5288 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.206e+00 1.204e+01 -0.349 0.7304 ## mph 4.203e+00 2.553e+00 1.646 0.1146 ## I(mph^2) -3.521e-01 2.012e-01 -1.750 0.0947 . ## I(mph^3) 1.579e-02 7.691e-03 2.053 0.0527 . ## I(mph^4) -3.473e-04 1.529e-04 -2.271 0.0338 * ## I(mph^5) 3.585e-06 1.518e-06 2.362 0.0279 * ## I(mph^6) -1.402e-08 5.941e-09 -2.360 0.0280 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8657 on 21 degrees of freedom ## Multiple R-squared: 0.9815, Adjusted R-squared: 0.9762 ## F-statistic: 186 on 6 and 21 DF, p-value: &lt; 2.2e-16 par(mfrow = c(1, 2)) plot_econ_curve(fit6) plot(fitted(fit6), resid(fit6), xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) Nuevamente, el término de sexto orden es significativo con los otros términos del modelo y aquí vemos menos patrón en el gráfico de residuos. Probemos ahora cuál de los dos modelos anteriores preferimos. Vamos a probar \\[ H_0: \\beta_5 = \\beta_6 = 0. \\] anova(fit4, fit6) ## Analysis of Variance Table ## ## Model 1: mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) ## Model 2: mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + I(mph^6) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 23 19.922 ## 2 21 15.739 2 4.1828 2.7905 0.0842 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Entonces, esta prueba no rechaza la hipótesis nula a un nivel de significancia de \\(\\alpha=0.05\\), sin embargo, el valor p es todavía bastante pequeño, y la gráfica ajustada versus residual es mucho mejor para el modelo con el término de sexto orden. Esto hace que el modelo de sexto orden sea una buena elección. Podríamos repetir este proceso una vez más. fit8 = lm(mpg ~ mph + I(mph ^ 2) + I(mph ^ 3) + I(mph ^ 4) + I(mph ^ 5) + I(mph ^ 6) + I(mph ^ 7) + I(mph ^ 8), data = econ) summary(fit8) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + ## I(mph^6) + I(mph^7) + I(mph^8), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.21938 -0.50464 -0.09105 0.49029 1.45440 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.202e+01 7.045e+01 -0.171 0.866 ## mph 6.021e+00 2.014e+01 0.299 0.768 ## I(mph^2) -5.037e-01 2.313e+00 -0.218 0.830 ## I(mph^3) 2.121e-02 1.408e-01 0.151 0.882 ## I(mph^4) -4.008e-04 5.017e-03 -0.080 0.937 ## I(mph^5) 1.789e-06 1.080e-04 0.017 0.987 ## I(mph^6) 4.486e-08 1.381e-06 0.032 0.974 ## I(mph^7) -6.456e-10 9.649e-09 -0.067 0.947 ## I(mph^8) 2.530e-12 2.835e-11 0.089 0.930 ## ## Residual standard error: 0.9034 on 19 degrees of freedom ## Multiple R-squared: 0.9818, Adjusted R-squared: 0.9741 ## F-statistic: 128.1 on 8 and 19 DF, p-value: 7.074e-15 par(mfrow = c(1, 2)) plot_econ_curve(fit8) plot(fitted(fit8), resid(fit8), xlab = &quot;Ajustados&quot;, ylab = &quot;Residuals&quot;, col = &quot;dodgerblue&quot;, pch = 20, cex =2) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) summary(fit8) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + ## I(mph^6) + I(mph^7) + I(mph^8), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.21938 -0.50464 -0.09105 0.49029 1.45440 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.202e+01 7.045e+01 -0.171 0.866 ## mph 6.021e+00 2.014e+01 0.299 0.768 ## I(mph^2) -5.037e-01 2.313e+00 -0.218 0.830 ## I(mph^3) 2.121e-02 1.408e-01 0.151 0.882 ## I(mph^4) -4.008e-04 5.017e-03 -0.080 0.937 ## I(mph^5) 1.789e-06 1.080e-04 0.017 0.987 ## I(mph^6) 4.486e-08 1.381e-06 0.032 0.974 ## I(mph^7) -6.456e-10 9.649e-09 -0.067 0.947 ## I(mph^8) 2.530e-12 2.835e-11 0.089 0.930 ## ## Residual standard error: 0.9034 on 19 degrees of freedom ## Multiple R-squared: 0.9818, Adjusted R-squared: 0.9741 ## F-statistic: 128.1 on 8 and 19 DF, p-value: 7.074e-15 anova(fit6, fit8) ## Analysis of Variance Table ## ## Model 1: mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + I(mph^6) ## Model 2: mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + I(mph^6) + ## I(mph^7) + I(mph^8) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 21 15.739 ## 2 19 15.506 2 0.2324 0.1424 0.8682 Claramente nos quedaríamos con fit6. El término de octavo orden no es significativo con los otros términos del modelo y la prueba F no rechaza. Aparte, tenga en cuenta que existe una forma más rápida de especificar un modelo con muchos términos de orden superior. fit6_alt = lm(mpg ~ poly(mph, 6), data = econ) all.equal(fitted(fit6), fitted(fit6_alt)) ## [1] TRUE Primero verificamos que este método produce los mismos valores ajustados. Sin embargo, los coeficientes estimados son diferentes. coef(fit6) ## (Intercept) mph I(mph^2) I(mph^3) I(mph^4) ## -4.206224e+00 4.203382e+00 -3.521452e-01 1.579340e-02 -3.472665e-04 ## I(mph^5) I(mph^6) ## 3.585201e-06 -1.401995e-08 coef(fit6_alt) ## (Intercept) poly(mph, 6)1 poly(mph, 6)2 poly(mph, 6)3 poly(mph, 6)4 ## 24.40714286 4.16769628 -27.66685755 0.13446747 7.01671480 ## poly(mph, 6)5 poly(mph, 6)6 ## 0.09288754 -2.04307796 Esto se debe a que poly() usa polinomios ortogonales, lo que resuelve un problema que discutiremos en el próximo capítulo. summary(fit6) ## ## Call: ## lm(formula = mpg ~ mph + I(mph^2) + I(mph^3) + I(mph^4) + I(mph^5) + ## I(mph^6), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1129 -0.5717 -0.1707 0.5026 1.5288 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.206e+00 1.204e+01 -0.349 0.7304 ## mph 4.203e+00 2.553e+00 1.646 0.1146 ## I(mph^2) -3.521e-01 2.012e-01 -1.750 0.0947 . ## I(mph^3) 1.579e-02 7.691e-03 2.053 0.0527 . ## I(mph^4) -3.473e-04 1.529e-04 -2.271 0.0338 * ## I(mph^5) 3.585e-06 1.518e-06 2.362 0.0279 * ## I(mph^6) -1.402e-08 5.941e-09 -2.360 0.0280 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8657 on 21 degrees of freedom ## Multiple R-squared: 0.9815, Adjusted R-squared: 0.9762 ## F-statistic: 186 on 6 and 21 DF, p-value: &lt; 2.2e-16 summary(fit6_alt) ## ## Call: ## lm(formula = mpg ~ poly(mph, 6), data = econ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1129 -0.5717 -0.1707 0.5026 1.5288 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.40714 0.16360 149.184 &lt; 2e-16 *** ## poly(mph, 6)1 4.16770 0.86571 4.814 9.31e-05 *** ## poly(mph, 6)2 -27.66686 0.86571 -31.958 &lt; 2e-16 *** ## poly(mph, 6)3 0.13447 0.86571 0.155 0.878 ## poly(mph, 6)4 7.01671 0.86571 8.105 6.68e-08 *** ## poly(mph, 6)5 0.09289 0.86571 0.107 0.916 ## poly(mph, 6)6 -2.04308 0.86571 -2.360 0.028 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8657 on 21 degrees of freedom ## Multiple R-squared: 0.9815, Adjusted R-squared: 0.9762 ## F-statistic: 186 on 6 and 21 DF, p-value: &lt; 2.2e-16 Sin embargo, tenga en cuenta que el valor p para probar el término de grado 6 es el mismo. Debido a esto, en su mayor parte podemos usar estos indistintamente. Para usar poly() y obtener los mismos resultados que usar I() repetidamente, necesitaríamos establecer raw = TRUE. fit6_alt2 = lm(mpg ~ poly(mph, 6, raw = TRUE), data = econ) coef(fit6_alt2) ## (Intercept) poly(mph, 6, raw = TRUE)1 poly(mph, 6, raw = TRUE)2 ## -4.206224e+00 4.203382e+00 -3.521452e-01 ## poly(mph, 6, raw = TRUE)3 poly(mph, 6, raw = TRUE)4 poly(mph, 6, raw = TRUE)5 ## 1.579340e-02 -3.472665e-04 3.585201e-06 ## poly(mph, 6, raw = TRUE)6 ## -1.401995e-08 Ahora hemos visto cómo transformar variables de predicción y respuesta. En este capítulo nos hemos centrado principalmente en usar esto en el contexto de la reparación de modelos SLR. Sin embargo, estos conceptos se pueden usar fácilmente junto con variables e interacciones categóricas para construir modelos más grandes y flexibles. En el próximo capítulo, discutiremos cómo elegir un buen modelo de una colección de posibles modelos. Transformaciones de respuesta initech = read.csv(&quot;data/initech.csv&quot;) plot(salary ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) initech_fit = lm(salary ~ years, data = initech) summary(initech_fit) ## ## Call: ## lm(formula = salary ~ years, data = initech) ## ## Residuals: ## Min 1Q Median 3Q Max ## -57225 -18104 241 15589 91332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5302 5750 0.922 0.359 ## years 8637 389 22.200 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27360 on 98 degrees of freedom ## Multiple R-squared: 0.8341, Adjusted R-squared: 0.8324 ## F-statistic: 492.8 on 1 and 98 DF, p-value: &lt; 2.2e-16 plot(salary ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) abline(initech_fit, col = &quot;darkorange&quot;, lwd = 2) par(mfrow = c(1, 2)) plot(fitted(initech_fit), resid(initech_fit), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(initech_fit), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(initech_fit), col = &quot;dodgerblue&quot;, lwd = 2) initech_fit_log = lm(log(salary) ~ years, data = initech) \\[ \\log(Y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] plot(log(salary) ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) abline(initech_fit_log, col = &quot;darkorange&quot;, lwd = 2) \\[ Y_i = \\exp(\\beta_0 + \\beta_1 x_i) \\cdot \\exp(\\epsilon_i) \\] plot(salary ~ years, data = initech, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Salarios en Initech, por antigüedad&quot;) curve(exp(initech_fit_log$coef[1] + initech_fit_log$coef[2] * x), from = 0, to = 30, add = TRUE, col = &quot;darkorange&quot;, lwd = 2) par(mfrow = c(1, 2)) plot(fitted(initech_fit_log), resid(initech_fit_log), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(initech_fit_log), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(initech_fit_log), col = &quot;dodgerblue&quot;, lwd = 2) sqrt(mean(resid(initech_fit) ^ 2)) ## [1] 27080.16 sqrt(mean(resid(initech_fit_log) ^ 2)) ## [1] 0.1934907 sqrt(mean((initech$salary - fitted(initech_fit)) ^ 2)) ## [1] 27080.16 sqrt(mean((initech$salary - exp(fitted(initech_fit_log))) ^ 2)) ## [1] 24280.36 Transformaciones de predictores 14.2.2 Un modelo cuadrático sim_quad = function(sample_size = 500) { x = runif(n = sample_size) * 5 y = 3 + 5 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 5) data.frame(x, y) } set.seed(314) quad_data = sim_quad(sample_size = 200) lin_fit = lm(y ~ x, data = quad_data) summary(lin_fit) ## ## Call: ## lm(formula = y ~ x, data = quad_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.363 -7.550 -3.416 8.472 26.181 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.3271 1.5494 -11.83 &lt;2e-16 *** ## x 24.8716 0.5343 46.55 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.79 on 198 degrees of freedom ## Multiple R-squared: 0.9163, Adjusted R-squared: 0.9158 ## F-statistic: 2167 on 1 and 198 DF, p-value: &lt; 2.2e-16 plot(y ~ x, data = quad_data, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Datos cuadráticos simulados&quot;) abline(lin_fit, col = &quot;darkorange&quot;, lwd = 2) par(mfrow = c(1, 2)) plot(fitted(lin_fit), resid(lin_fit), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(lin_fit), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(lin_fit), col = &quot;dodgerblue&quot;, lwd = 2) \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\] quad_fit = lm(y ~ x + I(x^2), data = quad_data) summary(quad_fit) ## ## Call: ## lm(formula = y ~ x + I(x^2), data = quad_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.4167 -3.0581 0.2297 3.1024 12.1256 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0649 0.9577 3.200 0.0016 ** ## x -0.5108 0.8637 -0.591 0.5549 ## I(x^2) 5.0740 0.1667 30.433 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.531 on 197 degrees of freedom ## Multiple R-squared: 0.9853, Adjusted R-squared: 0.9852 ## F-statistic: 6608 on 2 and 197 DF, p-value: &lt; 2.2e-16 plot(y ~ x, data = quad_data, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Datos cuadráticos simulados&quot;) curve(quad_fit$coef[1] + quad_fit$coef[2] * x + quad_fit$coef[3] * x ^ 2, from = -5, to = 30, add = TRUE, col = &quot;darkorange&quot;, lwd = 2) par(mfrow = c(1, 2)) plot(fitted(quad_fit), resid(quad_fit), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(quad_fit), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(quad_fit), col = &quot;dodgerblue&quot;, lwd = 2) 14.2.3 Sobreajuste y extrapolación sim_for_perf = function() { x = seq(0, 10) y = 3 + x - 4 * x ^ 2 + rnorm(n = 11, mean = 0, sd = 25) data.frame(x, y) } set.seed(1234) data_for_perf = sim_for_perf() fit_correct = lm(y ~ x + I(x ^ 2), data = data_for_perf) fit_perfect = lm(y ~ x + I(x ^ 2) + I(x ^ 3) + I(x ^ 4) + I(x ^ 5) + I(x ^ 6) + I(x ^ 7) + I(x ^ 8) + I(x ^ 9) + I(x ^ 10), data = data_for_perf) x_plot = seq(-5, 15, by = 0.1) plot(y ~ x, data = data_for_perf, ylim = c(-450, 100), cex = 2, pch = 20) lines(x_plot, predict(fit_correct, newdata = data.frame(x = x_plot)), col = &quot;dodgerblue&quot;, lwd = 2, lty = 1) lines(x_plot, predict(fit_perfect, newdata = data.frame(x = x_plot)), col = &quot;darkorange&quot;, lwd = 2, lty = 2) 14.2.4 Comparación de modelos polinomiales sim_higher = function(sample_size = 250) { x = runif(n = sample_size, min = -1, max = 1) * 2 y = 3 + -6 * x ^ 2 + 1 * x ^ 4 + rnorm(n = sample_size, mean = 0, sd = 3) data.frame(x, y) } \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i \\] \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + \\epsilon_i \\] \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + \\beta_5 x_i^5 + \\beta_6 x_i^6 + \\epsilon_i \\] set.seed(42) data_higher = sim_higher() plot(y ~ x, data = data_higher, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Datos cuarticos simulados&quot;) fit_2 = lm(y ~ poly(x, 2), data = data_higher) fit_4 = lm(y ~ poly(x, 4), data = data_higher) plot(y ~ x, data = data_higher, col = &quot;grey&quot;, pch = 20, cex = 1.5, main = &quot;Datos cuarticos simulados&quot;) x_plot = seq(-5, 5, by = 0.05) lines(x_plot, predict(fit_2, newdata = data.frame(x = x_plot)), col = &quot;dodgerblue&quot;, lwd = 2, lty = 1) lines(x_plot, predict(fit_4, newdata = data.frame(x = x_plot)), col = &quot;darkorange&quot;, lwd = 2, lty = 2) par(mfrow = c(1, 2)) plot(fitted(fit_2), resid(fit_2), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(fit_2), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(fit_2), col = &quot;dodgerblue&quot;, lwd = 2) par(mfrow = c(1, 2)) plot(fitted(fit_4), resid(fit_4), col = &quot;grey&quot;, pch = 20, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;, main = &quot;Ajustados versus Residuales&quot;) abline(h = 0, col = &quot;darkorange&quot;, lwd = 2) qqnorm(resid(fit_4), main = &quot;Normal Q-Q Plot&quot;, col = &quot;darkgrey&quot;) qqline(resid(fit_4), col = &quot;dodgerblue&quot;, lwd = 2) anova(fit_2, fit_4) ## Analysis of Variance Table ## ## Model 1: y ~ poly(x, 2) ## Model 2: y ~ poly(x, 4) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 247 2334.1 ## 2 245 1912.6 2 421.51 26.997 2.536e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fit_6 = lm(y ~ poly(x, 6), data = data_higher) anova(fit_4, fit_6) ## Analysis of Variance Table ## ## Model 1: y ~ poly(x, 4) ## Model 2: y ~ poly(x, 6) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 245 1912.6 ## 2 243 1904.4 2 8.1889 0.5224 0.5937 14.2.5 poly() Función y polinomios ortogonales \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\beta_4 x_i^4 + \\epsilon_i \\] fit_4a = lm(y ~ poly(x, degree = 4), data = data_higher) fit_4b = lm(y ~ poly(x, degree = 4, raw = TRUE), data = data_higher) fit_4c = lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = data_higher) coef(fit_4a) ## (Intercept) poly(x, degree = 4)1 poly(x, degree = 4)2 ## -1.980036 -2.053929 -49.344752 ## poly(x, degree = 4)3 poly(x, degree = 4)4 ## 0.669874 20.519759 coef(fit_4b) ## (Intercept) poly(x, degree = 4, raw = TRUE)1 ## 2.9996256 -0.3880250 ## poly(x, degree = 4, raw = TRUE)2 poly(x, degree = 4, raw = TRUE)3 ## -6.1511166 0.1269046 ## poly(x, degree = 4, raw = TRUE)4 ## 1.0282139 coef(fit_4c) ## (Intercept) x I(x^2) I(x^3) I(x^4) ## 2.9996256 -0.3880250 -6.1511166 0.1269046 1.0282139 unname(coef(fit_4a)) ## [1] -1.980036 -2.053929 -49.344752 0.669874 20.519759 unname(coef(fit_4b)) ## [1] 2.9996256 -0.3880250 -6.1511166 0.1269046 1.0282139 unname(coef(fit_4c)) ## [1] 2.9996256 -0.3880250 -6.1511166 0.1269046 1.0282139 all.equal(fitted(fit_4a), fitted(fit_4b)) ## [1] TRUE all.equal(resid(fit_4a), resid(fit_4b)) ## [1] TRUE summary(fit_4a) ## ## Call: ## lm(formula = y ~ poly(x, degree = 4), data = data_higher) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6982 -2.0334 0.0042 1.9532 7.4626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.9800 0.1767 -11.205 &lt; 2e-16 *** ## poly(x, degree = 4)1 -2.0539 2.7940 -0.735 0.463 ## poly(x, degree = 4)2 -49.3448 2.7940 -17.661 &lt; 2e-16 *** ## poly(x, degree = 4)3 0.6699 2.7940 0.240 0.811 ## poly(x, degree = 4)4 20.5198 2.7940 7.344 3.06e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.794 on 245 degrees of freedom ## Multiple R-squared: 0.5993, Adjusted R-squared: 0.5928 ## F-statistic: 91.61 on 4 and 245 DF, p-value: &lt; 2.2e-16 summary(fit_4c) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3) + I(x^4), data = data_higher) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6982 -2.0334 0.0042 1.9532 7.4626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.9996 0.3315 9.048 &lt; 2e-16 *** ## x -0.3880 0.3828 -1.014 0.312 ## I(x^2) -6.1511 0.5049 -12.183 &lt; 2e-16 *** ## I(x^3) 0.1269 0.1456 0.871 0.384 ## I(x^4) 1.0282 0.1400 7.344 3.06e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.794 on 245 degrees of freedom ## Multiple R-squared: 0.5993, Adjusted R-squared: 0.5928 ## F-statistic: 91.61 on 4 and 245 DF, p-value: &lt; 2.2e-16 14.2.6 Función de inhibición coef(lm(y ~ x + x ^ 2, data = quad_data)) ## (Intercept) x ## -18.32715 24.87163 coef(lm(y ~ x + I(x ^ 2), data = quad_data)) ## (Intercept) x I(x^2) ## 3.0649446 -0.5108131 5.0739805 coef(lm(y ~ x + x:x, data = quad_data)) ## (Intercept) x ## -18.32715 24.87163 coef(lm(y ~ x * x, data = quad_data)) ## (Intercept) x ## -18.32715 24.87163 coef(lm(y ~ x ^ 2, data = quad_data)) ## (Intercept) x ## -18.32715 24.87163 coef(lm(y ~ x + x ^ 2, data = quad_data)) ## (Intercept) x ## -18.32715 24.87163 coef(lm(y ~ I(x + x), data = quad_data)) ## (Intercept) I(x + x) ## -18.32715 12.43582 coef(lm(y ~ x + x, data = quad_data)) ## (Intercept) x ## -18.32715 24.87163 14.2.7 Ejemplo con datos pairs(autompg) mpg_hp = lm(mpg ~ hp, data = autompg) par(mfrow = c(1, 2)) plot(mpg ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) abline(mpg_hp, col = &quot;darkorange&quot;, lwd = 2) plot(fitted(mpg_hp), resid(mpg_hp), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) mpg_hp_log = lm(mpg ~ hp + I(hp ^ 2), data = autompg) par(mfrow = c(1, 2)) plot(mpg ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) xplot = seq(min(autompg$hp), max(autompg$hp), by = 0.1) lines(xplot, predict(mpg_hp_log, newdata = data.frame(hp = xplot)), col = &quot;darkorange&quot;, lwd = 2, lty = 1) plot(fitted(mpg_hp_log), resid(mpg_hp_log), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) mpg_hp_log = lm(log(mpg) ~ hp + I(hp ^ 2), data = autompg) par(mfrow = c(1, 2)) plot(log(mpg) ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) xplot = seq(min(autompg$hp), max(autompg$hp), by = 0.1) lines(xplot, predict(mpg_hp_log, newdata = data.frame(hp = xplot)), col = &quot;darkorange&quot;, lwd = 2, lty = 1) plot(fitted(mpg_hp_log), resid(mpg_hp_log), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) mpg_hp_loglog = lm(log(mpg) ~ log(hp), data = autompg) par(mfrow = c(1, 2)) plot(log(mpg) ~ log(hp), data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) abline(mpg_hp_loglog, col = &quot;darkorange&quot;, lwd = 2) plot(fitted(mpg_hp_loglog), resid(mpg_hp_loglog), col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5, xlab = &quot;Ajustados&quot;, ylab = &quot;Residuales&quot;) abline(h = 0, lty = 2, col = &quot;darkorange&quot;, lwd = 2) big_model = lm(mpg ~ disp * hp * domestic, data = autompg) qqnorm(resid(big_model), col = &quot;darkgrey&quot;) qqline(resid(big_model), col = &quot;dodgerblue&quot;, lwd = 2) bigger_model = lm(log(mpg) ~ disp * hp * domestic + I(disp ^ 2) + I(hp ^ 2), data = autompg) summary(bigger_model) ## ## Call: ## lm(formula = log(mpg) ~ disp * hp * domestic + I(disp^2) + I(hp^2), ## data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.40381 -0.08635 -0.01040 0.09995 0.71365 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.276e+00 2.564e-01 16.674 &lt;2e-16 *** ## disp -5.289e-03 2.565e-03 -2.062 0.0399 * ## hp -7.386e-03 3.309e-03 -2.232 0.0262 * ## domestic -2.496e-01 2.787e-01 -0.896 0.3710 ## I(disp^2) 8.552e-06 4.141e-06 2.065 0.0396 * ## I(hp^2) -1.565e-05 1.679e-05 -0.932 0.3519 ## disp:hp 2.685e-05 3.082e-05 0.871 0.3842 ## disp:domestic -1.101e-03 2.526e-03 -0.436 0.6631 ## hp:domestic 7.560e-03 3.689e-03 2.049 0.0411 * ## disp:hp:domestic -2.311e-05 2.662e-05 -0.868 0.3859 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1507 on 373 degrees of freedom ## Multiple R-squared: 0.8107, Adjusted R-squared: 0.8062 ## F-statistic: 177.5 on 9 and 373 DF, p-value: &lt; 2.2e-16 qqnorm(resid(bigger_model), col = &quot;darkgrey&quot;) qqline(resid(bigger_model), col = &quot;dodgerblue&quot;, lwd = 2) "],["colinealidad.html", "Capítulo 15 Colinealidad 15.1 Colinealidad exacta 15.2 Colinealidad 15.3 Simulación", " Capítulo 15 Colinealidad Si me veo confundido es porque estoy pensando.  Samuel Goldwyn Después de leer este capítulo, podrá: Identificar colinealidad en regresión. Comprender el efecto de la colinealidad en modelos de regresión. 15.1 Colinealidad exacta Creamos un conjunto de datos donde uno de los predictores, \\(x_3\\), sea una combinación lineal de los otros predictores. gen_exact_collin_data = function(num_samples = 100) { x1 = rnorm(n = num_samples, mean = 80, sd = 10) x2 = rnorm(n = num_samples, mean = 70, sd = 5) x3 = 2 * x1 + 4 * x2 + 3 y = 3 + x1 + x2 + rnorm(n = num_samples, mean = 0, sd = 1) data.frame(y, x1, x2, x3) } Observe que la forma en que estamos generando estos datos, la respuesta \\(y\\) solo depende realmente de \\(x_1\\) y \\(x_2\\). set.seed(42) exact_collin_data = gen_exact_collin_data() head(exact_collin_data) ## y x1 x2 x3 ## 1 170.7135 93.70958 76.00483 494.4385 ## 2 152.9106 74.35302 75.22376 452.6011 ## 3 152.7866 83.63128 64.98396 430.1984 ## 4 170.6306 86.32863 79.24241 492.6269 ## 5 152.3320 84.04268 66.66613 437.7499 ## 6 151.3155 78.93875 70.52757 442.9878 ¿Qué sucede cuando intentamos ajustar un modelo de regresión en R usando todos los predictores? exact_collin_fit = lm(y ~ x1 + x2 + x3, data = exact_collin_data) summary(exact_collin_fit) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3, data = exact_collin_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.57662 -0.66188 -0.08253 0.63706 2.52057 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.957336 1.735165 1.704 0.0915 . ## x1 0.985629 0.009788 100.702 &lt;2e-16 *** ## x2 1.017059 0.022545 45.112 &lt;2e-16 *** ## x3 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.014 on 97 degrees of freedom ## Multiple R-squared: 0.9923, Adjusted R-squared: 0.9921 ## F-statistic: 6236 on 2 and 97 DF, p-value: &lt; 2.2e-16 Vemos que R simplemente decide excluir una variable. ¿Por qué está pasando esto? X = cbind(1, as.matrix(exact_collin_data[,-1])) solve(t(X) %*% X) Si intentamos encontrar \\(\\boldsymbol{\\hat{\\beta}}\\) usando \\(\\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1}\\), vemos que no es posible, debido al hecho de que las columnas de \\(\\boldsymbol{X}\\) son linealmente dependientes. Las líneas de código anteriores no se ejecutaron, ¡porque producen un error! Cuando esto sucede, decimos que hay colinealidad exacta en el conjunto de datos. Como resultado de este problema, R esencialmente eligió ajustarse al modelo y ~ x1 + x2. Sin embargo, observe que otros dos modelos lograrán exactamente el mismo ajuste. fit1 = lm(y ~ x1 + x2, data = exact_collin_data) fit2 = lm(y ~ x1 + x3, data = exact_collin_data) fit3 = lm(y ~ x2 + x3, data = exact_collin_data) Vemos que los valores ajustados para cada uno de los tres modelos son exactamente los mismos. Este es el resultado de \\(x_3\\) que contiene toda la información de \\(x_1\\) y \\(x_2\\). Siempre que se incluya uno de \\(x_1\\) o \\(x_2\\) en el modelo, se puede usar \\(x_3\\) para recuperar la información de la variable no incluida. all.equal(fitted(fit1), fitted(fit2)) ## [1] TRUE all.equal(fitted(fit2), fitted(fit3)) ## [1] TRUE Si bien sus valores ajustados son todos iguales, sus coeficientes estimados son muy diferentes. ¡El signo de \\(x_2\\) se cambia en dos de los modelos! Así que solo fit1 explica correctamente la relación entre las variables,fit2 y fit3 aún predicen así comofit1, a pesar de que los coeficientes tienen poco o ningún significado, un concepto al que volveremos más adelante. coef(fit1) ## (Intercept) x1 x2 ## 2.9573357 0.9856291 1.0170586 coef(fit2) ## (Intercept) x1 x3 ## 2.1945418 0.4770998 0.2542647 coef(fit3) ## (Intercept) x2 x3 ## 1.4788921 -0.9541995 0.4928145 15.2 Colinealidad La colinealidad exacta es un ejemplo extremo de colinealidad, que ocurre en regresiones múltiples cuando las variables predictoras están altamente correlacionadas. La colinealidad a menudo se denomina multicolinealidad, ya que es un fenómeno que realmente solo ocurre durante la regresión múltiple. En el conjunto de datos seatpos del paquete faraway, veremos un ejemplo de este concepto. Los predictores en este conjunto de datos son varios atributos de los conductores de automóviles, como su altura, peso y edad. La variable respuesta hipcenter mide la distancia horizontal del punto medio de las caderas desde una ubicación fija en el automóvil en mm. Básicamente, mide la posición del asiento de un conductor determinado. Esta es información potencialmente útil para los fabricantes de automóviles que consideran la comodidad y la seguridad al diseñar vehículos. Intentaremos ajustar un modelo que prediga hipcenter. Dos variables predictoras son interesantes para nosotros: HtShoes y Ht. Ciertamente, esperamos que la altura de una persona esté altamente correlacionada con su altura cuando usa zapatos. Prestaremos especial atención a estas dos variables al ajustar modelos. library(faraway) pairs(seatpos, col = &quot;dodgerblue&quot;) round(cor(seatpos), 2) ## Age Weight HtShoes Ht Seated Arm Thigh Leg hipcenter ## Age 1.00 0.08 -0.08 -0.09 -0.17 0.36 0.09 -0.04 0.21 ## Weight 0.08 1.00 0.83 0.83 0.78 0.70 0.57 0.78 -0.64 ## HtShoes -0.08 0.83 1.00 1.00 0.93 0.75 0.72 0.91 -0.80 ## Ht -0.09 0.83 1.00 1.00 0.93 0.75 0.73 0.91 -0.80 ## Seated -0.17 0.78 0.93 0.93 1.00 0.63 0.61 0.81 -0.73 ## Arm 0.36 0.70 0.75 0.75 0.63 1.00 0.67 0.75 -0.59 ## Thigh 0.09 0.57 0.72 0.73 0.61 0.67 1.00 0.65 -0.59 ## Leg -0.04 0.78 0.91 0.91 0.81 0.75 0.65 1.00 -0.79 ## hipcenter 0.21 -0.64 -0.80 -0.80 -0.73 -0.59 -0.59 -0.79 1.00 Después de cargar el paquete faraway, realizamos algunas comprobaciones rápidas de correlación entre los predictores. Visualmente, podemos hacer esto con la función pairs(), que traza todos los diagramas de dispersión posibles entre pares de variables en el conjunto de datos. También podemos hacer esto numéricamente con la función cor(), que cuando se aplica a un conjunto de datos, devuelve todas las correlaciones por pares. Observe que esta es una matriz simétrica. Recuerde que la correlación mide la fuerza y la dirección de la relación lineal entre las variables. La correlación entre Ht y HtShoes es extremadamente alta. Tan alta, que redondeado a dos cifras decimales, ¡parece ser 1! A diferencia de la colinealidad exacta, aquí todavía podemos ajustar un modelo con todos los predictores, pero ¿qué efecto tiene esto? hip_model = lm(hipcenter ~ ., data = seatpos) summary(hip_model) ## ## Call: ## lm(formula = hipcenter ~ ., data = seatpos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -73.827 -22.833 -3.678 25.017 62.337 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 436.43213 166.57162 2.620 0.0138 * ## Age 0.77572 0.57033 1.360 0.1843 ## Weight 0.02631 0.33097 0.080 0.9372 ## HtShoes -2.69241 9.75304 -0.276 0.7845 ## Ht 0.60134 10.12987 0.059 0.9531 ## Seated 0.53375 3.76189 0.142 0.8882 ## Arm -1.32807 3.90020 -0.341 0.7359 ## Thigh -1.14312 2.66002 -0.430 0.6706 ## Leg -6.43905 4.71386 -1.366 0.1824 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 37.72 on 29 degrees of freedom ## Multiple R-squared: 0.6866, Adjusted R-squared: 0.6001 ## F-statistic: 7.94 on 8 and 29 DF, p-value: 1.306e-05 Una de las primeras cosas que debemos notar es que la prueba \\(F\\) para la regresión nos dice que la regresión es significativa, sin embargo, cada predictor individual no lo es. Otro resultado interesante son los signos opuestos de los coeficientes para Ht y HtShoes. Esto debería parecer bastante contrario a la intuición. ¿Aumentar Ht aumenta hipcenter, pero aumentar HtShoes disminuyehipcenter? Esto sucede como resultado de que los predictores están altamente correlacionados. Por ejemplo, la variable HtShoes explica gran parte de la variación en Ht. Cuando ambos están en el modelo, sus efectos sobre la respuesta disminuyen individualmente, pero juntos todavía explican una gran parte de la variación de hipcenter. Definimos \\(R_j^2\\) como la proporción de variación observada en el predictor \\(j\\)-ésimo explicada por los otros predictores. En otras palabras, \\(R_j^2\\) es el R-Cuadrado múltiple para la regresión de \\(x_j\\) en cada uno de los otros predictores. ht_shoes_model = lm(HtShoes ~ . - hipcenter, data = seatpos) summary(ht_shoes_model)$r.squared ## [1] 0.9967472 Aquí vemos que los otros predictores explican $99,67% $ de la variación en HtShoe. Al ajustar este modelo, eliminamos hipcenter ya que no es un predictor. 15.2.1 Factor de inflación de la varianza. Ahora tenga en cuenta que la varianza de \\(\\hat{\\beta_j}\\) se puede escribir como \\[ \\text{Var}(\\hat{\\beta_j}) = \\sigma^2 C_{jj} = \\sigma^2 \\left( \\frac{1}{1 - R_j^2} \\right) \\frac{1}{S_{x_j x_j}} \\] donde \\[ S_{x_j x_j} = \\sum(x_{ij}-\\bar{x}_j)^2. \\] Esto nos da una forma de comprender cómo afecta la colinealidad a nuestras estimaciones de regresión. llamaremos, \\[ \\frac{1}{1 - R_j^2} \\] el factor de inflación de la varianza. El factor de inflación de la varianza cuantifica el efecto de la colinealidad sobre la varianza de nuestras estimaciones de regresión. Cuando \\(R_j^2\\) es grande, es cercano a 1, \\(x_j\\) está bien explicado por los otros predictores. Con un \\(R_j^2\\) grande, el factor de inflación de la varianza se vuelve grande. Esto nos dice que cuando \\(x_j\\) está altamente correlacionado con otros predictores, nuestra estimación de \\(\\beta_j\\) es muy variable. La función vif del paquete faraway calcula los VIF para cada uno de los predictores de un modelo. vif(hip_model) ## Age Weight HtShoes Ht Seated Arm Thigh ## 1.997931 3.647030 307.429378 333.137832 8.951054 4.496368 2.762886 ## Leg ## 6.694291 En la práctica, es común decir que cualquier VIF superior a \\(5\\) es motivo de preocupación. Entonces, en este ejemplo, vemos que hay un gran problema de multicolinealidad, ya que muchos de los predictores tienen un VIF mayor a 5. Investiguemos más a fondo cómo la presencia de colinealidad afecta realmente a un modelo. Si agregamos una cantidad moderada de ruido a los datos, vemos que las estimaciones de los coeficientes cambian drásticamente. Este es un efecto bastante indeseable. Agregar ruido aleatorio no debería afectar los coeficientes de un modelo. set.seed(1337) noise = rnorm(n = nrow(seatpos), mean = 0, sd = 5) hip_model_noise = lm(hipcenter + noise ~ ., data = seatpos) La adición del ruido tuvo un efecto tan grande que el signo del coeficiente de Ht ha cambiado. coef(hip_model) ## (Intercept) Age Weight HtShoes Ht Seated ## 436.43212823 0.77571620 0.02631308 -2.69240774 0.60134458 0.53375170 ## Arm Thigh Leg ## -1.32806864 -1.14311888 -6.43904627 coef(hip_model_noise) ## (Intercept) Age Weight HtShoes Ht Seated ## 415.32909380 0.76578240 0.01910958 -2.90377584 -0.12068122 2.03241638 ## Arm Thigh Leg ## -1.02127944 -0.89034509 -5.61777220 Esto nos dice que un modelo con colinealidad es malo para explicar la relación entre la respuesta y los predictores. Ni siquiera podemos tener confianza en la dirección de la relación. Sin embargo, ¿la colinealidad afecta la predicción? plot(fitted(hip_model), fitted(hip_model_noise), col = &quot;dodgerblue&quot;, pch = 20, xlab = &quot;Predicho, sin ruido&quot;, ylab = &quot;Predicho, con ruido&quot;, cex = 1.5) abline(a = 0, b = 1, col = &quot;darkorange&quot;, lwd = 2) Vemos que al graficar los valores predichos usando ambos modelos uno contra el otro, en realidad son bastante similares. Veamos ahora un modelo más pequeño, hip_model_small = lm(hipcenter ~ Age + Arm + Ht, data = seatpos) summary(hip_model_small) ## ## Call: ## lm(formula = hipcenter ~ Age + Arm + Ht, data = seatpos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -82.347 -24.745 -0.094 23.555 58.314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 493.2491 101.0724 4.880 2.46e-05 *** ## Age 0.7988 0.5111 1.563 0.12735 ## Arm -2.9385 3.5210 -0.835 0.40979 ## Ht -3.4991 0.9954 -3.515 0.00127 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 36.12 on 34 degrees of freedom ## Multiple R-squared: 0.6631, Adjusted R-squared: 0.6333 ## F-statistic: 22.3 on 3 and 34 DF, p-value: 3.649e-08 vif(hip_model_small) ## Age Arm Ht ## 1.749943 3.996766 3.508693 Inmediatamente vemos que aquí la multicolinealidad no es un problema. anova(hip_model_small, hip_model) ## Analysis of Variance Table ## ## Model 1: hipcenter ~ Age + Arm + Ht ## Model 2: hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + ## Leg ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 34 44354 ## 2 29 41262 5 3091.9 0.4346 0.8207 También observe que usando una prueba \\(F\\) para comparar los dos modelos, preferiríamos el modelo más pequeño. Ahora investigamos el efecto de agregar otra variable a este modelo más pequeño. Específicamente, queremos ver la adición de la variable HtShoes. Así que ahora nuestros posibles predictores son HtShoes, Age, Arm y Ht. Nuestra respuesta sigue siendo hipcenter. Para cuantificar este efecto, veremos un gráfico de variable agregada y un coeficiente de correlación parcial. Para ambos, veremos los residuos de los modelos: Hacer una regresión de la respuesta (hipcenter) frente a todos los predictores excepto el predictor de interés (HtShoes). la regresión el predictor de interés (HtShoes) frente a los otros predictores (Age, Arm y Ht). ht_shoes_model_small = lm(HtShoes ~ Age + Arm + Ht, data = seatpos) Así que ahora, los residuos de hip_model_small nos dan la variación de hipcenter que es inexplicable por Age, Arm y Ht. De manera similar, los residuos de ht_shoes_model_small nos dan la variación de HtShoes sin explicación por Age, Arm y Ht. La correlación de estos dos residuos nos da el coeficiente de correlación parcial de HtShoes y hipcenter con los efectos de Age, Arm y Ht eliminados. cor(resid(ht_shoes_model_small), resid(hip_model_small)) ## [1] -0.03311061 Dado que este valor es pequeño, cercano a cero, significa que la variación de hipcenter que no se explica porAge, Arm yHt muestra muy poca correlación con la variación de HtShoes que no se explica por Age, Arm, y Ht. Por lo tanto, agregar HtShoes al modelo probablemente sería de poco beneficio. De manera similar, una gráfica de variable agregada visualiza estos residuos entre sí. También es útil hacer una regresión de los residuos de la respuesta frente a los residuos del predictor y agregar la línea de regresión al gráfico. plot(resid(hip_model_small) ~ resid(ht_shoes_model_small), col = &quot;dodgerblue&quot;, pch = 20, xlab = &quot;Residuales, Predictor agregado&quot;, ylab = &quot;Residuales, Modelo original&quot;) abline(h = 0, lty = 2) abline(v = 0, lty = 2) abline(lm(resid(hip_model_small) ~ resid(ht_shoes_model_small)), col = &quot;darkorange&quot;, lwd = 2) Aquí la gráfica agregada de variables no muestra casi ninguna relación lineal. Esto nos dice que agregar HtShoes al modelo probablemente no valdría la pena. Dado que su variación se explica en gran medida por los otros predictores, agregarlo al modelo no hará mucho por mejorarlo. Sin embargo, aumentará la variación de las estimaciones y hará que el modelo sea mucho más difícil de interpretar. Si hubiera habido una relación lineal fuerte, por lo tanto, un coeficiente de correlación parcial grande, probablemente hubiera sido útil agregar el predictor adicional al modelo. Esta compensación es mayoritariamente cierta en general. A medida que un modelo obtiene más predictores, los errores se harán más pequeños y su predicción será mejor, pero será más difícil de interpretar. Por eso, si estamos interesados en explicar la relación entre los predictores y la respuesta, a menudo queremos un modelo que se ajuste bien, pero con una pequeña cantidad de predictores con poca correlación. En el próximo capítulo, aprenderemos sobre métodos para encontrar modelos que se ajusten bien, pero que también tengan una pequeña cantidad de predictores. También discutiremos sobreajuste. Aunque agregar predictores adicionales siempre hará que los errores sean más pequeños, a veces ajustaremos el ruido y dicho modelo no se generalizará bien a observaciones adicionales. 15.3 Simulación Aquí simulamos datos de ejemplo con y sin colinealidad. Notaremos la diferencia en la distribución de las estimaciones de los parámetros \\(\\beta\\), en particular su varianza. Sin embargo, también notaremos la similitud en sus \\(MSE\\). Usaremos el modelo, \\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\] donde \\(\\epsilon \\sim N(\\mu = 0, \\sigma^2 = 25)\\) y los coeficientes \\(\\beta\\) se definen a continuación. set.seed(42) beta_0 = 7 beta_1 = 3 beta_2 = 4 sigma = 5 Usaremos un tamaño de muestra de 10 y 2500 simulaciones para ambas situaciones. sample_size = 10 num_sim = 2500 Primero consideraremos la situación con un problema de colinealidad, por lo que creamos manualmente las dos variables predictoras. x1 = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) x2 = c(1, 2, 3, 4, 5, 7, 6, 10, 9, 8) c(sd(x1), sd(x2)) ## [1] 3.02765 3.02765 cor(x1, x2) ## [1] 0.9393939 Observe que tienen una correlación extremadamente alta. true_line_bad = beta_0 + beta_1 * x1 + beta_2 * x2 beta_hat_bad = matrix(0, num_sim, 2) mse_bad = rep(0, num_sim) Realizamos la simulación 2500 veces, cada vez ajustando un modelo de regresión, y almacenando los coeficientes estimados y el MSE. for (s in 1:num_sim) { y = true_line_bad + rnorm(n = sample_size, mean = 0, sd = sigma) reg_out = lm(y ~ x1 + x2) beta_hat_bad[s, ] = coef(reg_out)[-1] mse_bad[s] = mean(resid(reg_out) ^ 2) } Ahora pasamos a la situación sin problema de colinealidad, por lo que nuevamente creamos manualmente las dos variables predictoras. z1 = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) z2 = c(9, 2, 7, 4, 5, 6, 3, 8, 1, 10) Observe que las desviaciones estándar de cada uno son las mismas que antes, sin embargo, ahora la correlación es extremadamente cercana a 0. c(sd(z1), sd(z2)) ## [1] 3.02765 3.02765 cor(z1, z2) ## [1] 0.03030303 true_line_good = beta_0 + beta_1 * z1 + beta_2 * z2 beta_hat_good = matrix(0, num_sim, 2) mse_good = rep(0, num_sim) Luego realizamos simulaciones y almacenamos los mismos resultados. for (s in 1:num_sim) { y = true_line_good + rnorm(n = sample_size, mean = 0, sd = sigma) reg_out = lm(y ~ z1 + z2) beta_hat_good[s, ] = coef(reg_out)[-1] mse_good[s] = mean(resid(reg_out) ^ 2) } Ahora investigaremos las diferencias. par(mfrow = c(1, 2)) hist(beta_hat_bad[, 1], col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, main = expression(&quot;Histograma de &quot; *hat(beta)[1]* &quot; con colinealidad&quot;), xlab = expression(hat(beta)[1]), breaks = 20) hist(beta_hat_good[, 1], col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, main = expression(&quot;Histograma de &quot; *hat(beta)[1]* &quot; sin colinealidad&quot;), xlab = expression(hat(beta)[1]), breaks = 20) Primero, para \\(\\beta_1\\), que tiene un valor real de \\(3\\), vemos que tanto con colinealidad como sin ella, los valores simulados están centrados cerca de \\(3\\). mean(beta_hat_bad[, 1]) ## [1] 2.963325 mean(beta_hat_good[, 1]) ## [1] 3.013414 La forma en que se crearon los predictores, la \\(S_{x_j x_j}\\) porción de la varianza es la misma para los predictores en ambos casos, pero la varianza es aún mucho mayor en las simulaciones realizadas con colinealidad. La varianza es tan grande en el caso colineal, que a veces el coeficiente estimado para \\(\\beta_1\\) es negativo. sd(beta_hat_bad[, 1]) ## [1] 1.633294 sd(beta_hat_good[, 1]) ## [1] 0.5484684 par(mfrow = c(1, 2)) hist(beta_hat_bad[, 2], col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, main = expression(&quot;Histograma de &quot; *hat(beta)[2]* &quot; con colinealidad&quot;), xlab = expression(hat(beta)[2]), breaks = 20) hist(beta_hat_good[, 2], col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, main = expression(&quot;Histograma de &quot; *hat(beta)[2]* &quot; sin colinealidad&quot;), xlab = expression(hat(beta)[2]), breaks = 20) Vemos los mismos problemas con \\(\\beta_2\\). En promedio, las estimaciones son correctas, pero la varianza es nuevamente mucho mayor con la colinealidad. mean(beta_hat_bad[, 2]) ## [1] 4.025059 mean(beta_hat_good[, 2]) ## [1] 4.004913 sd(beta_hat_bad[, 2]) ## [1] 1.642592 sd(beta_hat_good[, 2]) ## [1] 0.5470381 par(mfrow = c(1, 2)) hist(mse_bad, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, main = &quot;MSE, con colinealidad&quot;, xlab = &quot;MSE&quot;) hist(mse_good, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;, main = &quot;MSE, sin colinealidad&quot;, xlab = &quot;MSE&quot;) Curiosamente, en ambos casos, el MSE es aproximadamente el mismo en promedio. Nuevamente, esto se debe a que la colinealidad afecta la capacidad de un modelo para explicar, pero no predecir. mean(mse_bad) ## [1] 17.7186 mean(mse_good) ## [1] 17.70513 "],["selección-de-variables-y-construcción-de-modelos.html", "Capítulo 16 Selección de variables y construcción de modelos 16.1 Criterio de calidad 16.2 Procedimientos de selección 16.3 Términos de orden superior 16.4 Explicación versus predicción", " Capítulo 16 Selección de variables y construcción de modelos Elige bien. Tu elección es breve y, sin embargo, interminable.  Johann Wolfgang von Goethe Después de leer este capítulo, podrá: Comprender el equilibrio entre bondad de ajuste y complejidad del modelo. Utilizar procedimientos de selección de variables para encontrar un buen modelo a partir de un conjunto de modelos posibles. Comprender los dos usos de los modelos: explicación y predicción. En el capítulo anterior vimos cómo la correlación entre las variables predictoras puede tener efectos indeseables en los modelos. Usamos factores de inflación de la varianza para evaluar la gravedad de los problemas de colinealidad causados por estas correlaciones. También vimos cómo ajustar un modelo más pequeño, dejando de lado algunos de los predictores correlacionados, da como resultado un modelo que ya no sufre problemas de colinealidad. Pero, ¿cómo elegir este modelo más pequeño? En este capítulo, discutiremos varios criterios y procedimientos para elegir un modelo bueno de entre muchos. 16.1 Criterio de calidad Hasta ahora, hemos visto criterios como el \\(R^2\\) y \\(\\text{RMSE}\\) para evaluar la calidad de ajuste. Sin embargo, ambos tienen un defecto fatal. Al aumentar el tamaño de un modelo, se agregan predictores que, en el peor de los casos, no pueden mejorar. Es imposible agregar un predictor a un modelo y empeorar el \\(R^2\\) o \\(\\text{RMSE}\\). Eso significa que, si usáramos cualquiera de estos para elegir entre modelos, siempre simplemente elegiríamos el modelo más grande. Con el tiempo, simplemente estaríamos adaptados al ruido. Esto sugiere que necesitamos un criterio de calidad que tenga en cuenta el tamaño del modelo, ya que nuestra preferencia es por modelos pequeños que encajan bien. Estamos dispuestos a sacrificar una pequeña cantidad de bondad de ajuste para obtener un modelo más pequeño. (Aquí usamos bondad de ajuste para significar simplemente qué tan lejos están los datos del modelo, cuanto más pequeños sean los errores, mejor. A menudo, en la estadística, bondad de ajuste puede tener un significado más preciso), en tres criterios que hacen esto explícitamente: \\(\\text{AIC}\\), \\(\\text{BIC}\\) y \\(R^2\\) ajustados. También veremos uno, \\(\\text{RMSE}\\) con validación cruzada, que considera implícitamente el tamaño del modelo. 16.1.1 Criterio de información de Akaike El primer criterio que discutiremos es el Criterio de información de Akaike, o \\(\\text{AIC}\\) para abreviar. (Tenga en cuenta que, cuando Akaike introdujo por primera vez esta métrica, simplemente se llamó Un Criterio de información. Recuerde, la log-verosimilitud maximizada de un modelo de regresión se puede escribir como \\[ \\log L(\\boldsymbol{\\hat{\\beta}}, \\hat{\\sigma}^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\text{RSS}}{n}\\right) - \\frac{n}{2}, \\] donde \\(\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i) ^ 2\\), \\(\\boldsymbol{\\hat{\\beta}}\\) y \\(\\hat{\\sigma}^2\\) fueron elegidos para maximizar la verosimilitud. Entonces podemos definir \\(\\text{AIC}\\) como \\[ \\text{AIC} = -2 \\log L(\\boldsymbol{\\hat{\\beta}}, \\hat{\\sigma}^2) + 2p = n + n \\log(2\\pi) + n \\log\\left(\\frac{\\text{RSS}}{n}\\right) + 2p, \\] que es una medida de calidad del modelo. Cuanto menor sea el \\(\\text{AIC}\\), mejor. Para ver por qué, hablemos de los dos componentes principales de \\(\\text{AIC}\\), la verosimilitud (que mide la bondad de ajuste) y la penalización (que es una función de el tamaño del modelo). La parte de verosimilitud de \\(\\text{AIC}\\) es dada por \\[ -2 \\log L(\\boldsymbol{\\hat{\\beta}}, \\hat{\\sigma}^2) = n + n \\log(2\\pi) + n \\log\\left(\\frac{\\text{RSS}}{n}\\right). \\] Para comparar modelos, el único término que cambiará es \\(n \\log\\left(\\frac{\\text{RSS}}{n}\\right)\\), que es función de \\(\\text{RSS}\\). \\[ n + n \\log(2\\pi) \\] los términos serán constantes en todos los modelos aplicados a los mismos datos. Entonces, cuando un modelo se ajusta bien, es decir, tiene un \\(\\text{RSS}\\) bajo, entonces este componente de verosimilitud será pequeño. De manera similar, podemos discutir el componente de penalización de \\(\\text{AIC}\\) que es, \\[ 2p, \\] donde \\(p\\) es el número de parámetros \\(\\beta\\) en el modelo. A esto lo llamamos penalización, porque es grande cuando \\(p\\) es grande, pero buscamos encontrar un \\(\\text{AIC}\\) pequeño. Por lo tanto, un buen modelo, es decir, uno con un \\(\\text{AIC}\\) pequeño, tendrá un buen equilibrio entre un buen ajuste y el uso de una pequeña cantidad de parámetros. Para comparar modelos \\[ \\text{AIC} = n\\log\\left(\\frac{\\text{RSS}}{n}\\right) + 2p \\] es una expresión suficiente, ya que \\(n + n \\log(2\\pi)\\) es el mismo en todos los modelos para cualquier conjunto de datos en particular. 16.1.2 Criterio de información Bayesiano El criterio de información Bayesiano, o \\(\\text{BIC}\\), es similar a \\(\\text{AIC}\\), pero tiene una penalización mayor. \\(\\text{BIC}\\) también cuantifica la compensación entre un modelo que se ajusta bien y el número de parámetros del modelo, sin embargo, para un tamaño de muestra razonable, generalmente elige un modelo más pequeño que \\(\\text{AIC}\\). Nuevamente, para la selección del modelo, use el modelo con el \\(\\text{BIC}\\) más pequeño. \\[ \\text{BIC} = -2 \\log L(\\boldsymbol{\\hat{\\beta}}, \\hat{\\sigma}^2) + \\log(n) p = n + n\\log(2\\pi) + n\\log\\left(\\frac{\\text{RSS}}{n}\\right) + \\log(n)p. \\] Observe que la penalización de \\(\\text{AIC}\\) fue \\[ 2p, \\] mientras que para \\(\\text{BIC}\\), la penalización es \\[ \\log(n) p. \\] Entonces, para cualquier conjunto de datos donde \\(log(n)&gt; 2\\), la penalización de \\(\\text{BIC}\\) será mayor que la penalización de \\(\\text{AIC}\\), por lo que \\(\\text{BIC}\\) probablemente preferirá un modelo pequeño. Tenga en cuenta que, a veces, la penalización se considera una expresión general de la forma \\[ k \\cdot p. \\] Entonces, para \\(\\text{AIC}\\) \\(k=2\\), y para \\(\\text{BIC}\\) \\(k=\\log(n)\\). Para comparar modelos \\[ \\text{BIC} = n\\log\\left(\\frac{\\text{RSS}}{n}\\right) + \\log(n)p \\] es nuevamente una expresión suficiente, ya que \\(n + n \\log(2\\pi)\\) es el mismo en todos los modelos para cualquier conjunto de datos en particular. 16.1.3 R cuadrado ajustado Recordemos que, \\[ R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}. \\] Ahora definimos \\[ R_a^2 = 1 - \\frac{\\text{SSE}/(n-p)}{\\text{SST}/(n-1)} = 1 - \\left( \\frac{n-1}{n-p} \\right)(1-R^2) \\] Lo que llamamos el \\(R^2\\) ajustado. A diferencia del \\(R^2\\), que nunca puede reducirse con predictores agregados, el \\(R^2\\) ajustado penaliza efectivamente los predictores adicionales y puede disminuir con predictores adicionales. Al igual que el \\(R^2\\), cuanto más grande, mejor. 16.1.4 RMSE con validación cruzada Cada una de las tres métricas anteriores utilizó explícitamente \\(p\\), el número de parámetros, en sus cálculos. Por lo tanto, todos ellos limitan explícitamente el tamaño de los modelos elegidos cuando se utilizan para comparar modelos. Ahora presentaremos brevemente sobreajuste y validación cruzada. make_poly_data = function(sample_size = 11) { x = seq(0, 10) y = 3 + x + 4 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 20) data.frame(x, y) } set.seed(1234) poly_data = make_poly_data() Aquí hemos generado datos donde la media de \\(Y\\) es una función cuadrática de un solo predictor \\(x\\), específicamente, \\[ Y = 3 + x + 4 x ^ 2 + \\epsilon. \\] Ahora ajustaremos dos modelos a estos datos, uno que tiene la forma correcta, cuadrática, y otro que es grande, que incluye términos hasta un octavo grado inclusive. fit_quad = lm(y ~ poly(x, degree = 2), data = poly_data) fit_big = lm(y ~ poly(x, degree = 8), data = poly_data) Luego graficamos los datos y los resultados de los dos modelos. plot(y ~ x, data = poly_data, ylim = c(-100, 400), cex = 2, pch = 20) xplot = seq(0, 10, by = 0.1) lines(xplot, predict(fit_quad, newdata = data.frame(x = xplot)), col = &quot;dodgerblue&quot;, lwd = 2, lty = 1) lines(xplot, predict(fit_big, newdata = data.frame(x = xplot)), col = &quot;darkorange&quot;, lwd = 2, lty = 2) Podemos ver que la curva azul sólida modela estos datos bastante bien. La curva naranja discontinua se ajusta mejor a los puntos y comete errores más pequeños; sin embargo, es poco probable que esté modelando correctamente la verdadera relación entre \\(x\\) y \\(y\\). Se ajusta al ruido aleatorio. Este es un ejemplo de sobreajuste. Vemos que el modelo más grande de hecho tiene un menor \\(\\text{RMSE}\\). sqrt(mean(resid(fit_quad) ^ 2)) ## [1] 17.61812 sqrt(mean(resid(fit_big) ^ 2)) ## [1] 10.4197 Para corregir esto, introduciremos la validación cruzada. Definimos el RMSE de validación cruzada leave-one-out como \\[ \\text{RMSE}_{\\text{LOOCV}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n e_{[i]}^2}. \\] Los \\(e_{[i]}\\) son el residuo de la \\(i\\)-ésima observación, cuando esa observación no se utiliza para ajustar el modelo. \\[ e_{[i]} = y_{i} - \\hat{y}_{[i]} \\] Es decir, el valor ajustado se calcula como \\[ \\hat{y}_{[i]} = \\boldsymbol{x}_i ^ \\top \\hat{\\beta}_{[i]} \\] donde \\(\\hat{\\beta}_{[i]}\\) son los coeficientes estimados cuando la \\(i\\)-ésima observación se elimina del conjunto de datos. En general, para realizar este cálculo, tendríamos que ajustar el modelo \\(n\\) veces, una vez con cada posible observación eliminada. Sin embargo, para modelos lineales y de validación cruzada leave-one-out, la ecuación se puede reescribir como \\[ \\text{RMSE}_{\\text{LOOCV}} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{e_{i}}{1-h_{i}}\\right)^2}, \\] donde \\(h_i\\) son los apalancamientos y \\(e_i\\) son los residuos habituales. ¡Esto es genial, porque ahora podemos obtener LOOCV \\(\\text{RMSE}\\) ajustando solo un modelo! En la práctica, la validación cruzada de 5 o 10 veces es mucho más popular. Por ejemplo, en la validación cruzada de 5 veces, el modelo se ajusta 5 veces, cada vez se omite una quinta parte de los datos y luego se predice sobre esos valores. Dejaremos el examen en profundidad de la validación cruzada a un curso de aprendizaje automático y simplemente usaremos LOOCV aquí. Calculemos LOOCV \\(\\text{RMSE}\\) para ambos modelos, luego analicemos por qué queremos hacerlo. Primero escribimos una función que calcula LOOCV \\(\\text{RMSE}\\) como se define usando la fórmula de acceso directo para modelos lineales. calc_loocv_rmse = function(model) { sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2)) } Luego calcule la métrica para ambos modelos. calc_loocv_rmse(fit_quad) ## [1] 23.57189 calc_loocv_rmse(fit_big) ## [1] 1334.357 Ahora vemos que el modelo cuadrático tiene un LOOCV \\(\\text{RMSE}\\) mucho más pequeño, por lo que preferiríamos este modelo. Esto se debe a que el modelo grande ha sobreajustado gravemente los datos. Al dejar un solo punto de datos y ajustar el modelo grande, el ajuste resultante es muy diferente al ajuste utilizando todos los datos. Por ejemplo, dejemos fuera el tercer punto de datos y ajustemos ambos modelos, luego tracemos el resultado. fit_quad_removed = lm(y ~ poly(x, degree = 2), data = poly_data[-3, ]) fit_big_removed = lm(y ~ poly(x, degree = 8), data = poly_data[-3, ]) plot(y ~ x, data = poly_data, ylim = c(-100, 400), cex = 2, pch = 20) xplot = seq(0, 10, by = 0.1) lines(xplot, predict(fit_quad_removed, newdata = data.frame(x = xplot)), col = &quot;dodgerblue&quot;, lwd = 2, lty = 1) lines(xplot, predict(fit_big_removed, newdata = data.frame(x = xplot)), col = &quot;darkorange&quot;, lwd = 2, lty = 2) Vemos que, en promedio, la línea azul sólida para el modelo cuadrático tiene errores similares a los de antes. Ha cambiado muy levemente. Sin embargo, la línea naranja discontinua para el modelo grande tiene un gran error en el punto que se eliminó y es muy diferente al ajuste anterior. Este es el propósito de la validación cruzada. Al evaluar cómo el modelo se ajusta a los puntos que no se utilizaron para realizar la regresión, tenemos una idea de qué tan bien funcionará el modelo para las observaciones futuras. Evalúa qué tan bien funciona el modelo en general, no simplemente en los datos observados. 16.2 Procedimientos de selección Ahora hemos visto una serie de criterios de calidad del modelo, pero ahora debemos abordar qué modelos considerar. La selección del modelo implica tanto un criterio de calidad como un procedimiento de búsqueda. library(faraway) hipcenter_mod = lm(hipcenter ~ ., data = seatpos) coef(hipcenter_mod) ## (Intercept) Age Weight HtShoes Ht Seated ## 436.43212823 0.77571620 0.02631308 -2.69240774 0.60134458 0.53375170 ## Arm Thigh Leg ## -1.32806864 -1.14311888 -6.43904627 Regresemos a los datos de seatpos del paquete faraway. Ahora, consideremos solo modelos con términos de primer orden, por lo tanto, sin interacciones ni polinomios. Hay ocho predictores en este modelo. Entonces, si consideramos todos los modelos posibles, que van desde el uso de 0 predictores hasta los ocho predictores, hay \\[ \\sum_{k = 0}^{p - 1} {{p - 1} \\choose {k}} = 2 ^ {p - 1} = 2 ^ 8 = 256 \\] posibles modelos. Si tuviéramos 10 o más predictores, ¡ya estaríamos considerando más de 1000 modelos! Por esta razón, a menudo buscamos a través de posibles modelos de una manera inteligente, pasando por alto algunos modelos que es poco probable que se consideren buenos. Consideraremos tres procedimientos de búsqueda: hacia atrás, hacia adelante y paso a paso. 16.2.1 Búsqueda hacia atrás (Backward) Los procedimientos de selección hacia atrás comienzan con todos los posibles predictores en el modelo, luego consideran cómo la eliminación de un solo predictor afectará una métrica elegida. Probemos esto con los datos seatpos. Usaremos la función step() en R que por defecto usa \\(\\text{AIC}\\) como su métrica de elección. hipcenter_mod_back_aic = step(hipcenter_mod, direction = &quot;backward&quot;) ## Start: AIC=283.62 ## hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + ## Leg ## ## Df Sum of Sq RSS AIC ## - Ht 1 5.01 41267 281.63 ## - Weight 1 8.99 41271 281.63 ## - Seated 1 28.64 41290 281.65 ## - HtShoes 1 108.43 41370 281.72 ## - Arm 1 164.97 41427 281.78 ## - Thigh 1 262.76 41525 281.87 ## &lt;none&gt; 41262 283.62 ## - Age 1 2632.12 43894 283.97 ## - Leg 1 2654.85 43917 283.99 ## ## Step: AIC=281.63 ## hipcenter ~ Age + Weight + HtShoes + Seated + Arm + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Weight 1 11.10 41278 279.64 ## - Seated 1 30.52 41297 279.66 ## - Arm 1 160.50 41427 279.78 ## - Thigh 1 269.08 41536 279.88 ## - HtShoes 1 971.84 42239 280.51 ## &lt;none&gt; 41267 281.63 ## - Leg 1 2664.65 43931 282.01 ## - Age 1 2808.52 44075 282.13 ## ## Step: AIC=279.64 ## hipcenter ~ Age + HtShoes + Seated + Arm + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Seated 1 35.10 41313 277.67 ## - Arm 1 156.47 41434 277.78 ## - Thigh 1 285.16 41563 277.90 ## - HtShoes 1 975.48 42253 278.53 ## &lt;none&gt; 41278 279.64 ## - Leg 1 2661.39 43939 280.01 ## - Age 1 3011.86 44290 280.31 ## ## Step: AIC=277.67 ## hipcenter ~ Age + HtShoes + Arm + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Arm 1 172.02 41485 275.83 ## - Thigh 1 344.61 41658 275.99 ## - HtShoes 1 1853.43 43166 277.34 ## &lt;none&gt; 41313 277.67 ## - Leg 1 2871.07 44184 278.22 ## - Age 1 2976.77 44290 278.31 ## ## Step: AIC=275.83 ## hipcenter ~ Age + HtShoes + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Thigh 1 472.8 41958 274.26 ## &lt;none&gt; 41485 275.83 ## - HtShoes 1 2340.7 43826 275.92 ## - Age 1 3501.0 44986 276.91 ## - Leg 1 3591.7 45077 276.98 ## ## Step: AIC=274.26 ## hipcenter ~ Age + HtShoes + Leg ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 41958 274.26 ## - Age 1 3108.8 45067 274.98 ## - Leg 1 3476.3 45434 275.28 ## - HtShoes 1 4218.6 46176 275.90 Comenzamos con el modelo hipcenter ~ ., que también se conoce como hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg. R intentará repetidamente eliminar un predictor hasta que se detenga o llegue al modelo hipcenter ~ 1, que no contiene predictores. En cada paso, R informa el modelo actual, su \\(\\text{AIC}\\), y los posibles pasos con su \\(\\text{RSS}\\) y, lo que es más importante, \\(\\text{AIC}\\). En este ejemplo, en el primer paso, el modelo actual es hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg que tiene un AIC de 283.62. Tenga en cuenta que cuando R está calculando este valor, está usando extractAIC(), que usa la expresión \\[ \\text{AIC} = n\\log\\left(\\frac{\\text{RSS}}{n}\\right) + 2p, \\] Lo que verificamos rápidamente. extractAIC(hipcenter_mod) # devuelve p y AIC ## [1] 9.000 283.624 n = length(resid(hipcenter_mod)) (p = length(coef(hipcenter_mod))) ## [1] 9 n * log(mean(resid(hipcenter_mod) ^ 2)) + 2 * p ## [1] 283.624 Volviendo al primer paso, R nos da una fila que muestra el efecto de eliminar cada uno de los predictores actuales. Los signos - al principio de cada fila indican que estamos considerando eliminar un predictor. También hay una fila con &lt;none&gt; que es una fila para mantener el modelo actual. Observe que esta fila tiene el \\(\\text{RSS}\\) más pequeño, ya que es el modelo más grande. Vemos que cada fila arriba de &lt;none&gt; tiene un \\(\\text{AIC}\\) más pequeño que la fila de &lt;none&gt; con el que está en la parte superior, Ht, dando el \\(\\text{AIC}\\) más bajo. Por lo tanto, eliminamos Ht del modelo y continuamos el proceso. Observe que, en el segundo paso, comenzamos con el modelo hipcenter ~ Age + Weight + HtShoes + Seated + Arm + Thigh + Leg y la variable Ht ya no se considera. Continuamos el proceso hasta llegar al modelo hipcenter ~ Age + HtShoes + Leg. En este paso, la fila para &lt;none&gt; encabeza la lista, ya que eliminar cualquier variable adicional no mejorará el \\(\\text{AIC}\\). Este es el modelo que está almacenado en hipcenter_mod_back_aic. coef(hipcenter_mod_back_aic) ## (Intercept) Age HtShoes Leg ## 456.2136538 0.5998327 -2.3022555 -6.8297461 También podríamos buscar a través de los posibles modelos hacia atrás usando \\(\\text{BIC}\\). Para hacerlo, usamos nuevamente la función step(), pero ahora especificamos k = log(n), donde n almacena el número de observaciones en los datos. n = length(resid(hipcenter_mod)) hipcenter_mod_back_bic = step(hipcenter_mod, direction = &quot;backward&quot;, k = log(n)) ## Start: AIC=298.36 ## hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + ## Leg ## ## Df Sum of Sq RSS AIC ## - Ht 1 5.01 41267 294.73 ## - Weight 1 8.99 41271 294.73 ## - Seated 1 28.64 41290 294.75 ## - HtShoes 1 108.43 41370 294.82 ## - Arm 1 164.97 41427 294.88 ## - Thigh 1 262.76 41525 294.97 ## - Age 1 2632.12 43894 297.07 ## - Leg 1 2654.85 43917 297.09 ## &lt;none&gt; 41262 298.36 ## ## Step: AIC=294.73 ## hipcenter ~ Age + Weight + HtShoes + Seated + Arm + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Weight 1 11.10 41278 291.10 ## - Seated 1 30.52 41297 291.12 ## - Arm 1 160.50 41427 291.24 ## - Thigh 1 269.08 41536 291.34 ## - HtShoes 1 971.84 42239 291.98 ## - Leg 1 2664.65 43931 293.47 ## - Age 1 2808.52 44075 293.59 ## &lt;none&gt; 41267 294.73 ## ## Step: AIC=291.1 ## hipcenter ~ Age + HtShoes + Seated + Arm + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Seated 1 35.10 41313 287.50 ## - Arm 1 156.47 41434 287.61 ## - Thigh 1 285.16 41563 287.73 ## - HtShoes 1 975.48 42253 288.35 ## - Leg 1 2661.39 43939 289.84 ## - Age 1 3011.86 44290 290.14 ## &lt;none&gt; 41278 291.10 ## ## Step: AIC=287.5 ## hipcenter ~ Age + HtShoes + Arm + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Arm 1 172.02 41485 284.02 ## - Thigh 1 344.61 41658 284.18 ## - HtShoes 1 1853.43 43166 285.53 ## - Leg 1 2871.07 44184 286.41 ## - Age 1 2976.77 44290 286.50 ## &lt;none&gt; 41313 287.50 ## ## Step: AIC=284.02 ## hipcenter ~ Age + HtShoes + Thigh + Leg ## ## Df Sum of Sq RSS AIC ## - Thigh 1 472.8 41958 280.81 ## - HtShoes 1 2340.7 43826 282.46 ## - Age 1 3501.0 44986 283.46 ## - Leg 1 3591.7 45077 283.54 ## &lt;none&gt; 41485 284.02 ## ## Step: AIC=280.81 ## hipcenter ~ Age + HtShoes + Leg ## ## Df Sum of Sq RSS AIC ## - Age 1 3108.8 45067 279.89 ## - Leg 1 3476.3 45434 280.20 ## &lt;none&gt; 41958 280.81 ## - HtShoes 1 4218.6 46176 280.81 ## ## Step: AIC=279.89 ## hipcenter ~ HtShoes + Leg ## ## Df Sum of Sq RSS AIC ## - Leg 1 3038.8 48105 278.73 ## &lt;none&gt; 45067 279.89 ## - HtShoes 1 5004.4 50071 280.25 ## ## Step: AIC=278.73 ## hipcenter ~ HtShoes ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 48105 278.73 ## - HtShoes 1 83534 131639 313.35 El procedimiento es exactamente el mismo, excepto que en cada paso buscamos mejorar \\(\\text{BIC}\\), que R todavía etiqueta \\(\\text{AIC}\\) en la salida. La variable hipcenter_mod_back_bic almacena el modelo elegido por este procedimiento. coef(hipcenter_mod_back_bic) ## (Intercept) HtShoes ## 565.592659 -4.262091 Observamos que este modelo es más pequeño, tiene menos predictores, que el modelo elegido por \\(\\text{AIC}\\), que es lo que esperaríamos. También tenga en cuenta que si bien ambos modelos son diferentes, ninguno usa tanto Ht como HtShoes, que están extremadamente correlacionados. Podemos usar información de la función summary() para comparar sus valores \\(R^2\\) ajustados. Tenga en cuenta que cualquiera de los modelos seleccionados funciona mejor que el modelo completo original. summary(hipcenter_mod)$adj.r.squared ## [1] 0.6000855 summary(hipcenter_mod_back_aic)$adj.r.squared ## [1] 0.6531427 summary(hipcenter_mod_back_bic)$adj.r.squared ## [1] 0.6244149 También podemos calcular el LOOCV \\(\\text{RMSE}\\) para ambos modelos seleccionados, así como el modelo completo. calc_loocv_rmse(hipcenter_mod) ## [1] 44.44564 calc_loocv_rmse(hipcenter_mod_back_aic) ## [1] 37.58473 calc_loocv_rmse(hipcenter_mod_back_bic) ## [1] 37.40564 Vemos que preferiríamos el modelo elegido a través de \\(\\text{BIC}\\) si usamos LOOCV \\(\\text{RMSE}\\) como nuestra métrica. 16.2.2 Búsqueda hacia adelante (Forward) La selección hacia adelante es exactamente lo contrario de la selección hacia atrás. Aquí le decimos a R que comience con un modelo que no utilice predictores, es decir,hipcenter ~ 1, luego, en cada paso, R intentará agregar un predictor hasta que encuentre un buen modelo o alcance hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg. hipcenter_mod_start = lm(hipcenter ~ 1, data = seatpos) hipcenter_mod_forw_aic = step( hipcenter_mod_start, scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, direction = &quot;forward&quot;) ## Start: AIC=311.71 ## hipcenter ~ 1 ## ## Df Sum of Sq RSS AIC ## + Ht 1 84023 47616 275.07 ## + HtShoes 1 83534 48105 275.45 ## + Leg 1 81568 50071 276.98 ## + Seated 1 70392 61247 284.63 ## + Weight 1 53975 77664 293.66 ## + Thigh 1 46010 85629 297.37 ## + Arm 1 45065 86574 297.78 ## &lt;none&gt; 131639 311.71 ## + Age 1 5541 126098 312.07 ## ## Step: AIC=275.07 ## hipcenter ~ Ht ## ## Df Sum of Sq RSS AIC ## + Leg 1 2781.10 44835 274.78 ## &lt;none&gt; 47616 275.07 ## + Age 1 2353.51 45262 275.14 ## + Weight 1 195.86 47420 276.91 ## + Seated 1 101.56 47514 276.99 ## + Arm 1 75.78 47540 277.01 ## + HtShoes 1 25.76 47590 277.05 ## + Thigh 1 4.63 47611 277.06 ## ## Step: AIC=274.78 ## hipcenter ~ Ht + Leg ## ## Df Sum of Sq RSS AIC ## + Age 1 2896.60 41938 274.24 ## &lt;none&gt; 44835 274.78 ## + Arm 1 522.72 44312 276.33 ## + Weight 1 445.10 44390 276.40 ## + HtShoes 1 34.11 44801 276.75 ## + Thigh 1 32.96 44802 276.75 ## + Seated 1 1.12 44834 276.78 ## ## Step: AIC=274.24 ## hipcenter ~ Ht + Leg + Age ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 41938 274.24 ## + Thigh 1 372.71 41565 275.90 ## + Arm 1 257.09 41681 276.01 ## + Seated 1 121.26 41817 276.13 ## + Weight 1 46.83 41891 276.20 ## + HtShoes 1 13.38 41925 276.23 Nuevamente, por defecto, R usa \\(\\text{AIC}\\) como su métrica de calidad cuando se usa la función step(). También tenga en cuenta que ahora las filas comienzan con un + que indica la adición de predictores al modelo actual desde cualquier paso. hipcenter_mod_forw_bic = step( hipcenter_mod_start, scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, direction = &quot;forward&quot;, k = log(n)) ## Start: AIC=313.35 ## hipcenter ~ 1 ## ## Df Sum of Sq RSS AIC ## + Ht 1 84023 47616 278.34 ## + HtShoes 1 83534 48105 278.73 ## + Leg 1 81568 50071 280.25 ## + Seated 1 70392 61247 287.91 ## + Weight 1 53975 77664 296.93 ## + Thigh 1 46010 85629 300.64 ## + Arm 1 45065 86574 301.06 ## &lt;none&gt; 131639 313.35 ## + Age 1 5541 126098 315.35 ## ## Step: AIC=278.34 ## hipcenter ~ Ht ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 47616 278.34 ## + Leg 1 2781.10 44835 279.69 ## + Age 1 2353.51 45262 280.05 ## + Weight 1 195.86 47420 281.82 ## + Seated 1 101.56 47514 281.90 ## + Arm 1 75.78 47540 281.92 ## + HtShoes 1 25.76 47590 281.96 ## + Thigh 1 4.63 47611 281.98 Podemos hacer la misma modificación que la última vez para usar \\(\\text{BIC}\\) con selección hacia adelante. summary(hipcenter_mod)$adj.r.squared ## [1] 0.6000855 summary(hipcenter_mod_forw_aic)$adj.r.squared ## [1] 0.6533055 summary(hipcenter_mod_forw_bic)$adj.r.squared ## [1] 0.6282374 Podemos comparar los dos modelos seleccionados con \\(R^2\\) ajustado así como su LOOCV \\(\\text{RMSE}\\). Los resultados son muy similares a los que utilizan la selección hacia atrás, aunque los modelos no son exactamente iguales. calc_loocv_rmse(hipcenter_mod) ## [1] 44.44564 calc_loocv_rmse(hipcenter_mod_forw_aic) ## [1] 37.62516 calc_loocv_rmse(hipcenter_mod_forw_bic) ## [1] 37.2511 16.2.3 Búsqueda por pasos (Stepwise) Las comprobaciones de búsqueda por pasos van hacia atrás y hacia adelante en cada paso. Considera la adición de cualquier variable que no esté actualmente en el modelo, así como la eliminación de cualquier variable que esté actualmente en el modelo. Aquí realizamos una búsqueda paso a paso usando \\(\\text{AIC}\\) como nuestra métrica. Comenzamos con el modelo hipcenter ~ 1 y buscamos hasta hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg. Observe que en muchos de los pasos, algunas filas comienzan con -, mientras que otras comienzan con+. hipcenter_mod_both_aic = step( hipcenter_mod_start, scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, direction = &quot;both&quot;) ## Start: AIC=311.71 ## hipcenter ~ 1 ## ## Df Sum of Sq RSS AIC ## + Ht 1 84023 47616 275.07 ## + HtShoes 1 83534 48105 275.45 ## + Leg 1 81568 50071 276.98 ## + Seated 1 70392 61247 284.63 ## + Weight 1 53975 77664 293.66 ## + Thigh 1 46010 85629 297.37 ## + Arm 1 45065 86574 297.78 ## &lt;none&gt; 131639 311.71 ## + Age 1 5541 126098 312.07 ## ## Step: AIC=275.07 ## hipcenter ~ Ht ## ## Df Sum of Sq RSS AIC ## + Leg 1 2781 44835 274.78 ## &lt;none&gt; 47616 275.07 ## + Age 1 2354 45262 275.14 ## + Weight 1 196 47420 276.91 ## + Seated 1 102 47514 276.99 ## + Arm 1 76 47540 277.01 ## + HtShoes 1 26 47590 277.05 ## + Thigh 1 5 47611 277.06 ## - Ht 1 84023 131639 311.71 ## ## Step: AIC=274.78 ## hipcenter ~ Ht + Leg ## ## Df Sum of Sq RSS AIC ## + Age 1 2896.6 41938 274.24 ## &lt;none&gt; 44835 274.78 ## - Leg 1 2781.1 47616 275.07 ## + Arm 1 522.7 44312 276.33 ## + Weight 1 445.1 44390 276.40 ## + HtShoes 1 34.1 44801 276.75 ## + Thigh 1 33.0 44802 276.75 ## + Seated 1 1.1 44834 276.78 ## - Ht 1 5236.3 50071 276.98 ## ## Step: AIC=274.24 ## hipcenter ~ Ht + Leg + Age ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 41938 274.24 ## - Age 1 2896.6 44835 274.78 ## - Leg 1 3324.2 45262 275.14 ## - Ht 1 4238.3 46176 275.90 ## + Thigh 1 372.7 41565 275.90 ## + Arm 1 257.1 41681 276.01 ## + Seated 1 121.3 41817 276.13 ## + Weight 1 46.8 41891 276.20 ## + HtShoes 1 13.4 41925 276.23 En su lugar, podríamos usar nuevamente \\(\\text{BIC}\\) como nuestra métrica. hipcenter_mod_both_bic = step( hipcenter_mod_start, scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, direction = &quot;both&quot;, k = log(n)) ## Start: AIC=313.35 ## hipcenter ~ 1 ## ## Df Sum of Sq RSS AIC ## + Ht 1 84023 47616 278.34 ## + HtShoes 1 83534 48105 278.73 ## + Leg 1 81568 50071 280.25 ## + Seated 1 70392 61247 287.91 ## + Weight 1 53975 77664 296.93 ## + Thigh 1 46010 85629 300.64 ## + Arm 1 45065 86574 301.06 ## &lt;none&gt; 131639 313.35 ## + Age 1 5541 126098 315.35 ## ## Step: AIC=278.34 ## hipcenter ~ Ht ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 47616 278.34 ## + Leg 1 2781 44835 279.69 ## + Age 1 2354 45262 280.05 ## + Weight 1 196 47420 281.82 ## + Seated 1 102 47514 281.90 ## + Arm 1 76 47540 281.92 ## + HtShoes 1 26 47590 281.96 ## + Thigh 1 5 47611 281.98 ## - Ht 1 84023 131639 313.35 Las comparaciones de \\(R^2\\) ajustado y LOOCV \\(\\text{RMSE}\\) son similares a los de hacia atrás y hacia adelante, lo cual no es para nada sorprendente, ya que algunos de los modelos seleccionados son los mismos que antes. summary(hipcenter_mod)$adj.r.squared ## [1] 0.6000855 summary(hipcenter_mod_both_aic)$adj.r.squared ## [1] 0.6533055 summary(hipcenter_mod_both_bic)$adj.r.squared ## [1] 0.6282374 calc_loocv_rmse(hipcenter_mod) ## [1] 44.44564 calc_loocv_rmse(hipcenter_mod_both_aic) ## [1] 37.62516 calc_loocv_rmse(hipcenter_mod_both_bic) ## [1] 37.2511 16.2.4 Búsqueda exhaustiva La búsqueda hacia atrás, hacia adelante y paso a paso son útiles, pero tienen un problema obvio. Al no comprobar todos los modelos posibles, a veces se perderán el mejor modelo posible. Con una cantidad extremadamente grande de predictores, a veces esto es necesario, ya que verificar todos los modelos posibles llevaría bastante tiempo, incluso con computadoras actuales. Sin embargo, con un conjunto de datos de tamaño razonable, no es demasiado difícil verificar todos los modelos posibles. Para hacerlo, usaremos la función regsubsets() en el paquete leaps de R. library(leaps) all_hipcenter_mod = summary(regsubsets(hipcenter ~ ., data = seatpos)) Algunos puntos sobre esta línea de código. Primero, tenga en cuenta que usamos inmediatamente summary() y almacenamos esos resultados. Ese es simplemente el uso previsto de regsubsets(). En segundo lugar, dentro de regsubsets() especificamos el modelo hipcenter ~ .. Este será el modelo más grande considerado, es decir, el modelo que utiliza todos los predictores de primer orden, y R comprobará todos los subconjuntos posibles. Ahora veremos la información almacenada en all_hipcenter_mod. all_hipcenter_mod$which ## (Intercept) Age Weight HtShoes Ht Seated Arm Thigh Leg ## 1 TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 2 TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## 3 TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## 4 TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE ## 5 TRUE TRUE FALSE TRUE FALSE FALSE TRUE TRUE TRUE ## 6 TRUE TRUE FALSE TRUE FALSE TRUE TRUE TRUE TRUE ## 7 TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE ## 8 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE El uso de $which nos da el mejor modelo, de acuerdo con \\(\\text{RSS}\\), para un modelo de cada tamaño posible, en este caso entre uno y ocho predictores. Por ejemplo, el mejor modelo con cuatro predictores (\\(p=5\\)) usaría Age, HtShoes, Thigh, y Leg. all_hipcenter_mod$rss ## [1] 47615.79 44834.69 41938.09 41485.01 41313.00 41277.90 41266.80 41261.78 Podemos obtener \\(\\text{RSS}\\) para cada uno de estos modelos usando $rss. Tenga en cuenta que estos están disminuyendo ya que los modelos varían de pequeños a grandes. Ahora que tenemos \\(\\text{RSS}\\) para cada uno de estos modelos, es bastante fácil obtener \\(\\text{AIC}\\), \\(\\text{BIC}\\) y \\(R^2\\) ajustado, ya que son toda una función de \\(\\text{RSS}\\) Además, dado que tenemos los modelos con los mejores \\(\\text{RSS}\\) para cada tamaño, darán como resultado los modelos con los mejores \\(\\text{AIC}\\), \\(\\text {BIC}\\) y \\(R^2\\) ajustados para cada tamaño. Luego, al elegir entre ellos, podemos encontrar el mejor \\(\\text{AIC}\\), \\(\\text{BIC}\\) y \\(R^2\\) ajustado. Convenientemente, \\(R^2\\) ajustado se calcula automáticamente. all_hipcenter_mod$adjr2 ## [1] 0.6282374 0.6399496 0.6533055 0.6466586 0.6371276 0.6257403 0.6133690 ## [8] 0.6000855 Para encontrar qué modelo tiene el \\(R^2\\) ajustado más alto, podemos usar la función which.max(). (best_r2_ind = which.max(all_hipcenter_mod$adjr2)) ## [1] 3 Luego podemos extraer los predictores de ese modelo. all_hipcenter_mod$which[best_r2_ind, ] ## (Intercept) Age Weight HtShoes Ht Seated ## TRUE TRUE FALSE FALSE TRUE FALSE ## Arm Thigh Leg ## FALSE FALSE TRUE Ahora calcularemos \\(\\text{AIC}\\) y \\(\\text{BIC}\\) para cada uno de los modelos con el mejor \\(\\text{RSS}\\). Para hacerlo, necesitaremos \\(n\\) y \\(p\\) para el modelo más grande posible. p = length(coef(hipcenter_mod)) n = length(resid(hipcenter_mod)) Usaremos la forma de \\(\\text{AIC}\\) que omite el término constante que es igual en todos los modelos. \\[ \\text{AIC} = n\\log\\left(\\frac{\\text{RSS}}{n}\\right) + 2p. \\] Dado que tenemos el \\(\\text{RSS}\\) de cada modelo almacenado, esto es fácil de calcular. hipcenter_mod_aic = n * log(all_hipcenter_mod$rss / n) + 2 * (2:p) Luego podemos extraer los predictores del modelo con el mejor \\(\\text{AIC}\\). best_aic_ind = which.min(hipcenter_mod_aic) all_hipcenter_mod$which[best_aic_ind,] ## (Intercept) Age Weight HtShoes Ht Seated ## TRUE TRUE FALSE FALSE TRUE FALSE ## Arm Thigh Leg ## FALSE FALSE TRUE Ajustemos este modelo para que podamos comparar con nuestros modelos previamente elegidos usando \\(\\text{AIC}\\) y procedimientos de búsqueda. hipcenter_mod_best_aic = lm(hipcenter ~ Age + Ht + Leg, data = seatpos) La función extractAIC() calculará el \\(\\text{AIC}\\) definido anteriormente para un modelo ajustado. extractAIC(hipcenter_mod_best_aic) ## [1] 4.0000 274.2418 extractAIC(hipcenter_mod_back_aic) ## [1] 4.0000 274.2597 extractAIC(hipcenter_mod_forw_aic) ## [1] 4.0000 274.2418 extractAIC(hipcenter_mod_both_aic) ## [1] 4.0000 274.2418 Vemos que dos de los modelos elegidos por los procedimientos de búsqueda tienen el mejor \\(\\text{AIC}\\) posible, ya que son el mismo modelo. Sin embargo, esto nunca está garantizado. Vemos que el modelo elegido usando la selección hacia atrás no alcanza el \\(\\text{AIC}\\) más pequeño posible. plot(hipcenter_mod_aic ~ I(2:p), ylab = &quot;AIC&quot;, xlab = &quot;p, número de parámetros&quot;, pch = 20, col = &quot;dodgerblue&quot;, type = &quot;b&quot;, cex = 2, main = &quot;AIC vs Complejidad del modelo&quot;) Fácilmente podríamos repetir este proceso para \\(\\text{BIC}\\). \\[ \\text{BIC} = n\\log\\left(\\frac{\\text{RSS}}{n}\\right) + \\log(n)p. \\] hipcenter_mod_bic = n * log(all_hipcenter_mod$rss / n) + log(n) * (2:p) which.min(hipcenter_mod_bic) ## [1] 1 all_hipcenter_mod$which[1,] ## (Intercept) Age Weight HtShoes Ht Seated ## TRUE FALSE FALSE FALSE TRUE FALSE ## Arm Thigh Leg ## FALSE FALSE FALSE hipcenter_mod_best_bic = lm(hipcenter ~ Ht, data = seatpos) extractAIC(hipcenter_mod_best_bic, k = log(n)) ## [1] 2.0000 278.3418 extractAIC(hipcenter_mod_back_bic, k = log(n)) ## [1] 2.0000 278.7306 extractAIC(hipcenter_mod_forw_bic, k = log(n)) ## [1] 2.0000 278.3418 extractAIC(hipcenter_mod_both_bic, k = log(n)) ## [1] 2.0000 278.3418 16.3 Términos de orden superior Hasta ahora solo hemos permitido términos de primer orden en nuestros modelos. Regresemos al conjunto de datos autompg para explorar términos de orden superior. autompg = read.table( &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;, quote = &quot;\\&quot;&quot;, comment.char = &quot;&quot;, stringsAsFactors = FALSE) colnames(autompg) = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;, &quot;origin&quot;, &quot;name&quot;) autompg = subset(autompg, autompg$hp != &quot;?&quot;) autompg = subset(autompg, autompg$name != &quot;plymouth reliant&quot;) rownames(autompg) = paste(autompg$cyl, &quot;cylinder&quot;, autompg$year, autompg$name) autompg$hp = as.numeric(autompg$hp) autompg$domestic = as.numeric(autompg$origin == 1) autompg = autompg[autompg$cyl != 5,] autompg = autompg[autompg$cyl != 3,] autompg$cyl = as.factor(autompg$cyl) autompg$domestic = as.factor(autompg$domestic) autompg = subset(autompg, select = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;, &quot;domestic&quot;)) str(autompg) ## &#39;data.frame&#39;: 383 obs. of 8 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ domestic: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... Recuerde que tenemos dos variables factor, cyl y domestic. La variable cyl tiene tres niveles, mientras que la variable domestic sólo tiene dos. Por lo tanto, la variable cyl se codificará usando dos variables ficticias, mientras que la variable domestic solo necesitará una. Prestaremos atención a esto más adelante. pairs(autompg, col = &quot;dodgerblue&quot;) Usaremos el gráfico pairs() para determinar qué variables pueden beneficiarse de una relación cuadrática con la respuesta. También consideraremos todas las posibles interacciones de dos vías No consideraremos ningún orden de tres o más. Por ejemplo, no consideraremos la interacción entre los términos de primer orden y los términos cuadráticos agregados. Así que ahora ajustaremos este modelo bastante grande. Usaremos una respuesta transformada logarítmicamente. Observe que log(mpg) ~ . ^ 2 considerará automáticamente todos los términos de primer orden, así como todas las interacciones de dos vías. Usamos I(var_name ^ 2) para agregar términos cuadráticos para algunas variables. Esto generalmente funciona mejor que usar poly() cuando se realiza la selección de variables. autompg_big_mod = lm( log(mpg) ~ . ^ 2 + I(disp ^ 2) + I(hp ^ 2) + I(wt ^ 2) + I(acc ^ 2), data = autompg) Creemos que es bastante improbable que realmente necesitemos todos estos términos. ¡Hay unos cuantos! length(coef(autompg_big_mod)) ## [1] 40 Intentaremos buscar hacia atrás con \\(\\text{AIC}\\) y \\(\\text{BIC}\\) para intentar encontrar un modelo más pequeño y razonable. autompg_mod_back_aic = step(autompg_big_mod, direction = &quot;backward&quot;, trace = 0) Observe que usamos trace = 0 en la llamada a la función. Esto suprime la salida de cada paso y simplemente almacena el modelo elegido. Esto es útil, ya que este código de otro modo crearía una gran cantidad de resultados. Si hubiéramos visto la salida, que puede probar por su cuenta eliminando trace = 0, veríamos que R solo considera la variable cyl como una única variable, a pesar de que está codificada usando dos variables ficticias. Por lo tanto, eliminar cyl eliminaría dos parámetros del modelo resultante. También debe notar que R respeta la jerarquía cuando intenta eliminar variables. Es decir, por ejemplo, R no considerará eliminar hp si hp:disp o I(hp ^ 2) están actualmente en el modelo. También usamos \\(\\text{BIC}\\). n = length(resid(autompg_big_mod)) autompg_mod_back_bic = step(autompg_big_mod, direction = &quot;backward&quot;, k = log(n), trace = 0) Al observar los coeficientes de los dos modelos elegidos, vemos que todavía son bastante grandes. coef(autompg_mod_back_aic) ## (Intercept) cyl6 cyl8 disp hp ## 3.671884e+00 -1.602563e-01 -8.581644e-01 -9.371971e-03 2.293534e-02 ## wt acc year domestic1 I(hp^2) ## -3.064497e-04 -1.393888e-01 -1.966361e-03 9.369324e-01 -1.497669e-05 ## cyl6:acc cyl8:acc disp:wt disp:year hp:acc ## 7.220298e-03 5.041915e-02 5.797816e-07 9.493770e-05 -5.062295e-04 ## hp:year acc:year acc:domestic1 year:domestic1 ## -1.838985e-04 2.345625e-03 -2.372468e-02 -7.332725e-03 coef(autompg_mod_back_bic) ## (Intercept) cyl6 cyl8 disp hp ## 4.657847e+00 -1.086165e-01 -7.611631e-01 -1.609316e-03 2.621266e-03 ## wt acc year domestic1 cyl6:acc ## -2.635972e-04 -1.670601e-01 -1.045646e-02 3.341579e-01 4.315493e-03 ## cyl8:acc disp:wt hp:acc acc:year acc:domestic1 ## 4.610095e-02 4.102804e-07 -3.386261e-04 2.500137e-03 -2.193294e-02 Sin embargo, son mucho más pequeños que el modelo completo original. También observe que los modelos resultantes respetan la jerarquía. length(coef(autompg_big_mod)) ## [1] 40 length(coef(autompg_mod_back_aic)) ## [1] 19 length(coef(autompg_mod_back_bic)) ## [1] 15 Calculando el LOOCV \\(\\text{RMSE}\\) para cada uno, vemos que el modelo elegido usando \\(\\text{BIC}\\) funciona mejor. Eso significa que es el mejor modelo para la predicción, ya que logra el mejor LOOCV \\(\\text{RMSE}\\), pero también el mejor modelo para la explicación, ya que también es el más pequeño. calc_loocv_rmse(autompg_big_mod) ## [1] 0.1112024 calc_loocv_rmse(autompg_mod_back_aic) ## [1] 0.1032888 calc_loocv_rmse(autompg_mod_back_bic) ## [1] 0.103134 16.4 Explicación versus predicción A lo largo de este capítulo, hemos intentado encontrar modelos razonablemente pequeños, que son buenos para explicar la relación entre la respuesta y los predictores, que también tienen errores pequeños que son buenos para hacer predicciones. Más adelante analizaremos el modelo autompg_mod_back_bic para explicar mejor la diferencia entre usar modelos para explicar y predecir. Este es el modelo que se ajusta a los datos de autompg que se eligieron usando busqueda hacia atrás y \\(\\text{BIC}\\), que obtuvo el LOOCV \\(\\text{RMSE}\\) más bajo de los modelos que consideramos. autompg_mod_back_bic ## ## Call: ## lm(formula = log(mpg) ~ cyl + disp + hp + wt + acc + year + domestic + ## cyl:acc + disp:wt + hp:acc + acc:year + acc:domestic, data = autompg) ## ## Coefficients: ## (Intercept) cyl6 cyl8 disp hp ## 4.658e+00 -1.086e-01 -7.612e-01 -1.609e-03 2.621e-03 ## wt acc year domestic1 cyl6:acc ## -2.636e-04 -1.671e-01 -1.046e-02 3.342e-01 4.315e-03 ## cyl8:acc disp:wt hp:acc acc:year acc:domestic1 ## 4.610e-02 4.103e-07 -3.386e-04 2.500e-03 -2.193e-02 Observe que este es un modelo algo grande, que usa parámetros 15, incluidos varios términos de interacción. ¿Nos importa que este sea un modelo grande? La respuesta es, depende. 16.4.1 Explicación Supongamos que nos gustaría utilizar este modelo como explicación. Quizás somos un fabricante de automóviles que intenta diseñar un vehículo de bajo consumo de combustible. Si este es el caso, estamos interesados tanto en qué variables predictoras son útiles para explicar la eficiencia de combustible del automóvil como en cómo esas variables que afectan la eficiencia de combustible. Al comprender esta relación, podemos utilizar este conocimiento en nuestro beneficio al diseñar un automóvil. Para explicar una relación, nos interesa mantener los modelos lo más pequeños posible, ya que los modelos más pequeños son fáciles de interpretar. Cuantos menos predictores, menos consideraciones debemos tener en cuenta en nuestro proceso de diseño. Además, cuantas menos interacciones y términos polinomiales, más fácil sea interpretar cualquier parámetro, ya que las interpretaciones de los parámetros están condicionadas a qué parámetros están en el modelo. Tenga en cuenta que los modelos lineales son bastante interpretables para empezar. Más adelante en sus carreras de análisis de datos, verá modelos más complicados que pueden ajustarse mejor a los datos, pero son mucho más difíciles, si no imposibles, de interpretar. Estos modelos no son muy útiles para explicar una relación. Para encontrar modelos pequeños e interpretables, usaríamos un criterio de selección que explícitamente penaliza a modelos más grandes, como AIC y BIC. En este caso todavía obtuvimos un modelo algo grande, pero mucho más pequeño que el modelo que usamos para iniciar el proceso de selección. 16.4.1.1 Correlación y causalidad Una advertencia al usar un modelo para explicar una relación. Hay dos términos que se utilizan a menudo para describir una relación entre dos variables: causalidad y correlación. Correlación a menudo también se conoce como asociación. El hecho de que dos variables estén correlacionadas no significa necesariamente que una cause la otra. Por ejemplo, considerando el modelado de mpg como solo una función de hp. plot(mpg ~ hp, data = autompg, col = &quot;dodgerblue&quot;, pch = 20, cex = 1.5) ¿Un aumento en los caballos de fuerza causa una disminución en la eficiencia del combustible? O quizás la causalidad se invierte y un aumento en la eficiencia del combustible causa una disminución en los caballos de fuerza. ¡O quizás haya una tercera variable que explique ambos! El problema es que tenemos datos de observación. Con datos de observación, solo podemos detectar asociaciones. Para hablar con confianza sobre la causalidad, necesitaríamos realizar experimentos. Este es un concepto que debe encontrar a menudo en su educación estadística. Para leer más y algunas falacias relacionadas, consulte: Wikipedia: La correlación no implica causalidad. 16.4.2 Predicción Supongamos que ahora, en lugar del fabricante al que le gustaría fabricar un automóvil, somos un consumidor que desea comprar un automóvil nuevo. Sin embargo, este automóvil en particular es tan nuevo que no ha sido probado rigurosamente, por lo que no estamos seguros de qué eficiencia de combustible esperar. (Y, como escépticos, no confiamos en lo que nos dice el fabricante). En este caso, nos gustaría usar el modelo para ayudar a predecir la eficiencia de combustible de este automóvil en función de sus atributos, que son los predictores del modelo. Cuanto menores son los errores que comete el modelo, más confianza tenemos en su predicción. Por lo tanto, para encontrar modelos de predicción, usaríamos un criterio de selección que implícitamente penaliza a los modelos más grandes, como LOOCV \\(\\text{RMSE}\\). Siempre que el modelo no se ajuste demasiado, en realidad no nos importa qué tan grande sea el modelo. Explicar la relación entre las variables no es nuestro objetivo aquí, ¡simplemente queremos saber qué tipo de eficiencia de combustible debemos esperar! Si ** solo ** nos preocupamos por la predicción, no debemos preocuparnos por la correlación frente a la causalidad, y no debemos preocuparnos por los supuestos del modelo. Si una variable está correlacionada con la respuesta, en realidad no importa si causa un efecto en la respuesta, aún puede ser útil para la predicción. Por ejemplo, en los niños en edad escolar primaria la talla de su zapato ciertamente no hace que lean a un nivel superior, sin embargo, podríamos usar la talla de un zapato muy fácilmente para hacer una predicción sobre la capacidad de lectura de un niño. Cuanto mayor sea el tamaño de sus zapatos, mejor leen. Sin embargo, hay una variable al acecho aquí, ¡su edad! (No envíe a sus hijos a la escuela con zapatos de talla 40, ¡no les hará leer mejor!) Tampoco nos importan los supuestos del modelo. Los mínimos cuadrados son mínimos cuadrados. Para un modelo específico, encontrará los valores de los parámetros que minimizarán la pérdida por error al cuadrado. Sus resultados pueden ser en gran parte ininterpretables e inútiles para la inferencia, pero para la predicción nada de eso importa. "],["regresión-logística.html", "Capítulo 17 Regresión logística 17.1 Modelos lineales generalizados 17.2 Respuesta binaria 17.3 Trabajar con regresión logística 17.4 Clasificación", " Capítulo 17 Regresión logística Después de leer este capítulo, podrá: Comprender cómo los modelos lineales generalizados son una generalización de los modelos lineales ordinarios. Utilice la regresión logística para modelar una respuesta binaria. Aplicar conceptos aprendidos para modelos lineales ordinarios a la regresión logística. Utilizar regresión logística para realizar la clasificación. Hasta ahora solo hemos considerado modelos para variables de respuesta numérica. ¿Qué pasa con las variables respuesta que solo toman valores enteros? ¿Qué pasa con una variable respuesta que es categórica? ¿Podemos utilizar modelos lineales en estas situaciones? ¡Sí! El modelo que hemos estado usando, al que llamaremos regresión lineal ordinaria, es en realidad un caso específico del modelo lineal generalizado más general. (¿No son buenos los estadísticos para nombrar cosas?) 17.1 Modelos lineales generalizados Hasta ahora, hemos tenido variables respuesta que, condicionadas por los predictores, se modelaron utilizando una distribución normal con una media que es una combinación lineal de los predictores. Esta combinación lineal es lo que hace que un modelo lineal sea lineal. \\[ Y \\mid {\\bf X} = {\\bf x} \\sim N(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_{p - 1}x_{p - 1}, \\ \\sigma^2) \\] Ahora permitiremos dos modificaciones de esta situación, lo que nos permitirá usar modelos lineales en muchas más situaciones. En lugar de utilizar una distribución normal para la respuesta condicionada a los predictores, permitiremos otras distribuciones. Además, en lugar de que la media condicional sea una combinación lineal de los predictores, puede ser alguna función de una combinación lineal de los predictores. En general, un modelo lineal generalizado tiene tres partes: Una distribución de la respuesta condicionada a los predictores. (Técnicamente, esta distribución debe ser de la familia exponencial). Una combinación lineal de los predictores \\(p - 1\\), \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_{p - 1} x_{p - 1}\\), que escribimos como \\(\\eta({\\bf x})\\). Es decir, \\[\\eta({\\bf x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_{p - 1} x_{p - 1}\\] Una función enlace, \\(g()\\), que define cómo $({}) $, la combinación lineal de los predictores, se relaciona con la media de la respuesta condicionada a los predictores, \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\). \\[ \\eta({\\bf x}) = g\\left(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\right). \\] La siguiente tabla resume tres ejemplos de un modelo lineal generalizado: Regresión lineal Regresión Poisson Regresión logística \\(Y \\mid {\\bf X} = {\\bf x}\\) \\(N(\\mu({\\bf x}), \\sigma^2)\\) \\(\\text{Pois}(\\lambda({\\bf x}))\\) \\(\\text{Bern}(p({\\bf x}))\\) Nombre de la distribución Normal Poisson Bernoulli (Binomial) \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\) \\(\\mu({\\bf x})\\) \\(\\lambda({\\bf x})\\) \\(p({\\bf x})\\) Soporte Real: \\((-\\infty, \\infty)\\) Entero: \\(0, 1, 2, \\ldots\\) Entero: \\(0, 1\\) Uso Datos numéricos Datos de conteos (entero) Datos binarios Nombre del enlace Identity Log Logit Función de enlace \\(\\eta({\\bf x}) = \\mu({\\bf x})\\) \\(\\eta({\\bf x}) = \\log(\\lambda({\\bf x}))\\) \\(\\eta({\\bf x}) = \\log \\left(\\frac{p({\\bf x})}{1 - p({\\bf x})} \\right)\\) Función de la media \\(\\mu({\\bf x}) = \\eta({\\bf x})\\) \\(\\lambda({\\bf x}) = e^{\\eta({\\bf x})}\\) \\(p({\\bf x}) = \\frac{e^{\\eta({\\bf x})}}{1 + e^{\\eta({\\bf x})}} = \\frac{1}{1 + e^{-\\eta({\\bf x})}}\\) Como en la regresión lineal ordinaria, buscaremos ajustar el modelo estimando los parámetros \\(\\beta\\). Para ello utilizaremos el método de máxima verosimilitud. Tenga en cuenta que una distribución Bernoulli es un caso específico de una distribución binomial donde el parámetro \\(n\\) de un binomio es \\(1\\). La regresión binomial también es posible, pero nos centraremos en el caso Bernoulli, mucho más popular. Entonces, en general, los GLM relacionan la media de la respuesta con una combinación lineal de predictores, \\(\\eta({\\bf x})\\), mediante el uso de una función de enlace, \\(g()\\). Es decir, \\[ \\eta({\\bf x}) = g\\left(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\right). \\] La media es entonces \\[ \\text{E}[Y \\mid {\\bf X} = {\\bf x}] = g^{-1}(\\eta({\\bf x})). \\] 17.2 Respuesta binaria Para ilustrar el uso de un GLM, nos centraremos en el caso de la variable respuestas binaria codificada con \\(0\\) y \\(1\\). En la práctica, estos \\(0\\) y \\(1\\) codificarán para dos clases como sí/no, gato/perro, enfermo/sano, etc. \\[ Y = \\begin{cases} 1 &amp; \\text{Si} \\\\ 0 &amp; \\text{No} \\end{cases} \\] Primero, definimos una notación que usaremos en todo momento. \\[ p({\\bf x}) = P[Y = 1 \\mid {\\bf X} = {\\bf x}] \\] Con una respuesta binaria (Bernoulli), nos centraremos principalmente en el caso cuando \\(Y = 1\\), ya que con solo dos posibilidades, es trivial obtener probabilidades cuando \\(Y = 0\\). \\[ P[Y = 0 \\mid {\\bf X} = {\\bf x}] + P[Y = 1 \\mid {\\bf X} = {\\bf x}] = 1 \\] \\[ P[Y = 0 \\mid {\\bf X} = {\\bf x}] = 1 - p({\\bf x}) \\] Ahora definimos el modelo de regresión logística. \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_{p - 1} x_{p - 1} \\] Inmediatamente notamos algunas similitudes con la regresión lineal ordinaria, en particular, del lado derecho. Esta es nuestra combinación lineal habitual de predictores. Tenemos nuestros predictores habituales \\(p - 1\\) para un total de \\(p\\) parámetros \\(\\beta\\). (Tenga en cuenta que muchos más textos centrados en el aprendizaje automático utilizarán \\(p\\) como número de predictores. Esta es una elección arbitraria, pero debe tenerla en cuenta). El lado izquierdo se llama log odds, que es el log de los odds. Los odds son la probabilidad de un evento positivo \\((Y = 1)\\) dividida por la probabilidad de un evento negativo \\((Y = 0)\\). Entonces, cuando los odds son \\(1\\), los dos eventos tienen la misma probabilidad. Los odds superiores a \\(1\\) favorecen un evento positivo. Lo contrario es cierto cuando los odds son inferiores a \\(1\\). \\[ \\frac{p({\\bf x})}{1 - p({\\bf x})} = \\frac{P[Y = 1 \\mid {\\bf X} = {\\bf x}]}{P[Y = 0 \\mid {\\bf X} = {\\bf x}]} \\] Básicamente, el log de los odds son la transformación logit aplicada a \\(p({\\bf x})\\). \\[ \\text{logit}(\\xi) = \\log\\left(\\frac{\\xi}{1 - \\xi}\\right) \\] También será útil definir el logit inverso, también conocido como la función logística o sigmoid. \\[ \\text{logit}^{-1}(\\xi) = \\frac{e^\\xi}{1 + e^{\\xi}} = \\frac{1}{1 + e^{-\\xi}} \\] Tenga en cuenta que para \\(x \\in (-\\infty, \\infty))\\), esta función genera valores entre 0 y 1. Los estudiantes a menudo preguntan, ¿dónde está el término de error? La respuesta es que es algo específico del modelo normal. Primero observe que el modelo con el término de error, \\[ Y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_qx_q + \\epsilon, \\ \\ \\epsilon \\sim N(0, \\sigma^2) \\] en su lugar se puede escribir como \\[ Y \\mid {\\bf X} = {\\bf x} \\sim N(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_qx_q, \\ \\sigma^2). \\] Si bien nuestro enfoque principal es estimar la media, \\(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_qx_q\\), también hay otro parámetro, \\(\\sigma^2\\) que debe estimarse. Este es el resultado de que la distribución normal tiene dos parámetros. Con la regresión logística, que usa la distribución de Bernoulli, solo necesitamos estimar el parámetro único de la distribución de Bernoulli \\(p({\\bf x})\\), que resulta ser su media. \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_{q} x_{q} \\] Entonces, aunque introdujimos primero la regresión lineal ordinaria, de alguna manera, la regresión logística es en realidad más simple. Tenga en cuenta que la aplicación de la transformación logit inversa nos permite obtener una expresión para \\(p({\\bf x})\\). \\[ p({\\bf x}) = P[Y = 1 \\mid {\\bf X} = {\\bf x}] = \\frac{e^{\\beta_0 + \\beta_1 x_{1} + \\cdots + \\beta_{p-1} x_{(p-1)}}}{1 + e^{\\beta_0 + \\beta_1 x_{1} + \\cdots + \\beta_{p-1} x_{(p-1)}}} \\] 17.2.1 Ajuste de la regresión logística Con \\(n\\) observaciones, escribimos el modelo indexado con \\(i\\) para notar que se está aplicando a cada observación. \\[ \\log\\left(\\frac{p({\\bf x_i})}{1 - p({\\bf x_i)})}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)} \\] Podemos aplicar la transformación logit inversa para obtener \\(P[Y_i = 1 \\mid {\\bf X_i} = {\\bf x_i}]\\) para cada observación. Dado que estas son probabilidades, es bueno que usemos una función que devuelve valores entre \\(0\\) y \\(1\\). \\[ p({\\bf x_i}) = P[Y_i = 1 \\mid {\\bf X_i} = {\\bf x_i}] = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}} \\] \\[ 1 - p({\\bf x_i}) = P[Y_i = 0 \\mid {\\bf X} = {\\bf x_i}] = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}} \\] Para ajustar este modelo, es decir, estimar los parámetros \\(\\beta\\), usaremos la máxima verosimilitud. \\[ \\boldsymbol{{\\beta}} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_{p - 1}] \\] Primero escribimos la verosimilitud dados los datos observados. \\[ L(\\boldsymbol{{\\beta}}) = \\prod_{i = 1}^{n} P[Y_i = y_i \\mid {\\bf X_i} = {\\bf x_i}] \\] Esto ya es técnicamente una función de los parámetros \\(\\beta\\), pero haremos algunos cambios para que esto sea más explícito. \\[ L(\\boldsymbol{{\\beta}}) = \\prod_{i = 1}^{n} p({\\bf x_i})^{y_i} (1 - p({\\bf x_i}))^{(1 - y_i)} \\] \\[ L(\\boldsymbol{{\\beta}}) = \\prod_{i : y_i = 1}^{n} p({\\bf x_i}) \\prod_{j : y_j = 0}^{n} (1 - p({\\bf x_j})) \\] \\[ L(\\boldsymbol{{\\beta}}) = \\prod_{i : y_i = 1}^{} \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}} \\prod_{j : y_j = 0}^{} \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_{j1} + \\cdots + \\beta_{p-1} x_{j(p-1)}}} \\] Desafortunadamente, a diferencia de la regresión lineal ordinaria, no existe una solución analítica para este problema de maximización. En cambio, deberá resolverse numéricamente. Afortunadamente, R se encargará de esto por nosotros usando un algoritmo de mínimos cuadrados reponderados iterativamente. (Dejaremos los detalles para un curso de optimización o aprendizaje automático, que probablemente también discutirá estrategias de optimización alternativas). 17.2.2 Problemas de ajuste Debemos tener en cuenta que, si existe algún \\(\\beta^*\\) tal que \\[ {\\bf x_i}^{\\top} \\boldsymbol{{\\beta}^*} &gt; 0 \\implies y_i = 1 \\] y \\[ {\\bf x_i}^{\\top} \\boldsymbol{{\\beta}^*} &lt; 0 \\implies y_i = 0 \\] para todas las observaciones, el MLE no es único. Se dice que estos datos son separables. Esto, y otros problemas numéricos similares relacionados con las probabilidades estimadas cercanas a 0 o 1, devolverán una advertencia en R: ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Cuando esto sucede, el modelo todavía está ajustado, pero hay consecuencias, es decir, los coeficientes estimados son muy sospechosos. Este es un problema al intentar interpretar el modelo. Cuando esto sucede, el modelo a menudo seguirá siendo útil para crear un clasificador, que se discutirá más adelante. Sin embargo, todavía está sujeto a las evaluaciones habituales de los clasificadores para determinar qué tan bien se está desempeñando. Para obtener más información, consulte Modern Applied Statistics with S-PLUS, Chapter 7. 17.2.3 Ejemplos de simulación sim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) { x = rnorm(n = sample_size) eta = beta_0 + beta_1 * x p = 1 / (1 + exp(-eta)) y = rbinom(n = sample_size, size = 1, prob = p) data.frame(y, x) } Podría pensar, ¿por qué no usar simplemente la regresión lineal ordinaria? Incluso con una respuesta binaria, nuestro objetivo sigue siendo modelar (alguna función de) \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\). Sin embargo, con una respuesta binaria codificada como \\(0\\) y \\(1\\), \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}] = P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) desde \\[ \\begin{aligned} \\text{E}[Y \\mid {\\bf X} = {\\bf x}] &amp;= 1 \\cdot P[Y = 1 \\mid {\\bf X} = {\\bf x}] + 0 \\cdot P[Y = 0 \\mid {\\bf X} = {\\bf x}] \\\\ &amp;= P[Y = 1 \\mid {\\bf X} = {\\bf x}] \\end{aligned} \\] Entonces, ¿por qué no podemos usar la regresión lineal ordinaria para estimar \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\), y por lo tanto \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\)? Para investigar, simulemos datos del siguiente modelo: \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -2 + 3 x \\] Otra forma de escribir esto, que coincide mejor con la función que estamos usando para simular los datos: \\[ \\begin{aligned} Y_i \\mid {\\bf X_i} = {\\bf x_i} &amp;\\sim \\text{Bern}(p_i) \\\\ p_i &amp;= p({\\bf x_i}) = \\frac{1}{1 + e^{-\\eta({\\bf x_i})}} \\\\ \\eta({\\bf x_i}) &amp;= -2 + 3 x_i \\end{aligned} \\] set.seed(1) example_data = sim_logistic_data() head(example_data) ## y x ## 1 0 -0.6264538 ## 2 1 0.1836433 ## 3 0 -0.8356286 ## 4 1 1.5952808 ## 5 0 0.3295078 ## 6 0 -0.8204684 Después de simular un conjunto de datos, ajustaremos tanto la regresión lineal ordinaria como la regresión logística. Observe que actualmente la variable respuesta y es una variable numérica que solo toma los valores 0 y 1. Más adelante veremos que también podemos ajustar la regresión logística cuando la respuesta es una variable factor con solo dos niveles. (Generalmente, se prefiere tener una respuesta factor, pero tener una respuesta ficticia (dummy) permite hacer la comparación con el uso de la regresión lineal ordinaria). # regresión lineal ordinaria fit_lm = lm(y ~ x, data = example_data) # Regresión logística fit_glm = glm(y ~ x, data = example_data, family = binomial) Observe que la sintaxis es extremadamente similar. ¿Qué ha cambiado? lm() se ha convertido en glm() Hemos agregado el argumento family = binomial En muchos sentidos, lm() es solo una versión más específica de glm(). Por ejemplo glm(y ~ x, data = example_data) en realidad ajustaría la regresión lineal ordinaria que hemos visto en el pasado. Por defecto, glm() usa el argumento family = gaussian. Es decir, estamos ajustando un GLM con una respuesta normalmente distribuida y la función de identidad como enlace. El argumento family para glm() en realidad especifica tanto la distribución como la función de enlace. Si no se hace explícita, la función de enlace se elige para que sea la función de enlace canónica, que es esencialmente la función de enlace matemática más conveniente. Consulte ?Glm y ?family para obtener más detalles. Por ejemplo, el siguiente código especifica explícitamente la función de enlace que se utilizó anteriormente de forma predeterminada. # llamada más detallada a glm para regresión logística fit_glm = glm(y ~ x, data = example_data, family = binomial(link = &quot;logit&quot;)) Hacer predicciones con un objeto de tipo glm es ligeramente diferente a hacer predicciones después de ajustar con lm(). En el caso de la regresión logística, con family = binomial, tenemos: type Devuelve \"link\" [por defecto] \\(\\hat{\\eta}({\\bf x}) = \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right)\\) \"response\" \\(\\hat{p}({\\bf x}) = \\frac{e^{\\hat{\\eta}({\\bf x})}}{1 + e^{\\hat{\\eta}({\\bf x})}} = \\frac{1}{1 + e^{-\\hat{\\eta}({\\bf x})}}\\) Es decir, type = \"link\" obtendrá el log odds, mientras que type = \"response\" devolverá la media estimada, en este caso, \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) para cada observación. plot(y ~ x, data = example_data, pch = 20, ylab = &quot;Probabilidad estimada&quot;, main = &quot;Regresión ordinaria vs logística&quot;) grid() abline(fit_lm, col = &quot;darkorange&quot;) curve(predict(fit_glm, data.frame(x), type = &quot;response&quot;), add = TRUE, col = &quot;dodgerblue&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;Ordinaria&quot;, &quot;Logística&quot;, &quot;Datos&quot;), lty = c(1, 2, 0), pch = c(NA, NA, 20), lwd = 2, col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;, &quot;black&quot;)) Dado que solo tenemos una única variable predictora, podemos mostrar gráficamente esta situación. Primero, tenga en cuenta que los datos se grafican utilizando puntos negros. La respuesta y solo toma los valores 0 y 1. A continuación, necesitamos discutir las dos líneas agregadas a la gráfica. La primera, la línea naranja sólida, es la regresión lineal ordinaria ajustada. La curva azul punteada es la regresión logística estimada. Es útil darse cuenta de que no estamos trazando una estimación de \\(Y\\) para ninguno de los dos. (A veces puede parecer así con la regresión lineal ordinaria, pero eso no es lo que está sucediendo). Para ambos, estamos trazando \\(\\hat{\\text{E}}[Y \\mid {\\bf X} = {\\bf x}]\\), la media estimada, que para una respuesta binaria resulta ser una estimación de \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\). Vemos inmediatamente por qué la regresión lineal ordinaria no es una buena idea. Mientras estima la media, vemos que produce estimaciones que son menores que 0. (¡Y en otras situaciones podría producir estimaciones mayores que 1!) Si la media es una probabilidad, no queremos probabilidades menores que 0 o mayores que 1. Introduzca regresión logística. Dado que la salida de la función logit inversa está restringida a estar entre 0 y 1, nuestras estimaciones tienen mucho más sentido como probabilidades. Veamos nuestros coeficientes estimados. (Con mucho redondeo, por simplicidad). round(coef(fit_glm), 1) ## (Intercept) x ## -2.3 3.7 Nuestro modelo estimado es entonces: \\[ \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right) = -2.3 + 3.7 x \\] Debido a que no estamos estimando directamente la media, sino una función de la media, debemos tener cuidado con nuestra interpretación de \\(\\hat{\\beta}_1 = 3.7\\). Esto significa que, para un aumento de una unidad en \\(x\\), los log odds cambian (en este caso aumentan) en \\(3.7\\). Además, dado que \\(\\hat{\\beta}_1\\) es positivo, a medida que aumentamos \\(x\\), también aumentamos \\(\\hat{p}({\\bf x})\\). Para ver cuánto, tenemos que considerar la función logística inversa. Por ejemplo, tenemos: \\[ \\hat{P}[Y = 1 \\mid X = -0.5] = \\frac{e^{-2.3 + 3.7 \\cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \\cdot (-0.5)}} \\approx 0.016 \\] \\[ \\hat{P}[Y = 1 \\mid X = 0] = \\frac{e^{-2.3 + 3.7 \\cdot (0)}}{1 + e^{-2.3 + 3.7 \\cdot (0)}} \\approx 0.09112296 \\] \\[ \\hat{P}[Y = 1 \\mid X = 1] = \\frac{e^{-2.3 + 3.7 \\cdot (1)}}{1 + e^{-2.3 + 3.7 \\cdot (1)}} \\approx 0.8021839 \\] Ahora que sabemos que debemos usar la regresión logística y no la regresión lineal ordinaria, consideremos otro ejemplo. Esta vez, consideremos el modelo \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = 1 + -4 x. \\] Nuevamente, podríamos reescribir esto para que coincida mejor con la función que estamos usando para simular los datos: \\[ \\begin{aligned} Y_i \\mid {\\bf X_i} = {\\bf x_i} &amp;\\sim \\text{Bern}(p_i) \\\\ p_i &amp;= p({\\bf x_i}) = \\frac{1}{1 + e^{-\\eta({\\bf x_i})}} \\\\ \\eta({\\bf x_i}) &amp;= 1 + -4 x_i \\end{aligned} \\] En este modelo, a medida que aumenta \\(x\\), las probabilidades logarítmicas disminuyen. set.seed(1) example_data = sim_logistic_data(sample_size = 50, beta_0 = 1, beta_1 = -4) Simulamos nuevamente algunas observaciones de este modelo, luego ajustamos la regresión logística. fit_glm = glm(y ~ x, data = example_data, family = binomial) plot(y ~ x, data = example_data, pch = 20, ylab = &quot;Probabilidad estimada&quot;, main = &quot;Regresión logística, probabilidad decreciente&quot;) grid() curve(predict(fit_glm, data.frame(x), type = &quot;response&quot;), add = TRUE, col = &quot;dodgerblue&quot;, lty = 2) curve(boot::inv.logit(1 - 4 * x), add = TRUE, col = &quot;darkorange&quot;, lty = 1) legend(&quot;bottomleft&quot;, c(&quot;Probabilidad verdadera&quot;, &quot;Probabilidad estimada&quot;, &quot;Datos&quot;), lty = c(1, 2, 0), pch = c(NA, NA, 20), lwd = 2, col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;, &quot;black&quot;)) Vemos que esta vez, a medida que \\(x\\) aumenta, \\(\\hat{p}({\\bf x})\\) disminuye. Ahora veamos un ejemplo en el que la probabilidad estimada no siempre simplemente aumenta o disminuye. Al igual que la regresión lineal ordinaria, la combinación lineal de predictores puede contener transformaciones de predictores (en este caso, un término cuadrático) e interacciones. sim_quadratic_logistic_data = function(sample_size = 25) { x = rnorm(n = sample_size) eta = -1.5 + 0.5 * x + x ^ 2 p = 1 / (1 + exp(-eta)) y = rbinom(n = sample_size, size = 1, prob = p) data.frame(y, x) } \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -1.5 + 0.5x + x^2. \\] Nuevamente, podríamos reescribir esto para que coincida mejor con la función que estamos usando para simular los datos: \\[ \\begin{aligned} Y_i \\mid {\\bf X_i} = {\\bf x_i} &amp;\\sim \\text{Bern}(p_i) \\\\ p_i &amp;= p({\\bf x_i}) = \\frac{1}{1 + e^{-\\eta({\\bf x_i})}} \\\\ \\eta({\\bf x_i}) &amp;= -1.5 + 0.5x_i + x_i^2 \\end{aligned} \\] set.seed(42) example_data = sim_quadratic_logistic_data(sample_size = 50) fit_glm = glm(y ~ x + I(x^2), data = example_data, family = binomial) plot(y ~ x, data = example_data, pch = 20, ylab = &quot;Probabilidad estimada&quot;, main = &quot;Regresión logística, relación cuadrática&quot;) grid() curve(predict(fit_glm, data.frame(x), type = &quot;response&quot;), add = TRUE, col = &quot;dodgerblue&quot;, lty = 2) curve(boot::inv.logit(-1.5 + 0.5 * x + x ^ 2), add = TRUE, col = &quot;darkorange&quot;, lty = 1) legend(&quot;bottomleft&quot;, c(&quot;Probabilidad verdadera&quot;, &quot;Probabilidad estimada&quot;, &quot;Datos&quot;), lty = c(1, 2, 0), pch = c(NA, NA, 20), lwd = 2, col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;, &quot;black&quot;)) 17.3 Trabajar con regresión logística Si bien el modelo de regresión logística no es exactamente el mismo que el modelo de regresión lineal ordinario, porque ambos usan una combinación lineal de los predictores \\[ \\eta({\\bf x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_{p - 1} x_{p - 1} \\] trabajar con regresión logística es muy similar. Muchas de las cosas que hicimos con la regresión lineal ordinaria se pueden hacer con la regresión logística de una manera muy similar. Por ejemplo, Prueba de un solo parámetro \\(\\beta\\) Prueba de un conjunto de parámetros \\(\\beta\\) Especificación de la fórmula en R Interpretación de parámetros y estimaciones Intervalos de confianza para parámetros Intervalos de confianza para la respuesta media Selección de variable Después de una introducción a las nuevas pruebas, demostraremos cada una de ellas con un ejemplo. 17.3.1 Pruebas con GLMs Al igual que la regresión lineal ordinaria, vamos a querer realizar la prueba de hipótesis. Nuevamente querremos pruebas de un solo parámetro y de múltiples parámetros. 17.3.2 Prueba de Wald En regresión lineal ordinaria, realizamos la prueba de \\[ H_0: \\beta_j = 0 \\quad \\text{vs} \\quad H_1: \\beta_j \\neq 0 \\] usando una prueba \\(t\\). Para el modelo de regresión logística, \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_{p - 1} x_{p - 1} \\] podemos volver a realizar una prueba de \\[ H_0: \\beta_j = 0 \\quad \\text{vs} \\quad H_1: \\beta_j \\neq 0 \\] sin embargo, el estadístico de prueba y su distribución ya no son \\(t\\). Vemos que el estadístico de prueba toma la misma forma \\[ z = \\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}[\\hat{\\beta}_j]} \\overset{\\text{approx}}{\\sim} N(0, 1) \\] pero ahora estamos realizando una prueba \\(z\\), ya que el estadístico de prueba se aproxima a una distribución normal estándar, siempre que tengamos una muestra lo suficientemente grande. (La prueba \\(t\\) para la regresión lineal ordinaria, asumiendo que los supuestos eran correctos, tenía una distribución exacta para cualquier tamaño de muestra). Omitiremos algunos de los detalles exactos de los cálculos, ya que R obtendrá el error estándar para nosotros. El uso de esta prueba será extremadamente similar a la prueba \\(t\\) para la regresión lineal ordinaria. Básicamente, lo único que cambia es la distribución del estadístico de prueba. 17.3.3 Prueba de razón de verosimilitud Considere el siguiente modelo completo, \\[ \\log\\left(\\frac{p({\\bf x_i})}{1 - p({\\bf x_i})}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(p-1)} x_{i(p-1)} + \\epsilon_i \\] Este modelo tiene \\(p-1\\) predictores, para un total de \\(p\\) parámetros \\(\\beta\\). Denotaremos el MLE de estos parámetros \\(\\beta\\) como \\(\\hat{\\beta}_{\\text{Full}}\\) Ahora considere un modelo nulo (o reducido), \\[ \\log\\left(\\frac{p({\\bf x_i})}{1 - p({\\bf x_i})}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(q-1)} x_{i(q-1)} + \\epsilon_i \\] donde \\(q&lt;p\\). Este modelo tiene \\(q-1\\) predictores, para un total de \\(q\\) parámetros \\(\\beta\\). Denotaremos el MLE de estos parámetros \\(\\beta\\) como \\(\\hat{\\beta}_{\\text{Null}}\\) La diferencia entre estos dos modelos se puede codificar mediante la hipótesis nula de una prueba. \\[ H_0: \\beta_q = \\beta_{q+1} = \\cdots = \\beta_{p - 1} = 0. \\] Esto implica que el modelo reducido está anidado dentro del modelo completo. Luego definimos un estadístico de prueba, \\(D\\), \\[ D = -2 \\log \\left( \\frac{L(\\boldsymbol{\\hat{\\beta}_{\\text{Null}}})} {L(\\boldsymbol{\\hat{\\beta}_{\\text{Full}}})} \\right) = 2 \\log \\left( \\frac{L(\\boldsymbol{\\hat{\\beta}_{\\text{Full}}})} {L(\\boldsymbol{\\hat{\\beta}_{\\text{Null}}})} \\right) = 2 \\left( \\ell(\\hat{\\beta}_{\\text{Full}}) - \\ell(\\hat{\\beta}_{\\text{Null}})\\right) \\] donde \\(L\\) denota la verosimilitud y \\(\\ell\\) denota la log-verosimilitud Para una muestra lo suficientemente grande, este estadístico de prueba tiene una distribución Chi-cuadrado aproximada \\[ D \\overset{\\text{approx}}{\\sim} \\chi^2_{k} \\] donde \\(k = p - q\\), la diferencia en el número de parámetros de los dos modelos. Esta prueba, que llamaremos Prueba de razón de verosimilitud, será análoga a la prueba ANOVA \\(F\\) para regresión logística. Curiosamente, para realizar la prueba de razón de verosimilitud, en realidad usaremos nuevamente la función anova() en R !. La prueba de razón de verosimilitud es en realidad una prueba bastante general, sin embargo, aquí hemos presentado una aplicación específica a los modelos de regresión logística anidados. 17.3.4 Ejemplo SAheart Para ilustrar el uso de la regresión logística, usaremos el conjunto de datos SAheart del paquete ElemStatLearn. # install.packages(&quot;bestglm&quot;) library(bestglm) data(&quot;SAheart&quot;) sbp tobacco ldl adiposity famhist typea obesity alcohol age chd 160 12.00 5.73 23.11 Present 49 25.30 97.20 52 1 144 0.01 4.41 28.61 Absent 55 28.87 2.06 63 1 118 0.08 3.48 32.28 Present 52 29.14 3.81 46 0 170 7.50 6.41 38.03 Present 51 31.99 24.26 58 1 134 13.60 3.50 27.78 Present 60 25.99 57.34 49 1 132 6.20 6.47 36.21 Present 62 30.77 14.14 45 0 Estos datos provienen de una muestra retrospectiva de hombres en una región de alto riesgo de enfermedades cardíacas en Western Cape, Sudáfrica. La variable chd, que usaremos como respuesta, indica si la enfermedad coronaria está presente en un individuo o no. Tenga en cuenta que esto está codificado como una variable numérica 0 / 1. Usando esto como respuesta con glm() es importante indicar family = binomial, de lo contrario se ajustará la regresión lineal ordinaria. Más adelante, veremos el uso de una respuesta de variable de factor, que en realidad se prefiere, ya que no se puede ajustar la regresión lineal ordinaria accidentalmente. Los predictores son varias medidas para cada individuo, muchas de ellas relacionadas con la salud del corazón. Por ejemplo, sbp, presión arterial sistólica y ldl, colesterol unido a lipoproteínas de baja densidad. Para obtener todos los detalles, utilice ?SAheart. Comenzaremos por intentar modelar la probabilidad de enfermedad coronaria con base en el colesterol de lipoproteínas de baja densidad. Es decir, nos ajustaremos al modelo. \\[ \\log\\left(\\frac{P[\\texttt{chd} = 1]}{1 - P[\\texttt{chd} = 1]}\\right) = \\beta_0 + \\beta_{\\texttt{ldl}} x_{\\texttt{ldl}} \\] chd_mod_ldl = glm(chd ~ ldl, data = SAheart, family = binomial) plot(jitter(chd, factor = 0.1) ~ ldl, data = SAheart, pch = 20, ylab = &quot;Probabilidad de CHD&quot;, xlab = &quot;Colesterol de lipoproteínas de baja densidad&quot;) grid() curve(predict(chd_mod_ldl, data.frame(ldl = x), type = &quot;response&quot;), add = TRUE, col = &quot;dodgerblue&quot;, lty = 2) Como antes, graficamos los datos además de las probabilidades estimadas. Tenga en cuenta que hemos alterado los datos para que sean más fáciles de visualizar, pero los datos solo toman valores 0 y 1. Como era de esperar, este gráfico indica que a medida que aumenta ldl, también lo hace la probabilidad de chd. coef(summary(chd_mod_ldl)) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.9686681 0.27307908 -7.209150 5.630207e-13 ## ldl 0.2746613 0.05163983 5.318787 1.044615e-07 Para realizar la prueba \\[ H_0: \\beta_{\\texttt{ldl}} = 0 \\] usamos la función summary() como lo hemos hecho tantas veces antes. Al igual que la prueba \\(t\\) para la regresión lineal ordinaria, devuelve la estimación del parámetro, su error estándar, el estadístico de prueba relevante (\\(z\\)) y su valor p. Aquí tenemos un valor p increíblemente bajo, por lo que rechazamos la hipótesis nula. La variable ldl parece ser un predictor significativo. Al ajustar la regresión logística, podemos usar la misma sintaxis de fórmula que la regresión lineal ordinaria. Entonces, para ajustar un modelo aditivo usando todos los predictores disponibles, usamos: chd_mod_additive = glm(chd ~ ., data = SAheart, family = binomial) Luego, podemos usar la prueba de razón de verosimilitud para comparar los dos modelos. Específicamente, estamos probando \\[ H_0: \\beta_{\\texttt{sbp}} = \\beta_{\\texttt{tobacco}} = \\beta_{\\texttt{adiposity}} = \\beta_{\\texttt{famhist}} = \\beta_{\\texttt{typea}} = \\beta_{\\texttt{obesity}} = \\beta_{\\texttt{alcohol}} = \\beta_{\\texttt{age}} = 0 \\] Podríamos calcular manualmente el estadístico de prueba, -2 * as.numeric(logLik(chd_mod_ldl) - logLik(chd_mod_additive)) ## [1] 92.13879 O podríamos utilizar la función anova(). Al especificar test = \"LRT\", R usará la prueba de razón de verosimilitud para comparar los dos modelos. anova(chd_mod_ldl, chd_mod_additive, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: chd ~ ldl ## Model 2: chd ~ sbp + tobacco + ldl + adiposity + famhist + typea + obesity + ## alcohol + age ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 460 564.28 ## 2 452 472.14 8 92.139 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Vemos que el estadístico de prueba que acabamos de calcular aparece en la salida. El valor p muy pequeño sugiere que preferimos el modelo más grande. Si bien preferimos el modelo aditivo en comparación con el modelo de un solo predictor, ¿realmente necesitamos todos los predictores en el modelo aditivo? Para seleccionar un subconjunto de predictores, podemos usar un procedimiento paso a paso como hicimos con la regresión lineal ordinaria. Recuerde que AIC y BIC se definieron en términos de probabilidades. Aquí demostramos el uso de AIC con un procedimiento de selección hacia atrás. chd_mod_selected = step(chd_mod_additive, trace = 0) coef(chd_mod_selected) ## (Intercept) tobacco ldl famhistPresent typea ## -6.44644451 0.08037533 0.16199164 0.90817526 0.03711521 ## age ## 0.05046038 Podríamos volver a comparar este modelo con los modelos aditivos. \\[ H_0: \\beta_{\\texttt{sbp}} = \\beta_{\\texttt{adiposity}} = \\beta_{\\texttt{obesity}} = \\beta_{\\texttt{alcohol}} = 0 \\] anova(chd_mod_selected, chd_mod_additive, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: chd ~ tobacco + ldl + famhist + typea + age ## Model 2: chd ~ sbp + tobacco + ldl + adiposity + famhist + typea + obesity + ## alcohol + age ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 456 475.69 ## 2 452 472.14 4 3.5455 0.471 Aquí parece que preferiríamos el modelo seleccionado. 17.3.5 Intervalos de confianza Podemos crear intervalos de confianza para los parámetros \\(\\beta\\) usando la función confint() como hicimos con la regresión lineal ordinaria. confint(chd_mod_selected, level = 0.99) ## Waiting for profiling to be done... ## 0.5 % 99.5 % ## (Intercept) -8.941825274 -4.18278990 ## tobacco 0.015704975 0.14986616 ## ldl 0.022923610 0.30784590 ## famhistPresent 0.330033483 1.49603366 ## typea 0.006408724 0.06932612 ## age 0.024847330 0.07764277 Tenga en cuenta que podríamos crear intervalos reordenando los resultados de la prueba de Wald para obtener el intervalo de confianza de Wald. Esto estaría dado por \\[ \\hat{\\beta}_j \\pm z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\beta}_j]. \\] Sin embargo, R utiliza un enfoque ligeramente diferente basado en un concepto llamado verosimilitud de perfil. (Los detalles los omitiremos). En última instancia, los intervalos informados serán similares, pero el método utilizado por R es más común en la práctica, probablemente al menos parcialmente porque es el enfoque predeterminado en R. Compruebe cómo se comparan los intervalos que utilizan la fórmula anterior con los de la salida de confint(). (O, tenga en cuenta que el uso de confint.default() devolverá los resultados del cálculo del intervalo de confianza de Wald). 17.3.6 Intervalos de confianza para la respuesta promedio Los intervalos de confianza para la respuesta promedio requieren una reflexión adicional. Con una muestra suficientemente grande, tenemos \\[ \\frac{\\hat{\\eta}({\\bf x}) - \\eta({\\bf x})}{\\text{SE}[\\hat{\\eta}({\\bf x})]} \\overset{\\text{approx}}{\\sim} N(0, 1) \\] Entonces podemos crear intervalos de confianza aproximados, al \\((1 - \\alpha)\\%\\) para \\(\\eta({\\bf x})\\) usando \\[ \\hat{\\eta}({\\bf x}) \\pm z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\eta}({\\bf x})] \\] donde \\(z_{\\alpha/2}\\) es el valor crítico tal que \\(P(Z &gt; z_{\\alpha/2}) = \\alpha/2\\). Este no es un intervalo particularmente interesante. En cambio, lo que realmente queremos es un intervalo para la respuesta promedio, \\(p({\\bf x})\\). Para obtener un intervalo para \\(p({\\bf x})\\), simplemente aplicamos la transformación logit inversa a los puntos finales del intervalo para \\(\\eta.\\) \\[ \\left(\\text{logit}^{-1}(\\hat{\\eta}({\\bf x}) - z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\eta}({\\bf x})] ), \\ \\text{logit}^{-1}(\\hat{\\eta}({\\bf x}) + z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\eta}({\\bf x})])\\right) \\] Para demostrar la creación de estos intervalos, consideraremos una nueva observación. new_obs = data.frame( sbp = 148.0, tobacco = 5, ldl = 12, adiposity = 31.23, famhist = &quot;Present&quot;, typea = 47, obesity = 28.50, alcohol = 23.89, age = 60 ) Primero, usaremos la función predict() para obtener \\(\\hat{\\eta}({\\bf x})\\) para esta observación. eta_hat = predict(chd_mod_selected, new_obs, se.fit = TRUE, type = &quot;link&quot;) eta_hat ## $fit ## 1 ## 1.579545 ## ## $se.fit ## [1] 0.4114796 ## ## $residual.scale ## [1] 1 Al establecer se.fit = TRUE, R también calcula \\(\\text{SE}[\\hat{\\eta}({\\bf x})]\\). Tenga en cuenta que usamos type = \"link\", pero este es en realidad un valor predeterminado. Lo agregamos para enfatizar que la salida de predict() será el valor de la función de enlace. z_crit = round(qnorm(0.975), 2) round(z_crit, 2) ## [1] 1.96 Después de obtener el valor crítico correcto, podemos crear fácilmente un intervalo de confianza al \\(95\\%\\) para \\(\\eta({\\bf x})\\). eta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit ## [1] 0.773045 2.386045 Ahora simplemente necesitamos aplicar la transformación correcta para hacer de este un intervalo de confianza para \\(p({\\bf x})\\), la probabilidad de enfermedad coronaria para esta observación. Tenga en cuenta que el paquete boot contiene las funciones logit() e inv.logit() que son las transformaciones logit y logit inverso, respectivamente. boot::inv.logit(eta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit) ## [1] 0.6841792 0.9157570 Como era de esperar, los límites de este intervalo están entre 0 y 1. Además, dado que ambos límites del intervalo para \\(\\eta({\\bf x})\\) son positivos, ambos límites del intervalo para \\(p({\\bf x})\\) son mayores que 0.5. 17.3.7 Sintaxis de la fórmula Sin pensarlo realmente, hemos estado usando nuestro conocimiento previo de la sintaxis de la fórmula del modelo de R para ajustar la regresión logística. 17.3.7.1 Interacciones Agreguemos una interacción entre LDL e historia familiar para el modelo que seleccionamos. chd_mod_interaction = glm(chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist, data = SAheart, family = binomial) summary(chd_mod_interaction) ## ## Call: ## glm(formula = chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist, ## family = binomial, data = SAheart) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9082 -0.8308 -0.4550 0.9286 2.5152 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.043472 0.937186 -6.449 1.13e-10 *** ## alcohol 0.003800 0.004332 0.877 0.38033 ## ldl 0.035593 0.071448 0.498 0.61837 ## famhistPresent -0.733836 0.618131 -1.187 0.23515 ## typea 0.036253 0.012172 2.978 0.00290 ** ## age 0.062416 0.009723 6.419 1.37e-10 *** ## ldl:famhistPresent 0.314311 0.114922 2.735 0.00624 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 477.46 on 455 degrees of freedom ## AIC: 491.46 ## ## Number of Fisher Scoring iterations: 5 Según la prueba \\(z\\) vista en el resumen anterior, esta interacción es significativa. El efecto de las LDL sobre la probabilidad de CHD es diferente según los antecedentes familiares. 17.3.7.2 Términos polinomiales Tomemos el modelo anterior y ahora agreguemos un término polinomial. chd_mod_int_quad = glm(chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist + I(ldl^2), data = SAheart, family = binomial) summary(chd_mod_int_quad) ## ## Call: ## glm(formula = chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist + ## I(ldl^2), family = binomial, data = SAheart) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8953 -0.8311 -0.4556 0.9276 2.5204 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.096747 1.065952 -5.720 1.07e-08 *** ## alcohol 0.003842 0.004350 0.883 0.37716 ## ldl 0.056876 0.214420 0.265 0.79081 ## famhistPresent -0.723769 0.625167 -1.158 0.24698 ## typea 0.036248 0.012171 2.978 0.00290 ** ## age 0.062299 0.009788 6.365 1.95e-10 *** ## I(ldl^2) -0.001587 0.015076 -0.105 0.91617 ## ldl:famhistPresent 0.311615 0.117559 2.651 0.00803 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 596.11 on 461 degrees of freedom ## Residual deviance: 477.45 on 454 degrees of freedom ## AIC: 493.45 ## ## Number of Fisher Scoring iterations: 5 Como era de esperar, dado que esta variable transformada adicional no se eligió inteligentemente, no es significativa. Sin embargo, esto nos permite enfatizar el hecho de que la notación de sintaxis que habíamos estado usando con lm() funciona básicamente exactamente igual para glm(), sin embargo ahora entendemos que esto está especificando la combinación lineal de predicciones , \\(\\eta({\\bf x})\\). Es decir, lo anterior se ajusta al modelo. \\[ \\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_{1}x_{\\texttt{alcohol}} + \\beta_{2}x_{\\texttt{ldl}} + \\beta_{3}x_{\\texttt{famhist}} + \\beta_{4}x_{\\texttt{typea}} + \\beta_{5}x_{\\texttt{age}} + \\beta_{6}x_{\\texttt{ldl}}x_{\\texttt{famhist}} + \\beta_{7}x_{\\texttt{ldl}}^2 \\] ¡Es posible que se haya dado cuenta de esto antes de que lo escribiéramos explícitamente! 17.3.8 Desviación Probablemente haya notado que la salida de summary() también es muy similar a la de la regresión lineal ordinaria. Una diferencia es la desviación que se informa. Null deviance es la desviación del modelo nulo, es decir, un modelo sin predictores. La Residual deviance es la desviación del modelo que se ajustó. Desviación compara el modelo con un modelo saturado. (Sin observaciones repetidas, un modelo saturado es un modelo que se ajusta perfectamente, utilizando un parámetro para cada observación). Esencialmente, la desviación es una suma de cuadrados residual generalizada para GLM. Al igual que RSS, la desviación disminuye a medida que aumenta la complejidad del modelo. deviance(chd_mod_ldl) ## [1] 564.2788 deviance(chd_mod_selected) ## [1] 475.6856 deviance(chd_mod_additive) ## [1] 472.14 Tenga en cuenta que estos están anidados y vemos que la desviación disminuye a medida que aumenta el tamaño del modelo. Entonces, si bien una desviación más baja es mejor, si el modelo se vuelve demasiado grande, puede estar sobreajustado. Tenga en cuenta que R también genera AIC en el resumen, que penalizará según el tamaño del modelo, para evitar el sobreajuste. 17.4 Clasificación Hasta ahora, hemos utilizado principalmente la regresión logística para estimar las probabilidades de clase. El siguiente paso algo obvio es usar estas probabilidades para hacer predicciones, que en este contexto, llamaríamos clasificaciones. Según los valores de los predictores, ¿debería clasificarse una observación como \\(Y=1\\) o como \\(Y=0\\)? Supongamos que no necesitáramos estimar probabilidades a partir de datos y, en cambio, supiéramos ambos \\[ p({\\bf x}) = P[Y = 1 \\mid {\\bf X} = {\\bf x}] \\] y \\[ 1 - p({\\bf x}) = P[Y = 0 \\mid {\\bf X} = {\\bf x}]. \\] Con esta información, clasificar las observaciones basadas en los valores de los predictores es realmente muy fácil. Simplemente clasifique una observación en la clase (\\(0\\) o \\(1\\)) con mayor probabilidad. En general, este resultado se denomina Clasificador de Bayes, \\[ C^B({\\bf x}) = \\underset{k}{\\mathrm{argmax}} \\ P[Y = k \\mid {\\bf X = x}]. \\] Para una respuesta binaria, es decir, \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; p({\\bf x}) &gt; 0.5 \\\\ 0 &amp; p({\\bf x}) \\leq 0.5 \\end{cases} \\] En pocas palabras, el clasificador de Bayes (que no debe confundirse con el clasificador Naive Bayes) minimiza la probabilidad de clasificación errónea al clasificar cada observación en la clase con la probabilidad más alta. Desafortunadamente, en la práctica, no conoceremos las probabilidades necesarias para usar directamente el clasificador de Bayes. En su lugar, tendremos que usar probabilidades estimadas. Entonces, para crear un clasificador que busque minimizar las clasificaciones erróneas, usaríamos, \\[ \\hat{C}({\\bf x}) = \\underset{k}{\\mathrm{argmax}} \\ \\hat{P}[Y = k \\mid {\\bf X = x}]. \\] En el caso de una respuesta binaria desde \\(\\hat{p}({\\bf x}) = 1 - \\hat{p}({\\bf x})\\), esto se convierte en \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; \\hat{p}({\\bf x}) &gt; 0.5 \\\\ 0 &amp; \\hat{p}({\\bf x}) \\leq 0.5 \\end{cases} \\] Usando esta simple regla de clasificación, podemos convertir la regresión logística en un clasificador. Para usarla en la clasificación, primero usamos la regresión logística para obtener probabilidades estimadas, \\(\\hat{p}({\\bf x})\\), luego las usamos junto con la regla de clasificación anterior. La regresión logística es solo una de las muchas formas en que se pueden estimar estas probabilidades. En un curso completamente centrado en el aprendizaje automático, aprenderá muchas formas adicionales de hacer esto, así como métodos para realizar clasificaciones directamente sin necesidad de estimar las probabilidades primero. Pero como ya habíamos introducido la regresión logística, tiene sentido discutirla en el contexto de la clasificación. 17.4.1 Ejemplo spam Para ilustrar el uso de la regresión logística como clasificador, usaremos el conjunto de datos spam del paquete kernlab. # install.packages(&quot;kernlab&quot;) library(kernlab) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha data(&quot;spam&quot;) tibble::as.tibble(spam) ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## # A tibble: 4,601 x 58 ## make address all num3d our over remove internet order mail receive ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.64 0.64 0 0.32 0 0 0 0 0 0 ## 2 0.21 0.28 0.5 0 0.14 0.28 0.21 0.07 0 0.94 0.21 ## 3 0.06 0 0.71 0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 ## 4 0 0 0 0 0.63 0 0.31 0.63 0.31 0.63 0.31 ## 5 0 0 0 0 0.63 0 0.31 0.63 0.31 0.63 0.31 ## 6 0 0 0 0 1.85 0 0 1.85 0 0 0 ## 7 0 0 0 0 1.92 0 0 0 0 0.64 0.96 ## 8 0 0 0 0 1.88 0 0 1.88 0 0 0 ## 9 0.15 0 0.46 0 0.61 0 0.3 0 0.92 0.76 0.76 ## 10 0.06 0.12 0.77 0 0.19 0.32 0.38 0 0.06 0 0 ## # ... with 4,591 more rows, and 47 more variables: will &lt;dbl&gt;, people &lt;dbl&gt;, ## # report &lt;dbl&gt;, addresses &lt;dbl&gt;, free &lt;dbl&gt;, business &lt;dbl&gt;, email &lt;dbl&gt;, ## # you &lt;dbl&gt;, credit &lt;dbl&gt;, your &lt;dbl&gt;, font &lt;dbl&gt;, num000 &lt;dbl&gt;, money &lt;dbl&gt;, ## # hp &lt;dbl&gt;, hpl &lt;dbl&gt;, george &lt;dbl&gt;, num650 &lt;dbl&gt;, lab &lt;dbl&gt;, labs &lt;dbl&gt;, ## # telnet &lt;dbl&gt;, num857 &lt;dbl&gt;, data &lt;dbl&gt;, num415 &lt;dbl&gt;, num85 &lt;dbl&gt;, ## # technology &lt;dbl&gt;, num1999 &lt;dbl&gt;, parts &lt;dbl&gt;, pm &lt;dbl&gt;, direct &lt;dbl&gt;, ## # cs &lt;dbl&gt;, meeting &lt;dbl&gt;, original &lt;dbl&gt;, project &lt;dbl&gt;, re &lt;dbl&gt;, ## # edu &lt;dbl&gt;, table &lt;dbl&gt;, conference &lt;dbl&gt;, charSemicolon &lt;dbl&gt;, ## # charRoundbracket &lt;dbl&gt;, charSquarebracket &lt;dbl&gt;, charExclamation &lt;dbl&gt;, ## # charDollar &lt;dbl&gt;, charHash &lt;dbl&gt;, capitalAve &lt;dbl&gt;, capitalLong &lt;dbl&gt;, ## # capitalTotal &lt;dbl&gt;, type &lt;fct&gt; Este conjunto de datos, creado a finales de la década de 1990 en Hewlett-Packard Labs, contiene 4601 correos electrónicos, de los cuales 1813 se consideran spam. El resto no es spam. (Que para simplificar, podríamos llamar ham.) Se pueden obtener detalles adicionales usando ?Spam o visitando UCI Machine Learning Repository. . La variable de respuesta, type, es un factor con niveles que etiquetan cada correo electrónico como spam o nonspam. Al ajustar los modelos, nonspam será el nivel de referencia, \\(Y=0\\), ya que aparece primero en orden alfabético. is.factor(spam$type) ## [1] TRUE levels(spam$type) ## [1] &quot;nonspam&quot; &quot;spam&quot; Muchos de los predictores (a menudo llamados características en el aprendizaje automático) se diseñan en función de los correos electrónicos. Por ejemplo, charDollar es el número de veces que un correo electrónico contiene el carácter $. Algunas variables son muy específicas de este conjunto de datos, por ejemplo, george y num650. (El nombre y código de área de uno de los investigadores cuyos correos electrónicos se utilizaron). Debemos tener en cuenta que este conjunto de datos se creó a partir de correos electrónicos enviados a un investigador de tipo académico en la década de 1990. Es probable que los resultados que obtengamos no se generalicen a los correos electrónicos modernos para el público en general. Para comenzar, primero dividiremos los datos en entrenamiento-prueba (test-train). set.seed(42) # spam_idx = sample(nrow(spam), round(nrow(spam) / 2)) spam_idx = sample(nrow(spam), 1000) spam_trn = spam[spam_idx, ] spam_tst = spam[-spam_idx, ] Hemos utilizado un conjunto de entrenamiento algo pequeño en relación con el tamaño total del conjunto de datos. En la práctica, probablemente debería ser más grande, pero esto es simplemente para reducir el tiempo de entrenamiento para la ilustración y reproducción de este documento. fit_caps = glm(type ~ capitalTotal, data = spam_trn, family = binomial) fit_selected = glm(type ~ edu + money + capitalTotal + charDollar, data = spam_trn, family = binomial) fit_additive = glm(type ~ ., data = spam_trn, family = binomial) fit_over = glm(type ~ capitalTotal * (.), data = spam_trn, family = binomial, maxit = 50) Ajustaremos cuatro regresiones logísticas, cada una más compleja que la anterior. Tenga en cuenta que estamos suprimiendo dos advertencias. La primera la mencionamos anteriormente. ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Tenga en cuenta que, cuando recibamos esta advertencia, deberíamos sospechar mucho de las estimaciones de los parámetros. coef(fit_selected) ## (Intercept) edu money capitalTotal charDollar ## -1.1199744712 -1.9837988840 0.9784675298 0.0007757011 11.5772904667 Sin embargo, el modelo aún se puede usar para crear un clasificador, y evaluaremos ese clasificador por sus propios méritos. También suprimimos la advertencia: ## Warning: glm.fit: algorithm did not converge En realidad, no lo suprimimos, sino que cambiamos maxit a 50, al ajustar el modelo fit_over. Estas fueron suficientes iteraciones adicionales para permitir que el algoritmo de mínimos cuadrados reponderados iterativamente converja al ajustar el modelo. 17.4.2 Evaluar clasificadores La métrica que más nos interesará para evaluar el rendimiento general de un clasificador es la tasa de clasificación errónea. (A veces, en cambio, se informa la precisión, que es la proporción de clasificaciones correctas, por lo que ambas métricas tienen el mismo propósito). \\[ \\text{Misclass}(\\hat{C}, \\text{Data}) = \\frac{1}{n}\\sum_{i = 1}^{n}I(y_i \\neq \\hat{C}({\\bf x_i})) \\] \\[ I(y_i \\neq \\hat{C}({\\bf x_i})) = \\begin{cases} 0 &amp; y_i = \\hat{C}({\\bf x_i}) \\\\ 1 &amp; y_i \\neq \\hat{C}({\\bf x_i}) \\\\ \\end{cases} \\] Al usar esta métrica en los datos de entrenamiento, tendrá los mismos problemas que RSS para la regresión lineal ordinaria, es decir, solo disminuirá. # tasa de clasificación errónea de entrenamiento mean(ifelse(predict(fit_caps) &gt; 0, &quot;spam&quot;, &quot;nonspam&quot;) != spam_trn$type) ## [1] 0.339 mean(ifelse(predict(fit_selected) &gt; 0, &quot;spam&quot;, &quot;nonspam&quot;) != spam_trn$type) ## [1] 0.224 mean(ifelse(predict(fit_additive) &gt; 0, &quot;spam&quot;, &quot;nonspam&quot;) != spam_trn$type) ## [1] 0.066 mean(ifelse(predict(fit_over) &gt; 0, &quot;spam&quot;, &quot;nonspam&quot;) != spam_trn$type) ## [1] 0.136 Debido a esto, los datos de entrenamiento no son útiles para evaluar, ya que sugerirían que siempre deberíamos usar el modelo más grande posible, cuando en realidad, es probable que ese modelo esté sobreajustado. Recuerde, un modelo que es demasiado complejo se sobreajustará. Un modelo demasiado simple quedará bien. (Estamos buscando algo en el medio). Para superar esto, usaremos la validación cruzada como hicimos con la regresión lineal ordinaria, pero esta vez validaremos de forma cruzada la tasa de clasificación errónea. Para hacerlo, usaremos la función cv.glm() de la libreria boot. Toma argumentos para los datos (en este caso entrenamiento), un modelo ajustado a través de glm() y K, el número de veces Consulte ?Cv.glm para obtener más detalles. Anteriormente, para la validación cruzada de RMSE en regresión lineal ordinaria, usamos LOOCV. Ciertamente podríamos hacer eso aquí. Sin embargo, con la regresión logística, ya no tenemos el truco inteligente que permitiría obtener una métrica LOOCV sin necesidad de ajustar el modelo \\(n\\) veces. Entonces, en su lugar, usaremos una validación cruzada de 5 veces. (5 y 10 veces son las más comunes en la práctica). En lugar de omitir una sola observación repetidamente, omitiremos una quinta parte de los datos. Básicamente, repetiremos el siguiente proceso 5 veces: Separar al azar una quinta parte de los datos (cada observación solo se retendrá una vez) Modelo de entrenamiento con datos restantes Evaluar la tasa de clasificación errónea de los datos retenidos La tasa de clasificación errónea con validación cruzada de 5 veces será el promedio de estas tasas de clasificación errónea. Solo necesitando reajustar el modelo 5 veces, en lugar de \\(n\\) veces, ahorraremos mucho tiempo de cálculo. library(boot) set.seed(1) cv.glm(spam_trn, fit_caps, K = 5)$delta[1] ## [1] 0.2166961 cv.glm(spam_trn, fit_selected, K = 5)$delta[1] ## [1] 0.1587043 cv.glm(spam_trn, fit_additive, K = 5)$delta[1] ## [1] 0.08684467 cv.glm(spam_trn, fit_over, K = 5)$delta[1] ## [1] 0.137 Tenga en cuenta que estamos suprimiendo las advertencias nuevamente. (Ahora habría muchos más, ya que se ajustarían un total de 20 modelos). Según estos resultados, fit_caps y fit_selected son insuficientes en relación con fit_additive. De manera similar, fit_over está sobreajustado en relación con fit_additive. Por lo tanto, con base en estos resultados, preferimos el clasificador creado con base al ajuste de regresión logística y almacenado en fit_additive. En el futuro, para evaluar e informar sobre la eficacia de este clasificador, usaremos el conjunto de datos de prueba. Adoptaremos la posición de que el conjunto de datos de prueba nunca debe usarse en el entrenamiento, por lo que usamos la validación cruzada dentro del conjunto de datos de entrenamiento para seleccionar un modelo. Aunque la validación cruzada utiliza conjuntos de reserva para generar métricas, en algún momento todos los datos se utilizan para el entrenamiento. Para resumir rápidamente qué tan bien funciona este clasificador, crearemos una matriz de confusión. Matriz de confusión Además, desglosa los errores de clasificación en falsos positivos y falsos negativos. make_conf_mat = function(predicted, actual) { table(predicted = predicted, actual = actual) } Almacenemos explícitamente los valores predichos de nuestro clasificador en el conjunto de datos de prueba. spam_tst_pred = ifelse(predict(fit_additive, spam_tst) &gt; 0, &quot;spam&quot;, &quot;nonspam&quot;) spam_tst_pred = ifelse(predict(fit_additive, spam_tst, type = &quot;response&quot;) &gt; 0.5, &quot;spam&quot;, &quot;nonspam&quot;) Las dos líneas de código anteriores producen el mismo resultado, es decir, las mismas predicciones, ya que \\[ \\eta({\\bf x}) = 0 \\iff p({\\bf x}) = 0.5 \\] Ahora usaremos estas predicciones para crear una matriz de confusión. (conf_mat_50 = make_conf_mat(predicted = spam_tst_pred, actual = spam_tst$type)) ## actual ## predicted nonspam spam ## nonspam 2057 157 ## spam 127 1260 \\[ \\text{Prev} = \\frac{\\text{P}}{\\text{Total Obs}}= \\frac{\\text{TP + FN}}{\\text{Total Obs}} \\] table(spam_tst$type) / nrow(spam_tst) ## ## nonspam spam ## 0.6064982 0.3935018 Primero, tenga en cuenta que para ser un clasificador razonable, debe superar al clasificador obvio de simplemente clasificar todas las observaciones en la clase mayoritaria. En este caso, clasificar todo como no spam para una tasa de clasificación errónea de prueba de 0.3935018 A continuación, podemos ver que usando el clasificador creado a partir de fit_additive, un total de \\(137 + 161 = 298\\) del total de 3601 correos electrónicos en el conjunto de prueba están mal clasificados. En general, la precisión en la prueba se establece mean(spam_tst_pred == spam_tst$type) ## [1] 0.921133 En otras palabras, la clasificación errónea de la prueba es mean(spam_tst_pred != spam_tst$type) ## [1] 0.07886698 Esto parece un clasificador decente  Sin embargo, ¿todos los errores son iguales? En este caso, absolutamente no. Los 137 correos electrónicos no spam que se marcaron como spam (falsos positivos) son un problema. No podemos permitir que información importante, digamos, una oferta de trabajo, se pierda de nuestra bandeja de entrada y sea enviada a la carpeta de correo no deseado. Por otro lado, los 161 correos electrónicos no deseados que llegarían a una bandeja de entrada (falsos negativos) se tratan fácilmente, simplemente elimínelos. En lugar de simplemente evaluar un clasificador en función de su tasa de clasificación errónea (o precisión), definiremos dos métricas adicionales, sensibilidad y especificidad. Tenga en cuenta que estas son simplemente dos de muchas más métricas que se pueden considerar. La página de Wikipedia para sensibilidad y especificidad detalla una gran cantidad de métricas que pueden derivarse de una matriz de confusión. Sensibilidad es esencialmente la tasa de verdaderos positivos. Entonces, cuando la sensibilidad es alta, el número de falsos negativos es bajo. \\[ \\text{Sens} = \\text{True Positive Rate} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP + FN}} \\] Tenemos una función en R para calcular la sensibilidad basada en la matriz de confusión. Tenga en cuenta que esta función es buena para fines ilustrativos, pero se rompe fácilmente. (Piense en lo que sucede si no se pronostican positivos). get_sens = function(conf_mat) { conf_mat[2, 2] / sum(conf_mat[, 2]) } Especificidad es esencialmente la tasa de verdaderos negativos. Entonces, cuando la especificidad es alta, el número de falsos positivos es bajo. \\[ \\text{Spec} = \\text{True Negative Rate} = \\frac{\\text{TN}}{\\text{N}} = \\frac{\\text{TN}}{\\text{TN + FP}} \\] get_spec = function(conf_mat) { conf_mat[1, 1] / sum(conf_mat[, 1]) } Calculamos ambos en función de la matriz de confusión que habíamos creado para nuestro clasificador. get_sens(conf_mat_50) ## [1] 0.8892025 get_spec(conf_mat_50) ## [1] 0.9418498 Recuerde que habíamos creado este clasificador usando una probabilidad de \\(0.5\\) como un límite de cómo se deben clasificar las observaciones. Ahora modificaremos este límite. Veremos que al modificar el límite, \\(c\\), podemos mejorar la sensibilidad o la especificidad a expensas de la precisión general (tasa de clasificación errónea). \\[ \\hat{C}(\\bf x) = \\begin{cases} 1 &amp; \\hat{p}({\\bf x}) &gt; c \\\\ 0 &amp; \\hat{p}({\\bf x}) \\leq c \\end{cases} \\] Además, si cambiamos el límite para mejorar la sensibilidad, disminuiremos la especificidad y viceversa. Primero veamos qué sucede cuando bajamos el límite de \\(0.5\\) a \\(0.1\\) para crear un nuevo clasificador y, por lo tanto, nuevas predicciones. spam_tst_pred_10 = ifelse(predict(fit_additive, spam_tst, type = &quot;response&quot;) &gt; 0.1, &quot;spam&quot;, &quot;nonspam&quot;) Esto es esencialmente disminuir el umbral para que un correo electrónico sea etiquetado como spam, hasta ahora más correos electrónicos serán etiquetados como spam. Vemos eso en la siguiente matriz de confusión. (conf_mat_10 = make_conf_mat(predicted = spam_tst_pred_10, actual = spam_tst$type)) ## actual ## predicted nonspam spam ## nonspam 1583 29 ## spam 601 1388 Desafortunadamente, si bien esto reduce en gran medida los falsos negativos, los falsos positivos casi se han cuadriplicado. Vemos esto reflejado en la sensibilidad y especificidad. get_sens(conf_mat_10) ## [1] 0.9795342 get_spec(conf_mat_10) ## [1] 0.7248168 Este clasificador, que usa \\(0.1\\) en lugar de \\(0.5\\) tiene una mayor sensibilidad, pero una especificidad mucho menor. Claramente, deberíamos haber movido el límite en la otra dirección. Probemos \\(0.9\\). spam_tst_pred_90 = ifelse(predict(fit_additive, spam_tst, type = &quot;response&quot;) &gt; 0.9, &quot;spam&quot;, &quot;nonspam&quot;) Esto es esencialmente aumentar el umbral para que un correo electrónico sea etiquetado como spam, hasta ahora menos correos electrónicos serán etiquetados como spam. Nuevamente, vemos eso en la siguiente matriz de confusión. (conf_mat_90 = make_conf_mat(predicted = spam_tst_pred_90, actual = spam_tst$type)) ## actual ## predicted nonspam spam ## nonspam 2136 537 ## spam 48 880 Este es el resultado que buscamos. Tenemos muchos menos falsos positivos. Si bien la sensibilidad se reduce considerablemente, la especificidad ha aumentado. get_sens(conf_mat_90) ## [1] 0.6210303 get_spec(conf_mat_90) ## [1] 0.978022 Si bien se trata de muchos menos falsos positivos, ¿es aceptable? Probablemente todavía no. Además, no olvide que este sería un terrible detector de spam hoy en día, ya que se basa en datos de una era de Internet muy diferente, para un grupo de personas muy específico. ¡El spam ha cambiado mucho desde los 90! (Irónicamente, el aprendizaje automático es probablemente parcialmente el culpable). Este capítulo ha proporcionado una introducción bastante rápida a la clasificación y, por tanto, al aprendizaje automático. Para obtener una cobertura más completa del aprendizaje automático, An Introduction to Statistical Learning es un recurso muy recomendable. Además, R for Statistical Learning] se ha escrito como un suplemento que proporciona detalles adicionales sobre cómo realizar estos métodos utilizando R. "],["más-allá.html", "Capítulo 18 Más allá 18.1 RStudio 18.2 Tidy Data 18.3 Visualización 18.4 Aplicaciones web 18.5 Diseño experimental 18.6 Aprendizaje automático (Machine Learning) 18.7 Series de tiempo 18.8 Bayesiana 18.9 Computación de alto rendimiento. 18.10 Recursos adicionales de R", " Capítulo 18 Más allá ¿Fin? No, el viaje no termina aquí.  J.R.R. Tolkien Después de leer este capítulo, podrá: Comprender la hoja de ruta para la educación continua sobre modelos y el lenguaje de programación R. 18.1 RStudio ¡RStudio ha lanzado recientemente la versión 1.0! Esto es emocionante por varias razones, especialmente el lanzamiento de R Notebooks. Los Notebooks R combinan el RMarkdown que ya ha aprendido con la capacidad de trabajar de forma interactiva. 18.2 Tidy Data En este libro de texto, muchos de los datos que hemos visto han sido agradables y prolijos. Era rectangular donde cada fila es una observación y cada columna es una variable. ¡Este no es siempre el caso! Se han desarrollado muchos paquetes para manejar datos y forzarlos a un formato agradable, que se llama tidy data, que luego podemos usar para modelar. A menudo, durante el análisis, es aquí donde se dedicará una gran parte de su tiempo. La comunidad R ha comenzado a llamar a esta colección de paquetes Tidyverse. Una vez se llamó Hadleyverse, ya que Hadley Wickham ha sido el autor de muchos de los paquetes. Hadley está escribiendo un libro llamado R for Data Science que describe el uso de muchos de estos paquetes. (¡Y también cómo usar algunos para mejorar el proceso de modelado!) Este libro es un excelente punto de partida para profundizar en la comunidad de R. Los dos paquetes principales son dplyr y tidyr los cuales se utilizan internamente en RStudio. 18.3 Visualización En este curso, hemos utilizado principalmente los métodos de graficación base en R. Cuando se trabaja con datos ordenados, muchos usuarios prefieren utilizar el paquete ggplot2, también desarrollado por Hadley Wickham. RStudio proporciona una hoja de trucos bastante detallada para trabajar con ggplot2. La comunidad mantiene una galería de gráficos de ejemplos. El uso del paquete manipulate con RStudio da la capacidad para cambiar rápidamente un gráfico estático para que sea interactivo. 18.4 Aplicaciones web RStudio ha hecho que sea increíblemente fácil crear productos de datos mediante el uso de Shiny, que permite la creación de aplicaciones web con R. RStudio mantiene un tutorial en constante crecimiento y una galería de ejemplos. 18.5 Diseño experimental En el capítulo de ANOVA, discutimos brevemente el diseño experimental. Este tema fácilmente podría ser su propia clase y actualmente es un área de interés revitalizado con el aumento de las pruebas A/B Dos referencias estadísticas más clásicas incluyen Statistics for Experimenters de Box, Hunter y Hunter, así como Design and Analysis of Experiments de Douglas Montgomery. Hay varios paquetes en R para el diseño de experimentos, enumerados en la Vista de tareas CRAN. 18.6 Aprendizaje automático (Machine Learning) El uso de modelos para la predicción es el enfoque clave del aprendizaje automático. Hay muchos métodos, cada uno con su propio paquete, sin embargo, R tiene un paquete maravilloso llamado caret, Classification And REgression Training, que proporciona una interfaz unificada para entrenar estos modelos. También contiene varias utilidades para el procesamiento y visualización de datos que son útiles para el modelado predictivo. Applied Predictive Modeling de Max Kuhn, el autor del paquete caret es un buen recurso general para el modelado predictivo, que obviamente utiliza R. Una introducción al aprendizaje estadístico de James, Witten, Hastie y Tibshirani es una suave introducción al aprendizaje automáticodo desde una perspectiva estadística que usa R y retoma justo donde termina este curso. Esto se basa en The Elements of Statistical Learning al que se hace referencia a menudo de Hastie, Tibshirani y Friedman . Ambos están disponibles gratuitamente en línea. 18.6.1 Aprendizaje profundo (Deep Learning) Aunque probablemente no sea la mejor herramienta para el trabajo, R ahora tiene la capacidad de entrenar redes neuronales profundas a través de TensorFlow. 18.7 Series de tiempo En esta clase solo hemos considerado datos independientes. ¿Qué pasa si los datos son dependientes? La serie temporal es el área de las estadísticas que se ocupa de este problema y podría abarcar fácilmente varios cursos. Un libro de texto: Análisis de series de tiempo de la Universidad de Illinois que es gratuito es: Time Series Analysis and Its Applications: With R Examples by Shumway and Stoffer Algunos tutoriales: Little Book of R for Time Series Quick R: Time Series and Forecasting TSA: Start to Finish Examples Al realizar un análisis de series de tiempo en R, debe tener en cuenta los paquetes que son útiles para el análisis. Debería ser difícil evitar el forecast y zoo paquetes. A menudo, la parte más difícil será lidiar con los datos de fecha y hora. Asegúrese de utilizar uno de los muchos paquetes que le ayudarán con esto. 18.8 Bayesiana En esta clase, hemos trabajado dentro de la visión frecuentista de la estadística. Existe todo un universo alternativo de estadística bayesiana. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan de John Kruschke es una gran introducción al tema. Introduce el mundo de probabilistic programming, en particular Stan, que se puede utilizar tanto en R como en Python. 18.9 Computación de alto rendimiento. A menudo, R se llamará un lenguaje lento, por dos razones. Uno, porque muchos no entienden R. Dos, porque a veces realmente lo es. Afortunadamente, es fácil extender R a través del paquete Rcpp para permitir un código más rápido. Muchos paquetes de R modernos utilizan Rcpp para lograr un mejor rendimiento. 18.10 Recursos adicionales de R Además, no olvide que anteriormente en este libro hemos descrito una gran cantidad de recursos R. Ahora que ha comenzado con R, muchos de estos serán mucho más útiles. Si alguno de estos temas le interesa y desea obtener más información, no dude en iniciar una discusión en los foros. :) "]]
